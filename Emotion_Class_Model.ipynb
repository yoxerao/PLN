{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already lowercased and lacks punctuation. We will tokenize the text and remove stopwords, as well as apply lemmatization to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "data = pd.read_json('./data/data.jsonl', lines=True)\n",
    "def pre_process_data(dataset):\n",
    "    #tokenize\n",
    "    dataset['text'] = dataset['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data = pre_process_data(data)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "test_data = pd.read_json('./data/test.jsonl', lines=True)\n",
    "train_data = pd.read_json('./data/train.jsonl', lines=True)\n",
    "validation_data = pd.read_json('./data/validation.jsonl', lines=True)\n",
    "\n",
    "test_data = pre_process_data(test_data)\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:09:31.882317900Z",
     "start_time": "2024-03-12T21:09:28.844193200Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# 2. Vectorization (secção possívelmente temporária, mas queria experimentar as cenas de tf_idf depois do pré-processamento)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416809, 40000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=40000)\n",
    "X = vectorizer.fit_transform(data['text'].apply(lambda x: ' '.join(x)))\n",
    "# good idea to use two-grams??\n",
    "print(X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:46.201324100Z",
     "start_time": "2024-03-12T21:02:32.893616Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaron' 'ab' ... 'zoom class' 'zoom grey' 'zumba']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:48.301712700Z",
     "start_time": "2024-03-12T21:02:48.254918600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "X_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "X_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:19:44.864392500Z",
     "start_time": "2024-03-12T21:19:44.374282600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "y_train = train_data['label']\n",
    "y_val = validation_data['label']\n",
    "y_test = test_data['label']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:19:45.558978900Z",
     "start_time": "2024-03-12T21:19:45.536984400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 3. Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1. Model Selection\n",
    "Aqui também só estava a querer espetar modelos para começar a ver o que dá que ainda não sei que features vão ser usadas:\n",
    "tf-idf, word embeddings, ???, features mais feitas à mão?\n",
    "\n",
    "\n",
    "Isto pelos vistos é uma cena, que não implementei (ainda..)\n",
    "\n",
    "\"The validation set uses a subset of the training data to provide an unbiased evaluation of a model. The validation data set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it. It is in this phase that hyperparameter tuning occurs.\"\n",
    "\n",
    "Wikipedia:\n",
    "The basic process of using a validation data set for model selection (as part of training data set, validation data set, and test data set) is:\n",
    "\n",
    "Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.\n",
    "\n",
    "An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3.1.1. Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       550\n",
      "           1       0.84      0.97      0.90       704\n",
      "           2       0.93      0.63      0.75       178\n",
      "           3       0.93      0.83      0.87       275\n",
      "           4       0.89      0.75      0.81       212\n",
      "           5       0.92      0.57      0.70        81\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.90      0.78      0.83      2000\n",
      "weighted avg       0.88      0.88      0.87      2000\n",
      "\n",
      "[[526  13   0   5   5   1]\n",
      " [  9 682   8   2   2   1]\n",
      " [  9  53 113   3   0   0]\n",
      " [ 24  19   1 227   4   0]\n",
      " [ 19  26   0   7 158   2]\n",
      " [  9  16   0   1   9  46]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "y_pred = logreg_classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred)) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:25:02.914309600Z",
     "start_time": "2024-03-12T21:24:56.981788400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3.1.2. Multinomial Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.94      0.82       550\n",
      "           1       0.64      0.99      0.78       704\n",
      "           2       1.00      0.08      0.16       178\n",
      "           3       0.98      0.37      0.53       275\n",
      "           4       0.92      0.34      0.50       212\n",
      "           5       1.00      0.02      0.05        81\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.88      0.46      0.47      2000\n",
      "weighted avg       0.79      0.70      0.64      2000\n",
      "\n",
      "[[517  31   0   1   1   0]\n",
      " [  7 697   0   0   0   0]\n",
      " [ 22 141  15   0   0   0]\n",
      " [ 72 102   0 101   0   0]\n",
      " [ 67  72   0   1  72   0]\n",
      " [ 28  46   0   0   5   2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(X_train, y_train)\n",
    "y_pred = mnb_classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:25:04.605149100Z",
     "start_time": "2024-03-12T21:25:04.576734900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       581\n",
      "           1       0.84      0.97      0.90       695\n",
      "           2       0.85      0.54      0.66       159\n",
      "           3       0.90      0.81      0.85       275\n",
      "           4       0.91      0.82      0.86       224\n",
      "           5       0.97      0.42      0.59        66\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.89      0.75      0.80      2000\n",
      "weighted avg       0.88      0.87      0.87      2000\n",
      "\n",
      "[[549  19   1  10   2   0]\n",
      " [  4 675  14   1   0   1]\n",
      " [ 10  60  86   3   0   0]\n",
      " [ 26  22   0 224   3   0]\n",
      " [ 18  12   0  10 184   0]\n",
      " [  5  18   0   1  14  28]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate with test set\n",
    "y_pred = logreg_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:26:00.951289100Z",
     "start_time": "2024-03-12T21:26:00.934288300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.83       581\n",
      "           1       0.65      0.99      0.79       695\n",
      "           2       1.00      0.07      0.13       159\n",
      "           3       0.94      0.37      0.53       275\n",
      "           4       0.91      0.36      0.51       224\n",
      "           5       1.00      0.03      0.06        66\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.87      0.46      0.47      2000\n",
      "weighted avg       0.79      0.71      0.66      2000\n",
      "\n",
      "[[544  36   0   0   1   0]\n",
      " [  4 691   0   0   0   0]\n",
      " [ 28 120  11   0   0   0]\n",
      " [ 73 101   0 101   0   0]\n",
      " [ 68  70   0   6  80   0]\n",
      " [ 13  44   0   0   7   2]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mnb_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:26:02.675927800Z",
     "start_time": "2024-03-12T21:26:02.644922100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
