{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already lowercased and lacks punctuation. We will tokenize the text and remove stopwords, as well as apply lemmatization to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[feel, awful, job, get, position, succeed, hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[im, alone, feel, awful]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ive, probably, mentioned, really, feel, proud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[feeling, little, low, day, back]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[beleive, much, sensitive, people, feeling, te...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  [feel, awful, job, get, position, succeed, hap...      0\n",
       "1                           [im, alone, feel, awful]      0\n",
       "2  [ive, probably, mentioned, really, feel, proud...      1\n",
       "3                  [feeling, little, low, day, back]      0\n",
       "4  [beleive, much, sensitive, people, feeling, te...      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "data = pd.read_json('./data/data.jsonl', lines=True)\n",
    "def pre_process_data(dataset):\n",
    "\n",
    "    #tokenize\n",
    "    dataset['text'] = dataset['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data = pre_process_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:09:31.882317900Z",
     "start_time": "2024-03-12T21:09:28.844193200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_json('./data/test.jsonl', lines=True)\n",
    "train_data = pd.read_json('./data/train.jsonl', lines=True)\n",
    "validation_data = pd.read_json('./data/validation.jsonl', lines=True)\n",
    "\n",
    "test_data = pre_process_data(test_data)\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Vectorization (secção possívelmente temporária, mas queria experimentar as cenas de tf_idf depois do pré-processamento)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:46.201324100Z",
     "start_time": "2024-03-12T21:02:32.893616Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416809, 40000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=40000)\n",
    "X = vectorizer.fit_transform(data['text'].apply(lambda x: ' '.join(x)))\n",
    "# good idea to use two-grams??\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:48.301712700Z",
     "start_time": "2024-03-12T21:02:48.254918600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaron' 'ab' ... 'zoom class' 'zoom grey' 'zumba']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:19:44.864392500Z",
     "start_time": "2024-03-12T21:19:44.374282600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "X_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "X_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:19:45.558978900Z",
     "start_time": "2024-03-12T21:19:45.536984400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = train_data['label']\n",
    "y_val = validation_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    5362\n",
      "0    4666\n",
      "3    2159\n",
      "4    1937\n",
      "2    1304\n",
      "5     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check class distribution in training data\n",
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a substantial class imbalance (a full order of magnitude between most common and least common class), we will use SMOTE to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    5362\n",
      "3    5362\n",
      "2    5362\n",
      "5    5362\n",
      "4    5362\n",
      "1    5362\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42, sampling_strategy='auto', k_neighbors=10)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1. Model Selection\n",
    "Aqui também só estava a querer espetar modelos para começar a ver o que dá que ainda não sei que features vão ser usadas:\n",
    "tf-idf, word embeddings, ???, features mais feitas à mão?\n",
    "\n",
    "\n",
    "Isto pelos vistos é uma cena, que não implementei (ainda..)\n",
    "\n",
    "\"The validation set uses a subset of the training data to provide an unbiased evaluation of a model. The validation data set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it. It is in this phase that hyperparameter tuning occurs.\"\n",
    "\n",
    "Wikipedia:\n",
    "The basic process of using a validation data set for model selection (as part of training data set, validation data set, and test data set) is:\n",
    "\n",
    "Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.\n",
    "\n",
    "An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:25:02.914309600Z",
     "start_time": "2024-03-12T21:24:56.981788400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       550\n",
      "           1       0.92      0.92      0.92       704\n",
      "           2       0.80      0.88      0.84       178\n",
      "           3       0.92      0.89      0.91       275\n",
      "           4       0.86      0.82      0.84       212\n",
      "           5       0.74      0.81      0.78        81\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.86      0.88      0.87      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n",
      "[[511  10   4   9  13   3]\n",
      " [  9 646  35   4   3   7]\n",
      " [  2  16 157   2   1   0]\n",
      " [ 14   9   1 246   5   0]\n",
      " [  7  13   0   6 173  13]\n",
      " [  2   6   0   1   6  66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "y_pred = logreg_classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:25:04.605149100Z",
     "start_time": "2024-03-12T21:25:04.576734900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90       550\n",
      "           1       0.94      0.86      0.90       704\n",
      "           2       0.71      0.87      0.78       178\n",
      "           3       0.83      0.87      0.85       275\n",
      "           4       0.80      0.83      0.81       212\n",
      "           5       0.62      0.84      0.71        81\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.80      0.86      0.83      2000\n",
      "weighted avg       0.87      0.86      0.87      2000\n",
      "\n",
      "[[479  10  13  20  19   9]\n",
      " [ 11 608  42  20   9  14]\n",
      " [  4  15 155   1   1   2]\n",
      " [  9   6   5 240  12   3]\n",
      " [  9   4   2   8 175  14]\n",
      " [  3   5   0   1   4  68]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(X_train, y_train)\n",
    "y_pred = mnb_classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:26:00.951289100Z",
     "start_time": "2024-03-12T21:26:00.934288300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       581\n",
      "           1       0.93      0.90      0.91       695\n",
      "           2       0.72      0.86      0.79       159\n",
      "           3       0.89      0.89      0.89       275\n",
      "           4       0.90      0.84      0.87       224\n",
      "           5       0.69      0.83      0.75        66\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.84      0.88      0.86      2000\n",
      "weighted avg       0.90      0.89      0.90      2000\n",
      "\n",
      "[[539  15   2  17   7   1]\n",
      " [  6 624  49   4   2  10]\n",
      " [  3  16 137   3   0   0]\n",
      " [ 11  11   2 245   6   0]\n",
      " [ 12   3   0   7 188  14]\n",
      " [  2   4   0   0   5  55]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate with test set\n",
    "y_pred = logreg_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:26:02.675927800Z",
     "start_time": "2024-03-12T21:26:02.644922100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       581\n",
      "           1       0.93      0.85      0.89       695\n",
      "           2       0.62      0.86      0.72       159\n",
      "           3       0.85      0.86      0.86       275\n",
      "           4       0.86      0.81      0.83       224\n",
      "           5       0.54      0.88      0.67        66\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.79      0.86      0.81      2000\n",
      "weighted avg       0.87      0.86      0.86      2000\n",
      "\n",
      "[[515  12  10  23  12   9]\n",
      " [ 10 589  63   8   8  17]\n",
      " [  5  12 136   2   1   3]\n",
      " [ 12  11   6 237   8   1]\n",
      " [ 11   4   2   7 181  19]\n",
      " [  1   4   1   1   1  58]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mnb_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
