{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already lowercased and lacks punctuation. We will tokenize the text and remove stopwords, as well as apply lemmatization to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\AppData\\Local\\Temp\\ipykernel_37480\\3172380938.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fabio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[feel, awful, job, get, position, succeed, hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[im, alone, feel, awful]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ive, probably, mentioned, really, feel, proud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[feeling, little, low, day, back]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[beleive, much, sensitive, people, feeling, te...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  [feel, awful, job, get, position, succeed, hap...      0\n",
       "1                           [im, alone, feel, awful]      0\n",
       "2  [ive, probably, mentioned, really, feel, proud...      1\n",
       "3                  [feeling, little, low, day, back]      0\n",
       "4  [beleive, much, sensitive, people, feeling, te...      2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "data = pd.read_json('./data/data.jsonl', lines=True)\n",
    "def pre_process_data(dataset):\n",
    "    #tokenize\n",
    "    dataset['text'] = dataset['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data = pre_process_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:09:31.882317900Z",
     "start_time": "2024-03-12T21:09:28.844193200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [im, feeling, rather, rotten, im, ambitious, r...\n",
      "1                      [im, updating, blog, feel, shitty]\n",
      "2       [never, make, separate, ever, want, feel, like...\n",
      "3       [left, bouquet, red, yellow, tulip, arm, feeli...\n",
      "4                            [feeling, little, vain, one]\n",
      "                              ...                        \n",
      "1995    [keep, feeling, like, someone, unkind, wrong, ...\n",
      "1996    [im, feeling, little, cranky, negative, doctor...\n",
      "1997    [feel, useful, people, give, great, feeling, a...\n",
      "1998    [im, feeling, comfortable, derby, feel, though...\n",
      "1999    [feel, weird, meet, w, people, text, like, don...\n",
      "Name: text, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json('./data/test.jsonl', lines=True)\n",
    "train_data = pd.read_json('./data/train.jsonl', lines=True)\n",
    "validation_data = pd.read_json('./data/validation.jsonl', lines=True)\n",
    "\n",
    "test_data = pre_process_data(test_data)\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)\n",
    "\n",
    "print(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Vectorization\n",
    "(secção possívelmente temporária, mas queria experimentar as cenas de tf_idf depois do pré-processamento) -- matos\n",
    "não acho que convenha ser temporaria, dado que efetivamente melhora os resultados ihihihi, e é uma prática comum e recomendada pelo que estivemos a ver -- ines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertunning for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:46.201324100Z",
     "start_time": "2024-03-12T21:02:32.893616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidfVectorizer(data, train_data, validation_data, test_data):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=40000)\n",
    "    # good idea to use two-grams??\n",
    "    # print(X.shape)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "    \n",
    "    # print(vectorizer.get_feature_names_out())\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW model\n",
    "Count vectorizer which is an implementation of the BOW model.\n",
    "\n",
    "The disadvantage of the BOW model is it does not consider the sequence of words, and as language does involve sequence and context, sometimes the BOW model might not be a good fit for the best-case scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def countVectorizer(data, train_data, validation_data, test_data):\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=40000)\n",
    "\n",
    "    x_train= vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc 2 Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document 2 Vector training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x29d7b169780>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "\n",
    "def train_doc2vec(train_data):\n",
    "    max_epochs = 100\n",
    "    vec_size = 20\n",
    "    alpha = 0.025\n",
    "\n",
    "    tagged_data = [TaggedDocument(words=doc, tags=[str(label)]) for doc, label in zip(train_data['text'], train_data['label'])]\n",
    "\n",
    "    model = Doc2Vec(vector_size=vec_size,\n",
    "                    alpha=alpha,\n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm=1)\n",
    "    \n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=50)\n",
    "    print(model.epochs)\n",
    "    model.save(\"d2v_v50.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    return model\n",
    "\n",
    "train_doc2vec(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def doc2vec(data, train_data, validation_data, test_data):\n",
    "\n",
    "    # training the doc2vec model\n",
    "    #model = train_doc2vec(train_data)\n",
    "    model = Doc2Vec.load(\"d2v_v3.model\")\n",
    "    \n",
    "    x_train = [model.infer_vector(doc) for doc in train_data['text']]\n",
    "    x_val = [model.infer_vector(doc) for doc in validation_data['text']]\n",
    "    x_test = [model.infer_vector(doc) for doc in test_data['text']]\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def lala(model, data):\n",
    "    vectors = []\n",
    "    for doc in data['text']:\n",
    "        document = []\n",
    "        for word in doc:\n",
    "            if word in model.wv:\n",
    "                document.append(model.wv[word])\n",
    "        vectors.append(document)\n",
    "    return vectors\n",
    "\n",
    "# test [d,c,c]  | label\n",
    "########################\n",
    "## test \n",
    "## [ [] [] [] ] | label\n",
    "\n",
    "def wordEmbeddingsVectorizer(data):\n",
    "\n",
    "    data_model = Word2Vec(data[\"text\"], vector_size=3, window=10, min_count=2, workers=10, sg=1)\n",
    "    \n",
    "    x_train = lala(data_model, train_data)\n",
    "    x_val =lala(data_model, validation_data)\n",
    "    x_test = lala(data_model, test_data)\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose vectorizer (featurizer)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = tfidfVectorizer(data, train_data, validation_data, test_data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = countVectorizer(data, train_data, validation_data, test_data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = wordEmbeddingsVectorizer(data)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = doc2vec(data, train_data, validation_data, test_data)\n",
    "\n",
    "#print(x_train)\n",
    "\n",
    "## Apply SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy='auto', k_neighbors=10)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1. Model Selection\n",
    "Aqui também só estava a querer espetar modelos para começar a ver o que dá que ainda não sei que features vão ser usadas:\n",
    "tf-idf, word embeddings, ???, features mais feitas à mão?\n",
    "\n",
    "\n",
    "Isto pelos vistos é uma cena, que não implementei (ainda..)\n",
    "\n",
    "\"The validation set uses a subset of the training data to provide an unbiased evaluation of a model. The validation data set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it. It is in this phase that hyperparameter tuning occurs.\"\n",
    "\n",
    "Wikipedia:\n",
    "The basic process of using a validation data set for model selection (as part of training data set, validation data set, and test data set) is:\n",
    "\n",
    "Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.\n",
    "\n",
    "An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:25:02.914309600Z",
     "start_time": "2024-03-12T21:24:56.981788400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86       550\n",
      "           1       0.91      0.86      0.88       704\n",
      "           2       0.72      0.78      0.75       178\n",
      "           3       0.77      0.76      0.76       275\n",
      "           4       0.75      0.77      0.76       212\n",
      "           5       0.66      0.75      0.71        81\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.78      0.80      0.79      2000\n",
      "weighted avg       0.83      0.83      0.83      2000\n",
      "\n",
      "[[475  12   3  32  23   5]\n",
      " [ 27 604  42  14  10   7]\n",
      " [ 11  21 139   5   2   0]\n",
      " [ 22  15   7 209  16   6]\n",
      " [ 16   8   1  11 163  13]\n",
      " [  9   7   0   2   2  61]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "logreg_classifier.fit(x_train, y_train)\n",
    "y_pred = logreg_classifier.predict(x_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:25:04.605149100Z",
     "start_time": "2024-03-12T21:25:04.576734900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mnb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmnb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mnb_classifier\u001b[38;5;241m.\u001b[39mpredict(x_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:776\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    774\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 776\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:898\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 898\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1418\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1415\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(x_train, y_train)\n",
    "y_pred = mnb_classifier.predict(x_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8215\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       550\n",
      "           1       0.90      0.85      0.88       704\n",
      "           2       0.71      0.79      0.74       178\n",
      "           3       0.78      0.75      0.76       275\n",
      "           4       0.75      0.76      0.75       212\n",
      "           5       0.67      0.77      0.72        81\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.77      0.80      0.78      2000\n",
      "weighted avg       0.83      0.82      0.82      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[477  13   3  28  24   5]\n",
      " [ 29 597  48  14  10   6]\n",
      " [ 11  21 140   4   2   0]\n",
      " [ 26  15   5 206  18   5]\n",
      " [ 17   8   2  10 161  14]\n",
      " [ 10   6   0   2   1  62]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "y_pred = svm_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Algorithms\n",
    "\n",
    "Testing with some boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Evaluation:\n",
      "Accuracy: 0.592\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63       550\n",
      "           1       0.64      0.76      0.69       704\n",
      "           2       0.45      0.36      0.40       178\n",
      "           3       0.56      0.45      0.50       275\n",
      "           4       0.47      0.42      0.44       212\n",
      "           5       0.43      0.35      0.38        81\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.53      0.49      0.51      2000\n",
      "weighted avg       0.58      0.59      0.58      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[344 104  18  43  36   5]\n",
      " [ 79 535  39  15  26  10]\n",
      " [ 25  59  64  14  10   6]\n",
      " [ 54  60   7 124  20  10]\n",
      " [ 32  55   9  21  89   6]\n",
      " [ 11  23   5   4  10  28]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# XGBoost Classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for XGBoost\n",
    "accuracy_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "report_xgb = classification_report(y_val, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_val, y_pred_xgb)\n",
    "\n",
    "# Print the evaluation metrics for XGBoost\n",
    "print(\"XGBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_xgb)\n",
    "print(\"Classification Report:\\n\", report_xgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 32172, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "\n",
      "LightGBM Classifier Evaluation:\n",
      "Accuracy: 0.587\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63       550\n",
      "           1       0.67      0.72      0.69       704\n",
      "           2       0.44      0.43      0.44       178\n",
      "           3       0.50      0.42      0.45       275\n",
      "           4       0.46      0.46      0.46       212\n",
      "           5       0.45      0.42      0.44        81\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.52      0.51      0.52      2000\n",
      "weighted avg       0.58      0.59      0.58      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[342  88  24  48  40   8]\n",
      " [ 83 508  42  27  29  15]\n",
      " [ 24  45  77  19   9   4]\n",
      " [ 50  56  14 115  31   9]\n",
      " [ 36  44  11  18  98   5]\n",
      " [  7  21   6   5   8  34]]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier()\n",
    "lgb_classifier.fit(x_train, y_train)\n",
    "y_pred_lgb = lgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for LightGBM\n",
    "accuracy_lgb = accuracy_score(y_val, y_pred_lgb)\n",
    "report_lgb = classification_report(y_val, y_pred_lgb)\n",
    "conf_matrix_lgb = confusion_matrix(y_val, y_pred_lgb)\n",
    "\n",
    "# Print the evaluation metrics for LightGBM\n",
    "print(\"\\nLightGBM Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_lgb)\n",
    "print(\"Classification Report:\\n\", report_lgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Evaluation:\n",
      "Accuracy: 0.5015\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.47      0.53       550\n",
      "           1       0.66      0.61      0.63       704\n",
      "           2       0.32      0.47      0.38       178\n",
      "           3       0.45      0.39      0.42       275\n",
      "           4       0.33      0.44      0.38       212\n",
      "           5       0.22      0.40      0.28        81\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.43      0.46      0.44      2000\n",
      "weighted avg       0.54      0.50      0.51      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[259  84  43  60  70  34]\n",
      " [ 73 427  82  36  50  36]\n",
      " [ 17  42  83  12  17   7]\n",
      " [ 34  40  30 108  42  21]\n",
      " [ 29  35  15  21  94  18]\n",
      " [  9  18  10   3   9  32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost Classifier\n",
    "adaboost_classifier = AdaBoostClassifier()\n",
    "adaboost_classifier.fit(x_train, y_train)\n",
    "y_pred_adaboost = adaboost_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_val, y_pred_adaboost)\n",
    "report_adaboost = classification_report(y_val, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(y_val, y_pred_adaboost)\n",
    "\n",
    "# Print the evaluation metrics for AdaBoost\n",
    "print(\"AdaBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_adaboost)\n",
    "print(\"Classification Report:\\n\", report_adaboost)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_adaboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bagging Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:26:00.951289100Z",
     "start_time": "2024-03-12T21:26:00.934288300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67       581\n",
      "           1       0.76      0.56      0.64       695\n",
      "           2       0.30      0.53      0.39       159\n",
      "           3       0.48      0.48      0.48       275\n",
      "           4       0.44      0.54      0.48       224\n",
      "           5       0.22      0.65      0.33        66\n",
      "\n",
      "    accuracy                           0.56      2000\n",
      "   macro avg       0.49      0.56      0.50      2000\n",
      "weighted avg       0.63      0.56      0.58      2000\n",
      "\n",
      "[[351  50  43  44  55  38]\n",
      " [ 55 386  98  55  53  48]\n",
      " [  9  25  84  14  17  10]\n",
      " [ 36  26  29 132  24  28]\n",
      " [ 17  17  17  26 121  26]\n",
      " [  3   4   6   5   5  43]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate with test set\n",
    "y_pred = logreg_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:26:02.675927800Z",
     "start_time": "2024-03-12T21:26:02.644922100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.90       581\n",
      "           1       0.86      0.91      0.88       695\n",
      "           2       0.74      0.65      0.69       159\n",
      "           3       0.88      0.77      0.82       275\n",
      "           4       0.89      0.79      0.83       224\n",
      "           5       0.76      0.59      0.67        66\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.83      0.77      0.80      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n",
      "[[542  15   1  13   9   1]\n",
      " [ 15 633  35   3   4   5]\n",
      " [ 11  40 104   4   0   0]\n",
      " [ 29  29   1 211   5   0]\n",
      " [ 23  11   0   7 177   6]\n",
      " [  9  12   0   1   5  39]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mnb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.62      0.68       581\n",
      "           1       0.76      0.57      0.65       695\n",
      "           2       0.32      0.54      0.40       159\n",
      "           3       0.50      0.49      0.49       275\n",
      "           4       0.45      0.55      0.49       224\n",
      "           5       0.23      0.62      0.33        66\n",
      "\n",
      "    accuracy                           0.57      2000\n",
      "   macro avg       0.50      0.57      0.51      2000\n",
      "weighted avg       0.63      0.57      0.59      2000\n",
      "\n",
      "[[361  52  43  41  53  31]\n",
      " [ 57 397  94  48  52  47]\n",
      " [  8  26  86  13  16  10]\n",
      " [ 36  25  27 134  27  26]\n",
      " [ 16  19  15  26 123  25]\n",
      " [  3   5   7   5   5  41]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.67      0.66       581\n",
      "           1       0.63      0.70      0.67       695\n",
      "           2       0.39      0.33      0.36       159\n",
      "           3       0.54      0.41      0.47       275\n",
      "           4       0.47      0.43      0.45       224\n",
      "           5       0.41      0.44      0.43        66\n",
      "\n",
      "    accuracy                           0.58      2000\n",
      "   macro avg       0.51      0.50      0.50      2000\n",
      "weighted avg       0.58      0.58      0.58      2000\n",
      "\n",
      "[[389 107  14  26  39   6]\n",
      " [ 88 489  39  37  29  13]\n",
      " [ 22  57  52  13  10   5]\n",
      " [ 58  53  15 114  27   8]\n",
      " [ 39  55   7  17  97   9]\n",
      " [  7  14   6   4   6  29]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       581\n",
      "           1       0.65      0.71      0.68       695\n",
      "           2       0.41      0.40      0.41       159\n",
      "           3       0.51      0.42      0.46       275\n",
      "           4       0.48      0.48      0.48       224\n",
      "           5       0.42      0.45      0.43        66\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.52      0.52      0.52      2000\n",
      "weighted avg       0.59      0.60      0.59      2000\n",
      "\n",
      "[[383  93  21  37  41   6]\n",
      " [ 78 492  40  37  35  13]\n",
      " [ 19  45  64  15  12   4]\n",
      " [ 48  61  16 116  25   9]\n",
      " [ 27  50  10  20 107  10]\n",
      " [ 11  13   5   4   3  30]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.48      0.55       581\n",
      "           1       0.63      0.55      0.59       695\n",
      "           2       0.25      0.47      0.33       159\n",
      "           3       0.46      0.37      0.41       275\n",
      "           4       0.33      0.44      0.38       224\n",
      "           5       0.20      0.47      0.28        66\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.42      0.46      0.42      2000\n",
      "weighted avg       0.54      0.48      0.50      2000\n",
      "\n",
      "[[278  98  57  44  70  34]\n",
      " [ 68 380  94  44  66  43]\n",
      " [  9  35  75  14  18   8]\n",
      " [ 30  40  38 103  38  26]\n",
      " [ 30  38  23  19  98  16]\n",
      " [  9   9   8   2   7  31]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = adaboost_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
