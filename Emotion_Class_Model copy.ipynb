{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already lowercased and lacks punctuation. We will tokenize the text and remove stopwords, as well as apply lemmatization to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\inesc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\inesc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\inesc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[feel, awful, s, job, position, succeed, just,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[im, alone, feel, awful]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ive, probably, mentioned, but, really, feel, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[feeling, little, low, day]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[beleive, sensitive, people, feeling, tend, co...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  [feel, awful, s, job, position, succeed, just,...      0\n",
       "1                           [im, alone, feel, awful]      0\n",
       "2  [ive, probably, mentioned, but, really, feel, ...      1\n",
       "3                        [feeling, little, low, day]      0\n",
       "4  [beleive, sensitive, people, feeling, tend, co...      2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "#words to keep: no couldnt cry not cant cannot nor except nobody off but serious enough nothing alone down only without\n",
    "\n",
    "\n",
    "data = pd.read_json('./data/data.jsonl', lines=True)\n",
    "\n",
    "\n",
    "def pre_process_data(dataset):\n",
    "    #tokenize\n",
    "    dataset['text'] = dataset['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    #remove stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "    #print(my_stop_words)\n",
    "    words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody', 'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without','hereby'])\n",
    "    my_stop_words = my_stop_words - words_to_keep\n",
    "    \n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in my_stop_words])\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data = pre_process_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:09:31.882317900Z",
     "start_time": "2024-03-12T21:09:28.844193200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'feel', 'extremely', 'comfortable', 'group', 'people', 'dont', 'need', 'hide']\n",
      "0        [im, feeling, rotten, im, not, ambitious, right]\n",
      "1                      [im, updating, blog, feel, shitty]\n",
      "2       [make, separate, don, t, want, feel, like, m, ...\n",
      "3       [left, bouquet, red, yellow, tulip, arm, feeli...\n",
      "4                            [feeling, little, vain, did]\n",
      "                              ...                        \n",
      "1995    [just, feeling, like, unkind, doing, wrong, th...\n",
      "1996    [im, feeling, little, cranky, negative, doctor...\n",
      "1997    [feel, useful, people, give, great, feeling, a...\n",
      "1998    [im, feeling, comfortable, derby, feel, start,...\n",
      "1999    [feel, weird, meet, w, people, text, but, like...\n",
      "Name: text, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json('./data/test.jsonl', lines=True)\n",
    "train_data = pd.read_json('./data/train.jsonl', lines=True)\n",
    "validation_data = pd.read_json('./data/validation.jsonl', lines=True)\n",
    "\n",
    "test_data = pre_process_data(test_data)\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)\n",
    "print(test_data['text'][13])\n",
    "\n",
    "print(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Vectorization\n",
    "(secção possívelmente temporária, mas queria experimentar as cenas de tf_idf depois do pré-processamento) -- matos\n",
    "não acho que convenha ser temporaria, dado que efetivamente melhora os resultados ihihihi, e é uma prática comum e recomendada pelo que estivemos a ver -- ines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertunning for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:46.201324100Z",
     "start_time": "2024-03-12T21:02:32.893616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tfidfVectorizer(data, train_data, validation_data, test_data):\n",
    "    vectorizer = TfidfVectorizer(stop_words=list(my_stop_words), ngram_range=(1,2), max_features=40000)\n",
    "    # good idea to use two-grams??\n",
    "    # print(X.shape)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "    \n",
    "    # print(vectorizer.get_feature_names_out())\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW model\n",
    "Count vectorizer which is an implementation of the BOW model.\n",
    "\n",
    "The disadvantage of the BOW model is it does not consider the sequence of words, and as language does involve sequence and context, sometimes the BOW model might not be a good fit for the best-case scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def countVectorizer(data, train_data, validation_data, test_data):\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words=my_stop_words, ngram_range=(1,2), max_features=40000)\n",
    "\n",
    "    x_train= vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from gensim.models import Doc2Vec\n",
    "# from gensim.models.doc2vec import TaggedDocument\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# y_train = train_data['label']\n",
    "# y_val = validation_data['label']\n",
    "\n",
    "# def objective(trial):\n",
    "#     vector_size = trial.suggest_int(\"vector_size\", 50, 300)\n",
    "#     window = trial.suggest_int(\"window\", 3, 15)\n",
    "#     min_count = trial.suggest_int(\"min_count\", 1, 10)\n",
    "#     epochs = trial.suggest_int(\"epochs\", 10, 50)\n",
    "    \n",
    "#     # Train Doc2Vec model\n",
    "#     tagged_data = [TaggedDocument(words=doc, tags=[str(label)]) for doc, label in zip(train_data['text'], train_data['label'])]\n",
    "#     doc2vec_model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
    "#     doc2vec_model.build_vocab(tagged_data)\n",
    "#     doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "    \n",
    "#     # Prepare document vectors for training and test sets\n",
    "#     X_train_vecs  = [doc2vec_model.infer_vector(doc) for doc in train_data['text']]\n",
    "#     X_val_vecs = [doc2vec_model.infer_vector(doc) for doc in validation_data['text']]\n",
    "    \n",
    "#     # Train Logistic Regression classifier\n",
    "#     classifier = LogisticRegression(max_iter=1000)\n",
    "#     classifier.fit(X_train_vecs, y_train)\n",
    "    \n",
    "#     # Evaluate on test set\n",
    "#     y_pred = classifier.predict(X_val_vecs)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Define the study object and optimize the objective function\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Print the best hyperparameters found\n",
    "# best_params = study.best_params\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# # Train the final model with the best hyperparameters\n",
    "# best_vector_size = best_params[\"vector_size\"]\n",
    "# best_window = best_params[\"window\"]\n",
    "# best_min_count = best_params[\"min_count\"]\n",
    "# best_epochs = best_params[\"epochs\"]\n",
    "\n",
    "# # Train Doc2Vec model with best hyperparameters\n",
    "# tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(train_data[\"text\"])]\n",
    "# best_doc2vec_model = Doc2Vec(vector_size=best_vector_size, window=best_window, min_count=best_min_count, epochs=best_epochs)\n",
    "# best_doc2vec_model.build_vocab(tagged_data)\n",
    "# best_doc2vec_model.train(tagged_data, total_examples=best_doc2vec_model.corpus_count, epochs=best_doc2vec_model.epochs)\n",
    "\n",
    "# # Prepare document vectors for training and test sets with the best Doc2Vec model\n",
    "# X_train_vecs  = [best_doc2vec_model.infer_vector(doc) for doc in train_data['text']]\n",
    "# X_val_vecs = [best_doc2vec_model.infer_vector(doc) for doc in validation_data['text']]\n",
    "\n",
    "# # Train Logistic Regression classifier on the final Doc2Vec vectors\n",
    "# final_classifier = LogisticRegression(max_iter=1000)\n",
    "# final_classifier.fit(X_train_vecs, y_train)\n",
    "\n",
    "# # Evaluate on test set\n",
    "# final_accuracy = final_classifier.score(X_val_vecs, y_val)\n",
    "# print(\"Final accuracy on test set with the best model:\", final_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x215f8f15a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "\n",
    "def train_doc2vec(train_data):\n",
    "    max_epochs = 16\n",
    "    vec_size = 88\n",
    "    alpha = 0.025\n",
    "    window = 4\n",
    "    min_count = 7\n",
    "\n",
    "    tagged_data = [TaggedDocument(words=doc, tags=[str(label)]) for doc, label in zip(train_data['text'], train_data['label'])]\n",
    "\n",
    "    # antes tinha workers definidos\n",
    "    model = Doc2Vec(vector_size=vec_size, window=window, min_count=min_count, epochs=max_epochs)\n",
    "    \n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=50)\n",
    "\n",
    "\n",
    "    model.save(\"d2v_best_stopwords.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    return model\n",
    "\n",
    "train_doc2vec(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:56:31.788595Z",
     "start_time": "2024-03-30T23:56:31.783431Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def doc2vec(data, train_data, validation_data, test_data):\n",
    "\n",
    "    # training the doc2vec model\n",
    "    #model = train_doc2vec(train_data)\n",
    "    model = Doc2Vec.load(\"d2v_best_stopwords.model\")\n",
    "    \n",
    "    x_train = [model.infer_vector(doc) for doc in train_data['text']]\n",
    "    x_val = [model.infer_vector(doc) for doc in validation_data['text']]\n",
    "    x_test = [model.infer_vector(doc) for doc in test_data['text']]\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:54:28.516328Z",
     "start_time": "2024-03-30T23:54:28.508772Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def lala(model, data):\n",
    "    vectors = []\n",
    "    for doc in data['text']:\n",
    "        document = []\n",
    "        for word in doc:\n",
    "            if word in model.wv:\n",
    "                document.append(model.wv[word])\n",
    "        vectors.append(document)\n",
    "    return vectors\n",
    "\n",
    "# test [d,c,c]  | label\n",
    "########################\n",
    "## test \n",
    "## [ [] [] [] ] | label\n",
    "\n",
    "def wordEmbeddingsVectorizer(data):\n",
    "\n",
    "    data_model = Word2Vec(data[\"text\"], vector_size=3, window=10, min_count=2, workers=10, sg=1)\n",
    "    \n",
    "    x_train = lala(data_model, train_data)\n",
    "    x_val =lala(data_model, validation_data)\n",
    "    x_test = lala(data_model, test_data)\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - Word embeddings Matos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    \n",
    "    # split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "        finally:\n",
    "            i += 1\n",
    "    \n",
    "    # add blanks up to sequence_len, if needed\n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def text_to_mean_vector(embeddings, text):\n",
    "     # split text into tokens\n",
    "    tokens = text.split()\n",
    "\n",
    "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "    total_vector = []\n",
    "    i = 0\n",
    "    while i < len(tokens):   # while there are tokens and did not reach desired sequence length\n",
    "        try:\n",
    "            total_vector.append(embeddings.get_vector(tokens[i]))\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "        finally:\n",
    "            i += 1\n",
    "\n",
    "    return np.mean(total_vector, axis=0)\n",
    "\n",
    "def documents_to_vectors(embeddings, documents, sequence_len):\n",
    "    vectors = []\n",
    "    for doc in documents:\n",
    "        vectors.append(text_to_vector(embeddings, doc, sequence_len))\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "def documents_to_mean_vectors(embeddings, documents):\n",
    "    vectors = []\n",
    "    for doc in documents:\n",
    "        vectors.append(text_to_mean_vector(embeddings, doc))\n",
    "    return np.array(vectors)\n",
    "\n",
    "def wordEmbeddingsVectorizer(data):\n",
    "    \n",
    "    # use twitter word embeddings\n",
    "    model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "    \n",
    "    # Initialize Word2Vec\n",
    "    \n",
    "    train = train_data['text'].apply(lambda x: ' '.join(x))\n",
    "    val = validation_data['text'].apply(lambda x: ' '.join(x))\n",
    "    test = test_data['text'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    x_train = documents_to_vectors(model_glove_twitter, train, 40)\n",
    "    x_val = documents_to_vectors(model_glove_twitter, val, 40)\n",
    "    x_test = documents_to_vectors(model_glove_twitter, test, 40)\n",
    "    \n",
    "    print(x_train.shape, x_val.shape, x_test.shape)\n",
    "    print(x_train[0])\n",
    "    \n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def wordEmbeddingsMeanVectorizer(data):\n",
    "\n",
    "    # use twitter word embeddings\n",
    "    model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "\n",
    "    # Initialize Word2Vec\n",
    "\n",
    "    train = train_data['text'].apply(lambda x: ' '.join(x))\n",
    "    val = validation_data['text'].apply(lambda x: ' '.join(x))\n",
    "    test = test_data['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    x_train = documents_to_mean_vectors(model_glove_twitter, train)\n",
    "    x_val = documents_to_mean_vectors(model_glove_twitter, val)\n",
    "    x_test = documents_to_mean_vectors(model_glove_twitter, test)\n",
    "\n",
    "    # x_train = np.mean(x_train, axis=1)\n",
    "    # x_val = np.mean(x_val, axis=1)\n",
    "    # x_test = np.mean(x_test, axis=1)\n",
    "    #\n",
    "    print(x_train.shape, x_val.shape, x_test.shape)\n",
    "    print(x_train[0])\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:57:13.209485Z",
     "start_time": "2024-03-30T23:56:34.757556Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
      "(16000, 25) (2000, 25) (2000, 25)\n",
      "[-0.35627666  0.37016702  0.28562868 -0.47295332 -0.5652533  -0.24137668\n",
      "  0.8951466  -0.6888234  -0.95984     0.14970261 -0.6163067   0.84833336\n",
      " -3.2683165  -0.29839    -0.04744667  0.45890665  0.69503003 -0.26880667\n",
      "  0.34184667 -1.4003      0.45056033  0.8567834   0.38757202  0.03085001\n",
      " -0.44497335]\n"
     ]
    }
   ],
   "source": [
    "# Choose vectorizer (featurizer)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = tfidfVectorizer(data, train_data, validation_data, test_data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = countVectorizer(data, train_data, validation_data, test_data)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = wordEmbeddingsMeanVectorizer(data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = doc2vec(data, train_data, validation_data, test_data)\n",
    "\n",
    "#print(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply SMOTE\n",
    "smote = SMOTE(random_state=4, sampling_strategy='auto', k_neighbors=18)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertunning number of features (using PCA) for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define parameters to search\n",
    "#     n_components = trial.suggest_int('n_components', 65, 88)\n",
    "    \n",
    "#     # Define the pipeline with PCA and a classifier\n",
    "#     pipeline = Pipeline([\n",
    "#         ('pca', PCA(n_components=n_components)),\n",
    "#         ('clf', SVC())\n",
    "#         #('clf', LogisticRegression())\n",
    "#     ])\n",
    "\n",
    "#     # Fit the pipeline\n",
    "#     pipeline.fit(x_train, y_train)\n",
    "    \n",
    "#     # Evaluate on the validation set\n",
    "#     y_pred = pipeline.predict(x_val)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Optimize hyperparameters\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get the best parameters found\n",
    "# best_params = study.best_params\n",
    "\n",
    "# # Get the best model\n",
    "# best_n_components = best_params['n_components']\n",
    "# best_model = Pipeline([\n",
    "#     ('pca', PCA(n_components=best_n_components)),\n",
    "#     ('clf', )  \n",
    "# ])\n",
    "# best_model.fit(x_train, y_train)\n",
    "\n",
    "# # Use the best model for prediction\n",
    "# y_pred = best_model.predict(x_test)\n",
    "\n",
    "# # Evaluate the best model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1. Model Selection\n",
    "Aqui também só estava a querer espetar modelos para começar a ver o que dá que ainda não sei que features vão ser usadas:\n",
    "tf-idf, word embeddings, ???, features mais feitas à mão?\n",
    "\n",
    "\n",
    "Isto pelos vistos é uma cena, que não implementei (ainda..)\n",
    "\n",
    "\"The validation set uses a subset of the training data to provide an unbiased evaluation of a model. The validation data set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it. It is in this phase that hyperparameter tuning occurs.\"\n",
    "\n",
    "Wikipedia:\n",
    "The basic process of using a validation data set for model selection (as part of training data set, validation data set, and test data set) is:\n",
    "\n",
    "Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.\n",
    "\n",
    "An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model hypertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-01 11:41:53,457] A new study created in memory with name: no-name-6469883b-e42d-44e8-9384-d37484e49d1d\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:41:55,390] Trial 0 finished with value: 0.808 and parameters: {'solver': 'lbfgs', 'C': 0.08716883426232369}. Best is trial 0 with value: 0.808.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:41:57,715] Trial 1 finished with value: 0.8115 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.013011548692604527}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:42:01,758] Trial 2 finished with value: 0.808 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.3264564943772404}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:42:04,533] Trial 3 finished with value: 0.8055 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.20461937646665881}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:42:09,653] Trial 4 finished with value: 0.8 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.04403118708142428}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:42:13,303] Trial 5 finished with value: 0.7995 and parameters: {'solver': 'lbfgs', 'C': 0.7268644361714225}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:42:18,319] Trial 6 finished with value: 0.8065 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.23315067780034235}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:42:43,659] Trial 7 finished with value: 0.7955 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 9.351877091923791}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:05,605] Trial 8 finished with value: 0.8 and parameters: {'solver': 'saga', 'penalty': 'l1', 'C': 0.14454914996297802}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:10,410] Trial 9 finished with value: 0.798 and parameters: {'solver': 'lbfgs', 'C': 1.48370721187854}. Best is trial 1 with value: 0.8115.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:13,560] Trial 10 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.011630248986512999}. Best is trial 10 with value: 0.812.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:16,011] Trial 11 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.012554971780493179}. Best is trial 10 with value: 0.812.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:18,225] Trial 12 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.010082742930939246}. Best is trial 10 with value: 0.812.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:20,867] Trial 13 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03316360093523545}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:25,431] Trial 14 finished with value: 0.7945 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.03566578169416327}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:28,251] Trial 15 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.0352197660907379}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:31,060] Trial 16 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.036577134477708564}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:37,235] Trial 17 finished with value: 0.8045 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.07647279003571684}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:38,965] Trial 18 finished with value: 0.8115 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.025206950046076825}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:42,284] Trial 19 finished with value: 0.8 and parameters: {'solver': 'lbfgs', 'C': 0.6723289152701132}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:45,366] Trial 20 finished with value: 0.8105 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.07826059600564673}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:47,821] Trial 21 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.025531098794164344}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:50,702] Trial 22 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.04420323317153926}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:53,037] Trial 23 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.021787774179105386}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:56,030] Trial 24 finished with value: 0.8115 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.10864995727266787}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:43:58,861] Trial 25 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.049234763259585146}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:02,341] Trial 26 finished with value: 0.784 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.019617533143734253}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:06,935] Trial 27 finished with value: 0.806 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 7.763630935180855}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:12,541] Trial 28 finished with value: 0.796 and parameters: {'solver': 'lbfgs', 'C': 3.216949654570786}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:14,428] Trial 29 finished with value: 0.8085 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.06733547944531401}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:16,917] Trial 30 finished with value: 0.807 and parameters: {'solver': 'lbfgs', 'C': 0.13397339002570718}. Best is trial 13 with value: 0.813.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:19,989] Trial 31 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03784347512520397}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:22,629] Trial 32 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.028730579030076187}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:25,003] Trial 33 finished with value: 0.8115 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.01731870304627057}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:28,017] Trial 34 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.05755183753496904}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:32,285] Trial 35 finished with value: 0.8075 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.3793615148773512}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:35,472] Trial 36 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03364282173834229}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:38,029] Trial 37 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.015504915546421947}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:42,101] Trial 38 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.10273228259297121}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:44,310] Trial 39 finished with value: 0.81 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.04501055589708241}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:47,187] Trial 40 finished with value: 0.8055 and parameters: {'solver': 'lbfgs', 'C': 0.21988405389256147}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:50,457] Trial 41 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.04248246316307168}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:53,256] Trial 42 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03448399518536089}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:44:56,342] Trial 43 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.05975653589202467}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:01,481] Trial 44 finished with value: 0.8095 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.17288479201026213}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:05,299] Trial 45 finished with value: 0.776 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.016743258475988294}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:08,449] Trial 46 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03232527416929237}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:11,885] Trial 47 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.10232933541207775}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:13,918] Trial 48 finished with value: 0.812 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.013908038514062195}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:26,204] Trial 49 finished with value: 0.806 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.3195654544903297}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:28,729] Trial 50 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.02376545123081066}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:31,646] Trial 51 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03741795767588339}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:34,216] Trial 52 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.039776597586836}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:36,979] Trial 53 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.05293617631779699}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:40,827] Trial 54 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.04029530476760967}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:44,292] Trial 55 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.08041663224380514}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:47,537] Trial 56 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.04145704535055124}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:50,279] Trial 57 finished with value: 0.807 and parameters: {'solver': 'lbfgs', 'C': 0.13984291170749896}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:53,433] Trial 58 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.02772555093956897}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:45:59,008] Trial 59 finished with value: 0.777 and parameters: {'solver': 'saga', 'penalty': 'l1', 'C': 0.020327345840338656}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:02,028] Trial 60 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.012355342566008287}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:05,129] Trial 61 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.04194255601747685}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:08,202] Trial 62 finished with value: 0.8115 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.06657957448741951}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:11,196] Trial 63 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.04013509411988473}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:14,035] Trial 64 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.051155117405103284}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:17,210] Trial 65 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.08931701541879825}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:19,933] Trial 66 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.028230822151102997}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:24,306] Trial 67 finished with value: 0.8075 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0731781372969675}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:26,705] Trial 68 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.010229997844402598}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:28,299] Trial 69 finished with value: 0.812 and parameters: {'solver': 'lbfgs', 'C': 0.021583527288918665}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:33,695] Trial 70 finished with value: 0.798 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.03993242324859385}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:36,940] Trial 71 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.06873674430014264}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:39,901] Trial 72 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.040853360394429654}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:42,479] Trial 73 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03210889869129089}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:45,392] Trial 74 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.049057345060553856}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:47,903] Trial 75 finished with value: 0.8115 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.02448533727553615}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:50,769] Trial 76 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.06132168739223218}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:53,831] Trial 77 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03874724912913287}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:55,647] Trial 78 finished with value: 0.8115 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.031742690946127736}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:46:58,137] Trial 79 finished with value: 0.8115 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.01860974775502622}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:01,315] Trial 80 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.046625557182198}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:04,227] Trial 81 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.0411917788669257}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:07,039] Trial 82 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.05655126199731569}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:10,005] Trial 83 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.08872196969894978}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:12,835] Trial 84 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.035339576529725435}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:16,859] Trial 85 finished with value: 0.8085 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.4852177617562366}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:19,445] Trial 86 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.026962248972639085}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:21,279] Trial 87 finished with value: 0.8085 and parameters: {'solver': 'lbfgs', 'C': 0.07356535378589873}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:23,736] Trial 88 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.023154620341136908}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:35,531] Trial 89 finished with value: 0.8065 and parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 2.783960725113066}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:37,505] Trial 90 finished with value: 0.8075 and parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.11263774149314681}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:40,818] Trial 91 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.04239159128933545}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:43,610] Trial 92 finished with value: 0.8135 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03810747980870008}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:45,722] Trial 93 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.015249672531368297}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:48,485] Trial 94 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.030553949097925432}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:51,613] Trial 95 finished with value: 0.8125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.05214745132129341}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:54,452] Trial 96 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.0433352024940502}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:47:57,632] Trial 97 finished with value: 0.811 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.05878123498496633}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:48:00,779] Trial 98 finished with value: 0.813 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03619279760535714}. Best is trial 31 with value: 0.8135.\n",
      "C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\506814322.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10)\n",
      "[I 2024-04-01 11:48:03,456] Trial 99 finished with value: 0.812 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.02687303239306155}. Best is trial 31 with value: 0.8135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'liblinear', 'penalty': 'l2', 'C': 0.03784347512520397}\n"
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define objective function for Optuna\n",
    "# # Define objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     # Define solver\n",
    "#     solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "    \n",
    "#     if solver == 'lbfgs':\n",
    "#         penalty = 'l2'  # LBFGS only supports L2 penalty\n",
    "#     else:\n",
    "#         penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    \n",
    "#     C = trial.suggest_loguniform('C', 0.01, 10)\n",
    "    \n",
    "#     # Instantiate Logistic Regression classifier with hyperparameters\n",
    "#     logreg_classifier = LogisticRegression(max_iter=1000, solver=solver, penalty=penalty, C=C)\n",
    "    \n",
    "#     # Train classifier\n",
    "#     logreg_classifier.fit(x_train, y_train)\n",
    "    \n",
    "#     # Predict on validation set\n",
    "#     y_pred = logreg_classifier.predict(x_val)\n",
    "    \n",
    "#     # Calculate accuracy score\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Create Optuna study\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get best hyperparameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:57:44.308633Z",
     "start_time": "2024-03-30T23:57:43.289263Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.31      0.39       550\n",
      "           1       0.68      0.38      0.49       704\n",
      "           2       0.23      0.48      0.32       178\n",
      "           3       0.29      0.33      0.31       275\n",
      "           4       0.27      0.33      0.30       212\n",
      "           5       0.11      0.41      0.17        81\n",
      "\n",
      "    accuracy                           0.36      2000\n",
      "   macro avg       0.35      0.38      0.33      2000\n",
      "weighted avg       0.47      0.36      0.39      2000\n",
      "\n",
      "[[173  50  51 124  66  86]\n",
      " [ 55 271 176  48  54 100]\n",
      " [ 14  24  86  19  15  20]\n",
      " [ 62  17  22  91  45  38]\n",
      " [ 27  24  24  30  71  36]\n",
      " [ 15  12   7   6   8  33]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# pca = PCA(n_components=77)\n",
    "# x_train = pca.fit_transform(x_train)\n",
    "# x_val = pca.transform(x_val)\n",
    "# x_test = pca.transform(x_test)\n",
    "\n",
    "logreg_classifier = LogisticRegression(solver='liblinear', penalty='l2', C=0.03784347512520397)\n",
    "logreg_classifier.fit(x_train, y_train)\n",
    "y_pred = logreg_classifier.predict(x_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:57:47.417352Z",
     "start_time": "2024-03-30T23:57:47.325689Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# mnb_classifier = MultinomialNB()\n",
    "# mnb_classifier.fit(x_train, y_train)\n",
    "# y_pred = mnb_classifier.predict(x_val)\n",
    "# print(accuracy_score(y_val, y_pred))\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(confusion_matrix(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hypertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-01 13:36:28,710] A new study created in memory with name: no-name-db33fa6c-c7e1-46ac-8938-b0dcabd6890f\n",
      "[I 2024-04-01 13:36:57,924] Trial 0 finished with value: 0.704 and parameters: {'kernel': 'sigmoid', 'degree': 5}. Best is trial 0 with value: 0.704.\n",
      "[I 2024-04-01 13:37:26,935] Trial 1 finished with value: 0.704 and parameters: {'kernel': 'sigmoid', 'degree': 2}. Best is trial 0 with value: 0.704.\n",
      "[I 2024-04-01 13:37:56,615] Trial 2 finished with value: 0.704 and parameters: {'kernel': 'sigmoid', 'degree': 2}. Best is trial 0 with value: 0.704.\n",
      "[I 2024-04-01 13:38:08,327] Trial 3 finished with value: 0.808 and parameters: {'kernel': 'linear'}. Best is trial 3 with value: 0.808.\n",
      "[I 2024-04-01 13:38:29,996] Trial 4 finished with value: 0.821 and parameters: {'kernel': 'rbf', 'degree': 5}. Best is trial 4 with value: 0.821.\n",
      "[I 2024-04-01 13:38:49,239] Trial 5 finished with value: 0.821 and parameters: {'kernel': 'rbf', 'degree': 2}. Best is trial 4 with value: 0.821.\n",
      "[I 2024-04-01 13:39:08,478] Trial 6 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:39:20,272] Trial 7 finished with value: 0.808 and parameters: {'kernel': 'linear'}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:39:32,856] Trial 8 finished with value: 0.808 and parameters: {'kernel': 'linear'}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:40:16,548] Trial 9 finished with value: 0.799 and parameters: {'kernel': 'poly', 'degree': 4}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:40:41,529] Trial 10 finished with value: 0.8205 and parameters: {'kernel': 'poly', 'degree': 3}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:41:03,040] Trial 11 finished with value: 0.821 and parameters: {'kernel': 'rbf', 'degree': 5}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:41:26,554] Trial 12 finished with value: 0.821 and parameters: {'kernel': 'rbf', 'degree': 4}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:41:52,736] Trial 13 finished with value: 0.8205 and parameters: {'kernel': 'poly', 'degree': 3}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:42:41,293] Trial 14 finished with value: 0.799 and parameters: {'kernel': 'poly', 'degree': 4}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:43:02,848] Trial 15 finished with value: 0.821 and parameters: {'kernel': 'rbf', 'degree': 3}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:43:59,928] Trial 16 finished with value: 0.753 and parameters: {'kernel': 'poly', 'degree': 5}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:44:24,138] Trial 17 finished with value: 0.821 and parameters: {'kernel': 'rbf', 'degree': 3}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:44:47,342] Trial 18 finished with value: 0.821 and parameters: {'kernel': 'rbf', 'degree': 4}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:45:08,536] Trial 19 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:45:29,321] Trial 20 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:45:46,901] Trial 21 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:46:08,677] Trial 22 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:46:29,862] Trial 23 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:46:48,753] Trial 24 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:47:14,428] Trial 25 finished with value: 0.8205 and parameters: {'kernel': 'poly', 'degree': 3}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:47:39,326] Trial 26 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 6 with value: 0.8245.\n",
      "[I 2024-04-01 13:48:16,587] Trial 27 finished with value: 0.8205 and parameters: {'kernel': 'poly', 'degree': 3}. Best is trial 6 with value: 0.8245.\n",
      "[W 2024-04-01 13:48:38,358] Trial 28 failed with parameters: {'kernel': 'poly', 'degree': 2} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\inesc\\AppData\\Local\\Temp\\ipykernel_13684\\605618546.py\", line 21, in objective\n",
      "    svm_classifier.fit(x_train, y_train)\n",
      "  File \"c:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 252, in fit\n",
      "    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n",
      "  File \"c:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 331, in _dense_fit\n",
      "    ) = libsvm.fit(\n",
      "KeyboardInterrupt\n",
      "[W 2024-04-01 13:48:38,360] Trial 28 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Create Optuna study\u001b[39;00m\n\u001b[0;32m     32\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Get best hyperparameters\u001b[39;00m\n\u001b[0;32m     36\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m svm_classifier \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39mkernel, degree\u001b[38;5;241m=\u001b[39mdegree)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train classifier\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43msvm_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Predict on validation set\u001b[39;00m\n\u001b[0;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svm_classifier\u001b[38;5;241m.\u001b[39mpredict(x_val)\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:252\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    251\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 252\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:331\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    317\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    321\u001b[0m (\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 331\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     # Define kernel\n",
    "#     kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    \n",
    "#     # For 'poly', 'rbf', and 'sigmoid' kernels, suggest degree parameter\n",
    "#     if kernel in ['poly', 'rbf', 'sigmoid']:\n",
    "#         degree = trial.suggest_int('degree', 2, 5)\n",
    "#     else:\n",
    "#         degree = 3  # Default degree value for kernels that don't use it\n",
    "    \n",
    "#     # Instantiate SVM classifier with hyperparameters\n",
    "#     svm_classifier = SVC(kernel=kernel, degree=degree)\n",
    "    \n",
    "#     # Train classifier\n",
    "#     svm_classifier.fit(x_train, y_train)\n",
    "    \n",
    "#     # Predict on validation set\n",
    "#     y_pred = svm_classifier.predict(x_val)\n",
    "    \n",
    "#     # Calculate accuracy score\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Create Optuna study\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get best hyperparameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# print(best_params)\n",
    "\n",
    "# #[I 2024-04-01 12:39:28,014] Trial 36 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 36 with value: 0.8245.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8245\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       550\n",
      "           1       0.90      0.85      0.88       704\n",
      "           2       0.71      0.73      0.72       178\n",
      "           3       0.80      0.79      0.79       275\n",
      "           4       0.75      0.76      0.75       212\n",
      "           5       0.61      0.68      0.64        81\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.77      0.78      0.78      2000\n",
      "weighted avg       0.83      0.82      0.83      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[486  13   5  22  21   3]\n",
      " [ 24 601  39  16  13  11]\n",
      " [ 11  23 130   7   5   2]\n",
      " [ 28  12   3 216  10   6]\n",
      " [ 18   8   4   8 161  13]\n",
      " [  7  10   1   2   6  55]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "svm_classifier = SVC(kernel='poly', degree=2, random_state=42)\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "y_pred = svm_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hypertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-01 13:50:59,436] A new study created in memory with name: no-name-9368ba41-2d16-46b7-a9f4-4a1b490a48ce\n",
      "[I 2024-04-01 13:51:01,097] Trial 0 finished with value: 0.681 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 0 with value: 0.681.\n",
      "[I 2024-04-01 13:51:24,154] Trial 1 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'kd_tree'}. Best is trial 1 with value: 0.69.\n",
      "[I 2024-04-01 13:51:36,840] Trial 2 finished with value: 0.6735 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'ball_tree'}. Best is trial 1 with value: 0.69.\n",
      "[I 2024-04-01 13:51:38,195] Trial 3 finished with value: 0.6825 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 1 with value: 0.69.\n",
      "[I 2024-04-01 13:51:51,421] Trial 4 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'ball_tree'}. Best is trial 4 with value: 0.6925.\n",
      "[I 2024-04-01 13:52:10,098] Trial 5 finished with value: 0.668 and parameters: {'n_neighbors': 2, 'weights': 'distance', 'algorithm': 'kd_tree'}. Best is trial 4 with value: 0.6925.\n",
      "[I 2024-04-01 13:52:30,763] Trial 6 finished with value: 0.681 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'kd_tree'}. Best is trial 4 with value: 0.6925.\n",
      "[I 2024-04-01 13:52:32,414] Trial 7 finished with value: 0.679 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 4 with value: 0.6925.\n",
      "[I 2024-04-01 13:52:56,215] Trial 8 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'kd_tree'}. Best is trial 4 with value: 0.6925.\n",
      "[I 2024-04-01 13:53:11,899] Trial 9 finished with value: 0.679 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'ball_tree'}. Best is trial 4 with value: 0.6925.\n",
      "[I 2024-04-01 13:53:13,390] Trial 10 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:15,011] Trial 11 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:16,603] Trial 12 finished with value: 0.689 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:18,634] Trial 13 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:20,704] Trial 14 finished with value: 0.689 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:22,908] Trial 15 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:25,333] Trial 16 finished with value: 0.6945 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:27,954] Trial 17 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:29,822] Trial 18 finished with value: 0.668 and parameters: {'n_neighbors': 1, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:31,877] Trial 19 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:33,857] Trial 20 finished with value: 0.6945 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:35,878] Trial 21 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:37,765] Trial 22 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:39,715] Trial 23 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:42,053] Trial 24 finished with value: 0.689 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:43,955] Trial 25 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 10 with value: 0.697.\n",
      "[I 2024-04-01 13:53:46,403] Trial 26 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:53:48,231] Trial 27 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:53:50,425] Trial 28 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:53:52,118] Trial 29 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:53:53,543] Trial 30 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:53:55,033] Trial 31 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:53:56,658] Trial 32 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:53:58,762] Trial 33 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:00,232] Trial 34 finished with value: 0.6895 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:01,599] Trial 35 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:02,938] Trial 36 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:04,273] Trial 37 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:19,496] Trial 38 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'ball_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:21,259] Trial 39 finished with value: 0.674 and parameters: {'n_neighbors': 3, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:44,052] Trial 40 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'kd_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:45,609] Trial 41 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:47,434] Trial 42 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:49,308] Trial 43 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:54:50,961] Trial 44 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:07,268] Trial 45 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'ball_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:08,822] Trial 46 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:31,910] Trial 47 finished with value: 0.696 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'kd_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:33,308] Trial 48 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:34,801] Trial 49 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:48,596] Trial 50 finished with value: 0.674 and parameters: {'n_neighbors': 3, 'weights': 'uniform', 'algorithm': 'ball_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:50,284] Trial 51 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:51,832] Trial 52 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:53,415] Trial 53 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:54,821] Trial 54 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:55:56,271] Trial 55 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:17,790] Trial 56 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'kd_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:19,396] Trial 57 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:20,983] Trial 58 finished with value: 0.679 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:22,568] Trial 59 finished with value: 0.6945 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:37,954] Trial 60 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'ball_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:39,566] Trial 61 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:41,245] Trial 62 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:42,644] Trial 63 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:44,087] Trial 64 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:45,629] Trial 65 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:47,039] Trial 66 finished with value: 0.6775 and parameters: {'n_neighbors': 2, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:56:48,552] Trial 67 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:15,248] Trial 68 finished with value: 0.689 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'algorithm': 'kd_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:16,621] Trial 69 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:18,265] Trial 70 finished with value: 0.697 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:19,900] Trial 71 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:21,489] Trial 72 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:23,500] Trial 73 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:26,337] Trial 74 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:28,707] Trial 75 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:30,290] Trial 76 finished with value: 0.69 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:32,008] Trial 77 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:48,094] Trial 78 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'ball_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:50,434] Trial 79 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:53,580] Trial 80 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:56,769] Trial 81 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:57:58,883] Trial 82 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:00,746] Trial 83 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:02,780] Trial 84 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:28,256] Trial 85 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'kd_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:29,720] Trial 86 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:31,140] Trial 87 finished with value: 0.6895 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:32,539] Trial 88 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:33,891] Trial 89 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:35,564] Trial 90 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:37,274] Trial 91 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:38,813] Trial 92 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:40,462] Trial 93 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:41,886] Trial 94 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:43,661] Trial 95 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:58,061] Trial 96 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'ball_tree'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:58:59,339] Trial 97 finished with value: 0.668 and parameters: {'n_neighbors': 1, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:59:00,945] Trial 98 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n",
      "[I 2024-04-01 13:59:02,464] Trial 99 finished with value: 0.6925 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute'}. Best is trial 26 with value: 0.701.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to optimize\n",
    "#     n_neighbors = trial.suggest_int('n_neighbors', 1, 10)\n",
    "#     weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "#     algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "    \n",
    "#     # Instantiate KNN classifier with hyperparameters\n",
    "#     knn_classifier = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)\n",
    "    \n",
    "#     # Train classifier\n",
    "#     knn_classifier.fit(x_train, y_train)\n",
    "    \n",
    "#     # Predict on validation set\n",
    "#     y_pred = knn_classifier.predict(x_val)\n",
    "    \n",
    "#     # Calculate accuracy score\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Create Optuna study\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get best hyperparameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# print(best_params)\n",
    "\n",
    "# # [I 2024-04-01 13:53:46,403] Trial 26 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 26 with value: 0.701."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:58:47.991134Z",
     "start_time": "2024-03-30T23:58:45.992029Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.701\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77       550\n",
      "           1       0.88      0.69      0.78       704\n",
      "           2       0.51      0.74      0.60       178\n",
      "           3       0.62      0.68      0.65       275\n",
      "           4       0.58      0.65      0.61       212\n",
      "           5       0.37      0.74      0.50        81\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.63      0.70      0.65      2000\n",
      "weighted avg       0.74      0.70      0.71      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[401  18  18  51  37  25]\n",
      " [ 38 487  85  37  26  31]\n",
      " [  9  17 131   7   8   6]\n",
      " [ 23  17  10 186  25  14]\n",
      " [ 16   9  10  15 137  25]\n",
      " [  4   3   5   4   5  60]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "hyperparameters = {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto'}\n",
    "knn_classifier = KNeighborsClassifier(**hyperparameters)\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "y_pred = knn_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Boosting Algorithms\n",
    "\n",
    "Testing with some boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Evaluation:\n",
      "Accuracy: 0.788\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       550\n",
      "           1       0.82      0.87      0.85       704\n",
      "           2       0.71      0.61      0.65       178\n",
      "           3       0.77      0.68      0.72       275\n",
      "           4       0.73      0.70      0.71       212\n",
      "           5       0.71      0.57      0.63        81\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.76      0.71      0.73      2000\n",
      "weighted avg       0.78      0.79      0.78      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[476  28   8  20  15   3]\n",
      " [ 29 612  30  19  12   2]\n",
      " [ 16  46 108   4   3   1]\n",
      " [ 33  28   4 186  20   4]\n",
      " [ 27  16   2  10 148   9]\n",
      " [ 15  12   1   2   5  46]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# XGBoost Classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for XGBoost\n",
    "accuracy_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "report_xgb = classification_report(y_val, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_val, y_pred_xgb)\n",
    "\n",
    "# Print the evaluation metrics for XGBoost\n",
    "print(\"XGBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_xgb)\n",
    "print(\"Classification Report:\\n\", report_xgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22440\n",
      "[LightGBM] [Info] Number of data points in the train set: 32172, number of used features: 88\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "\n",
      "LightGBM Classifier Evaluation:\n",
      "Accuracy: 0.789\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       550\n",
      "           1       0.82      0.88      0.85       704\n",
      "           2       0.71      0.58      0.64       178\n",
      "           3       0.78      0.68      0.73       275\n",
      "           4       0.75      0.71      0.73       212\n",
      "           5       0.73      0.54      0.62        81\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.76      0.71      0.73      2000\n",
      "weighted avg       0.79      0.79      0.79      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[474  27   9  22  16   2]\n",
      " [ 29 620  29  13  11   2]\n",
      " [ 15  49 104   4   5   1]\n",
      " [ 39  29   3 186  15   3]\n",
      " [ 27  14   2  11 150   8]\n",
      " [ 17  14   0   2   4  44]]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier()\n",
    "lgb_classifier.fit(x_train, y_train)\n",
    "y_pred_lgb = lgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for LightGBM\n",
    "accuracy_lgb = accuracy_score(y_val, y_pred_lgb)\n",
    "report_lgb = classification_report(y_val, y_pred_lgb)\n",
    "conf_matrix_lgb = confusion_matrix(y_val, y_pred_lgb)\n",
    "\n",
    "# Print the evaluation metrics for LightGBM\n",
    "print(\"\\nLightGBM Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_lgb)\n",
    "print(\"Classification Report:\\n\", report_lgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Evaluation:\n",
      "Accuracy: 0.6875\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       550\n",
      "           1       0.79      0.76      0.77       704\n",
      "           2       0.56      0.54      0.55       178\n",
      "           3       0.61      0.52      0.56       275\n",
      "           4       0.55      0.69      0.62       212\n",
      "           5       0.44      0.58      0.50        81\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.62      0.64      0.63      2000\n",
      "weighted avg       0.69      0.69      0.69      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[407  28  17  37  46  15]\n",
      " [ 47 533  47  26  29  22]\n",
      " [  9  48  97   9  11   4]\n",
      " [ 52  37   8 144  27   7]\n",
      " [ 25  13   3  13 147  11]\n",
      " [  8  14   1   6   5  47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost Classifier\n",
    "adaboost_classifier = AdaBoostClassifier()\n",
    "adaboost_classifier.fit(x_train, y_train)\n",
    "y_pred_adaboost = adaboost_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_val, y_pred_adaboost)\n",
    "report_adaboost = classification_report(y_val, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(y_val, y_pred_adaboost)\n",
    "\n",
    "# Print the evaluation metrics for AdaBoost\n",
    "print(\"AdaBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_adaboost)\n",
    "print(\"Classification Report:\\n\", report_adaboost)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_adaboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Bagging Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "\n",
    "model1 = xgb_classifier\n",
    "model2 = svm_classifier\n",
    "model3 = logreg_classifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('model1', model1), \n",
    "    ('model2', model2),\n",
    "    ('model3', model3)\n",
    "], voting='soft')\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict the validation set\n",
    "y_pred_voting = voting_clf.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for the ensemble model\n",
    "accuracy_voting = accuracy_score(y_val, y_pred_voting)\n",
    "report_voting = classification_report(y_val, y_pred_voting)\n",
    "conf_matrix_voting = confusion_matrix(y_val, y_pred_voting)\n",
    "\n",
    "# Print the evaluation metrics for the ensemble model\n",
    "print(\"Voting Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_voting)\n",
    "print(\"Classification Report:\\n\", report_voting)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:04:17.155945Z",
     "start_time": "2024-03-31T00:04:17.139641Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.35      0.44       581\n",
      "           1       0.67      0.39      0.50       695\n",
      "           2       0.18      0.44      0.25       159\n",
      "           3       0.35      0.36      0.35       275\n",
      "           4       0.30      0.36      0.33       224\n",
      "           5       0.11      0.52      0.18        66\n",
      "\n",
      "    accuracy                           0.38      2000\n",
      "   macro avg       0.37      0.40      0.34      2000\n",
      "weighted avg       0.50      0.38      0.41      2000\n",
      "\n",
      "[[201  57  76  96  64  87]\n",
      " [ 47 273 181  48  52  94]\n",
      " [ 14  29  70  15  10  21]\n",
      " [ 42  17  31 100  50  35]\n",
      " [ 33  23  25  26  81  36]\n",
      " [  3   8   8   4   9  34]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate with test set\n",
    "y_pred = logreg_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_logreg = logreg_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:59:23.243407Z",
     "start_time": "2024-03-30T23:59:23.209807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred = mnb_classifier.predict(x_test)\n",
    "# print(accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:59:27.095314Z",
     "start_time": "2024-03-30T23:59:27.071996Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88       581\n",
      "           1       0.89      0.84      0.86       695\n",
      "           2       0.64      0.74      0.69       159\n",
      "           3       0.76      0.74      0.75       275\n",
      "           4       0.77      0.77      0.77       224\n",
      "           5       0.49      0.71      0.58        66\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.74      0.78      0.75      2000\n",
      "weighted avg       0.82      0.81      0.82      2000\n",
      "\n",
      "[[504  15   4  31  20   7]\n",
      " [ 16 584  57  11  10  17]\n",
      " [  6  23 118   7   2   3]\n",
      " [ 24  17   4 204  16  10]\n",
      " [ 16   7   2  14 173  12]\n",
      " [  5  10   0   1   3  47]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(y_test, y_pred))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       581\n",
      "           1       0.80      0.86      0.83       695\n",
      "           2       0.68      0.56      0.62       159\n",
      "           3       0.75      0.63      0.69       275\n",
      "           4       0.75      0.69      0.72       224\n",
      "           5       0.59      0.44      0.50        66\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.73      0.68      0.70      2000\n",
      "weighted avg       0.77      0.78      0.77      2000\n",
      "\n",
      "[[509  33   5  18  13   3]\n",
      " [ 31 598  31  14  14   7]\n",
      " [ 13  47  89   8   1   1]\n",
      " [ 43  41   2 174  13   2]\n",
      " [ 27  18   3  14 155   7]\n",
      " [ 10  14   0   3  10  29]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73       581\n",
      "           1       0.77      0.72      0.74       695\n",
      "           2       0.49      0.55      0.51       159\n",
      "           3       0.55      0.55      0.55       275\n",
      "           4       0.55      0.63      0.59       224\n",
      "           5       0.32      0.47      0.38        66\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.57      0.60      0.59      2000\n",
      "weighted avg       0.67      0.66      0.67      2000\n",
      "\n",
      "[[409  48  15  52  50   7]\n",
      " [ 46 501  59  42  17  30]\n",
      " [ 12  36  87   9  11   4]\n",
      " [ 39  35  12 152  27  10]\n",
      " [ 23  21   5  18 142  15]\n",
      " [ 10   9   1   5  10  31]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = adaboost_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:59:55.572789Z",
     "start_time": "2024-03-30T23:59:53.922972Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77       581\n",
      "           1       0.85      0.66      0.75       695\n",
      "           2       0.44      0.67      0.53       159\n",
      "           3       0.59      0.63      0.61       275\n",
      "           4       0.55      0.64      0.60       224\n",
      "           5       0.35      0.65      0.46        66\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.60      0.67      0.62      2000\n",
      "weighted avg       0.72      0.68      0.69      2000\n",
      "\n",
      "[[431  27  19  49  38  17]\n",
      " [ 38 462  88  42  37  28]\n",
      " [  8  21 107   7  10   6]\n",
      " [ 33  17  16 174  22  13]\n",
      " [ 24  11   9  20 144  16]\n",
      " [  3   6   3   2   9  43]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. Cause of errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:09:10.004261Z",
     "start_time": "2024-03-31T00:09:09.997932Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotions_to_int = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'love': 2,\n",
    "    'anger': 3,\n",
    "    'fear': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "int_to_emotions = {v: k for k, v in emotions_to_int.items()}\n",
    "\n",
    "\n",
    "wrong_predictions = y_test[y_test != y_pred_logreg].index\n",
    "for i, index in enumerate(wrong_predictions):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(test_data['text'][index])\n",
    "    print('Real: ', int_to_emotions[test_data['label'][index]])\n",
    "    print('Pred:', int_to_emotions[y_pred_logreg[index]])\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:31:02.008118Z",
     "start_time": "2024-03-31T00:31:01.993186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:28:51.102411Z",
     "start_time": "2024-03-31T00:28:50.943776Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check which emotions are being confused\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "conf_matrix_copy = conf_matrix.copy()\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    conf_matrix[i, i] = 0 # make the diagonal null, to not eclipse the other values\n",
    "    \n",
    "plt.imshow(conf_matrix, cmap='viridis', interpolation='nearest')\n",
    "# annotate the axes with the emotion names\n",
    "plt.xticks(range(6), int_to_emotions.values(), rotation=45)\n",
    "plt.yticks(range(6), int_to_emotions.values())\n",
    "# add colorbar more to the right\n",
    "# plt.colorbar()\n",
    "# legend the axes with predicted and true values\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# add counts in the plot\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        if i == j:\n",
    "            plt.text(j, i, conf_matrix_copy[i, j], ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j, i, conf_matrix[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "for i in range(conf_matrix.shape[0]): # this does not count correct predictions\n",
    "    plt.text(6, i, conf_matrix[i, :].sum(), ha='center', va='center', color='black')\n",
    "for i in range(conf_matrix.shape[1]):\n",
    "    plt.text(i, 7, conf_matrix[:, i].sum(), ha='center', va='center', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The test set appears to be unbalance.\n",
    "- 'joy' is a lot more mixed with 'love' than the contrary. \n",
    "- 'surprise' has a low Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:45:22.096653Z",
     "start_time": "2024-03-31T00:45:21.865171Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "conf_matrix = conf_matrix / conf_matrix.sum(axis=1)[:, None] # normalize the confusion matrix\n",
    "conf_matrix_copy = conf_matrix.copy()\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    conf_matrix[i, i] = 0 # make the diagonal null, to not eclipse the other values\n",
    "plt.imshow(conf_matrix, cmap='viridis', interpolation='nearest')\n",
    "\n",
    "plt.xticks(range(6), int_to_emotions.values(), rotation=45)\n",
    "plt.yticks(range(6), int_to_emotions.values())\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# add counts in the plot\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        if i == j:\n",
    "            plt.text(j, i, f'{conf_matrix_copy[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j, i, f'{conf_matrix[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "        \n",
    "plt.title('Percentage of predictions (row sum=1)')\n",
    "# Rows add to 1\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- sadness mixed with joy and anger\n",
    "- joy mixed with love\n",
    "- love mixed with joy\n",
    "- anger mixed with sadness and joy??\n",
    "- fear mixed with sadness\n",
    "- surprise mixed with almost everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Experiment with user-inputed setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "emotions = {\n",
    "    0: \"0. sadness\",\n",
    "    1: \"1. joy\",\n",
    "    2: \"2. love\",\n",
    "    3: \"3. anger\",\n",
    "    4: \"4. fear\",\n",
    "    5: \"5. surprise\"\n",
    "}\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(phrase.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    my_stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody', 'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without','hereby'])\n",
    "    my_stop_words -= words_to_keep\n",
    "    tokens = [word for word in tokens if word not in my_stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def doc2vec_text(tokens):\n",
    "    model = Doc2Vec.load(\"d2v_best_stopwords.model\")\n",
    "    return model.infer_vector(tokens)\n",
    "\n",
    "def classify_emotion(number):\n",
    "    if number==0:\n",
    "        return \"0. sadness\"\n",
    "    if number==1:\n",
    "        return \"1. joy\"\n",
    "    if number==2:\n",
    "        return \"2. love\"\n",
    "    if number==3:\n",
    "        return \"3. anger\"\n",
    "    if number==4:\n",
    "        return \"4. fear\"\n",
    "    if number==5:\n",
    "        return \"5. surprise\"\n",
    "\n",
    "    return \"Not possible to identify\"\n",
    "\n",
    "\n",
    "def analyze_sentiment(phrase):\n",
    "    tokens = preprocess_text(phrase)\n",
    "    tokens_embeddings = doc2vec_text(tokens)\n",
    "\n",
    "    # Reshape to have proper structure\n",
    "    tokens_embeddings = np.array(tokens_embeddings).reshape(1, -1)\n",
    "\n",
    "    # Predict the class using the SVM classifier\n",
    "    predicted_class = svm_classifier.predict(tokens_embeddings)\n",
    "    print(classify_emotion(predicted_class[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. joy\n"
     ]
    }
   ],
   "source": [
    "phrase = \"i love love\"\n",
    "analyze_sentiment(phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
