{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already lowercased and lacks punctuation. We will tokenize the text and remove stopwords, as well as apply lemmatization to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\inesc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\inesc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\inesc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[feel, awful, s, job, position, succeed, just,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[im, alone, feel, awful]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ive, probably, mentioned, but, really, feel, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[feeling, little, low, day]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[beleive, sensitive, people, feeling, tend, co...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  [feel, awful, s, job, position, succeed, just,...      0\n",
       "1                           [im, alone, feel, awful]      0\n",
       "2  [ive, probably, mentioned, but, really, feel, ...      1\n",
       "3                        [feeling, little, low, day]      0\n",
       "4  [beleive, sensitive, people, feeling, tend, co...      2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "#words to keep: no couldnt cry not cant cannot nor except nobody off but serious enough nothing alone down only without\n",
    "\n",
    "\n",
    "data = pd.read_json('./data/data.jsonl', lines=True)\n",
    "\n",
    "\n",
    "def pre_process_data(dataset):\n",
    "    #tokenize\n",
    "    dataset['text'] = dataset['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    #remove stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "    #print(my_stop_words)\n",
    "    words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody', 'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without','hereby'])\n",
    "    my_stop_words = my_stop_words - words_to_keep\n",
    "    \n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in my_stop_words])\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data = pre_process_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:09:31.882317900Z",
     "start_time": "2024-03-12T21:09:28.844193200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'feel', 'extremely', 'comfortable', 'group', 'people', 'dont', 'need', 'hide']\n",
      "0        [im, feeling, rotten, im, not, ambitious, right]\n",
      "1                      [im, updating, blog, feel, shitty]\n",
      "2       [make, separate, don, t, want, feel, like, m, ...\n",
      "3       [left, bouquet, red, yellow, tulip, arm, feeli...\n",
      "4                            [feeling, little, vain, did]\n",
      "                              ...                        \n",
      "1995    [just, feeling, like, unkind, doing, wrong, th...\n",
      "1996    [im, feeling, little, cranky, negative, doctor...\n",
      "1997    [feel, useful, people, give, great, feeling, a...\n",
      "1998    [im, feeling, comfortable, derby, feel, start,...\n",
      "1999    [feel, weird, meet, w, people, text, but, like...\n",
      "Name: text, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json('./data/test.jsonl', lines=True)\n",
    "train_data = pd.read_json('./data/train.jsonl', lines=True)\n",
    "validation_data = pd.read_json('./data/validation.jsonl', lines=True)\n",
    "\n",
    "test_data = pre_process_data(test_data)\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)\n",
    "print(test_data['text'][13])\n",
    "\n",
    "print(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Vectorization\n",
    "(secção possívelmente temporária, mas queria experimentar as cenas de tf_idf depois do pré-processamento) -- matos\n",
    "não acho que convenha ser temporaria, dado que efetivamente melhora os resultados ihihihi, e é uma prática comum e recomendada pelo que estivemos a ver -- ines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertunning for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:46.201324100Z",
     "start_time": "2024-03-12T21:02:32.893616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tfidfVectorizer(data, train_data, validation_data, test_data):\n",
    "    vectorizer = TfidfVectorizer(stop_words=list(my_stop_words), ngram_range=(1,2), max_features=40000)\n",
    "    # good idea to use two-grams??\n",
    "    # print(X.shape)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "    \n",
    "    # print(vectorizer.get_feature_names_out())\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW model\n",
    "Count vectorizer which is an implementation of the BOW model.\n",
    "\n",
    "The disadvantage of the BOW model is it does not consider the sequence of words, and as language does involve sequence and context, sometimes the BOW model might not be a good fit for the best-case scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def countVectorizer(data, train_data, validation_data, test_data):\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words=my_stop_words, ngram_range=(1,2), max_features=40000)\n",
    "\n",
    "    x_train= vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from gensim.models import Doc2Vec\n",
    "# from gensim.models.doc2vec import TaggedDocument\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# y_train = train_data['label']\n",
    "# y_val = validation_data['label']\n",
    "\n",
    "# def objective(trial):\n",
    "#     vector_size = trial.suggest_int(\"vector_size\", 50, 300)\n",
    "#     window = trial.suggest_int(\"window\", 3, 15)\n",
    "#     min_count = trial.suggest_int(\"min_count\", 1, 10)\n",
    "#     epochs = trial.suggest_int(\"epochs\", 10, 50)\n",
    "    \n",
    "#     # Train Doc2Vec model\n",
    "#     tagged_data = [TaggedDocument(words=doc, tags=[str(label)]) for doc, label in zip(train_data['text'], train_data['label'])]\n",
    "#     doc2vec_model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
    "#     doc2vec_model.build_vocab(tagged_data)\n",
    "#     doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "    \n",
    "#     # Prepare document vectors for training and test sets\n",
    "#     X_train_vecs  = [doc2vec_model.infer_vector(doc) for doc in train_data['text']]\n",
    "#     X_val_vecs = [doc2vec_model.infer_vector(doc) for doc in validation_data['text']]\n",
    "    \n",
    "#     # Train Logistic Regression classifier\n",
    "#     classifier = LogisticRegression(max_iter=1000)\n",
    "#     classifier.fit(X_train_vecs, y_train)\n",
    "    \n",
    "#     # Evaluate on test set\n",
    "#     y_pred = classifier.predict(X_val_vecs)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Define the study object and optimize the objective function\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Print the best hyperparameters found\n",
    "# best_params = study.best_params\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# # Train the final model with the best hyperparameters\n",
    "# best_vector_size = best_params[\"vector_size\"]\n",
    "# best_window = best_params[\"window\"]\n",
    "# best_min_count = best_params[\"min_count\"]\n",
    "# best_epochs = best_params[\"epochs\"]\n",
    "\n",
    "# # Train Doc2Vec model with best hyperparameters\n",
    "# tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(train_data[\"text\"])]\n",
    "# best_doc2vec_model = Doc2Vec(vector_size=best_vector_size, window=best_window, min_count=best_min_count, epochs=best_epochs)\n",
    "# best_doc2vec_model.build_vocab(tagged_data)\n",
    "# best_doc2vec_model.train(tagged_data, total_examples=best_doc2vec_model.corpus_count, epochs=best_doc2vec_model.epochs)\n",
    "\n",
    "# # Prepare document vectors for training and test sets with the best Doc2Vec model\n",
    "# X_train_vecs  = [best_doc2vec_model.infer_vector(doc) for doc in train_data['text']]\n",
    "# X_val_vecs = [best_doc2vec_model.infer_vector(doc) for doc in validation_data['text']]\n",
    "\n",
    "# # Train Logistic Regression classifier on the final Doc2Vec vectors\n",
    "# final_classifier = LogisticRegression(max_iter=1000)\n",
    "# final_classifier.fit(X_train_vecs, y_train)\n",
    "\n",
    "# # Evaluate on test set\n",
    "# final_accuracy = final_classifier.score(X_val_vecs, y_val)\n",
    "# print(\"Final accuracy on test set with the best model:\", final_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x18a96038730>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "\n",
    "def train_doc2vec(train_data):\n",
    "    max_epochs = 16\n",
    "    vec_size = 88\n",
    "    alpha = 0.025\n",
    "    window = 4\n",
    "    min_count = 7\n",
    "\n",
    "    tagged_data = [TaggedDocument(words=doc, tags=[str(label)]) for doc, label in zip(train_data['text'], train_data['label'])]\n",
    "\n",
    "    # antes tinha workers definidos\n",
    "    model = Doc2Vec(vector_size=vec_size, window=window, min_count=min_count, epochs=max_epochs)\n",
    "    \n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=50)\n",
    "\n",
    "\n",
    "    model.save(\"d2v_best_stopwords.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    return model\n",
    "\n",
    "train_doc2vec(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:56:31.788595Z",
     "start_time": "2024-03-30T23:56:31.783431Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def doc2vec(data, train_data, validation_data, test_data):\n",
    "\n",
    "    # training the doc2vec model\n",
    "    #model = train_doc2vec(train_data)\n",
    "    model = Doc2Vec.load(\"d2v_best_stopwords.model\")\n",
    "    \n",
    "    x_train = [model.infer_vector(doc) for doc in train_data['text']]\n",
    "    x_val = [model.infer_vector(doc) for doc in validation_data['text']]\n",
    "    x_test = [model.infer_vector(doc) for doc in test_data['text']]\n",
    "\n",
    "    # pca = PCA(n_components=3)\n",
    "    # x_train = pca.fit_transform(x_train)\n",
    "    # x_val = pca.transform(x_val)\n",
    "    # x_test = pca.transform(x_test)\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:54:28.516328Z",
     "start_time": "2024-03-30T23:54:28.508772Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def lala(model, data):\n",
    "    vectors = []\n",
    "    for doc in data['text']:\n",
    "        document = []\n",
    "        for word in doc:\n",
    "            if word in model.wv:\n",
    "                document.append(model.wv[word])\n",
    "        vectors.append(document)\n",
    "    return vectors\n",
    "\n",
    "# test [d,c,c]  | label\n",
    "########################\n",
    "## test \n",
    "## [ [] [] [] ] | label\n",
    "\n",
    "def wordEmbeddingsVectorizer(data):\n",
    "\n",
    "    data_model = Word2Vec(data[\"text\"], vector_size=3, window=10, min_count=2, workers=10, sg=1)\n",
    "    \n",
    "    x_train = lala(data_model, train_data)\n",
    "    x_val =lala(data_model, validation_data)\n",
    "    x_test = lala(data_model, test_data)\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:57:13.209485Z",
     "start_time": "2024-03-30T23:56:34.757556Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose vectorizer (featurizer)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = tfidfVectorizer(data, train_data, validation_data, test_data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = countVectorizer(data, train_data, validation_data, test_data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = wordEmbeddingsVectorizer(data)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = doc2vec(data, train_data, validation_data, test_data)\n",
    "\n",
    "#print(x_train)\n",
    "\n",
    "## Apply SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy='auto', k_neighbors=10)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1. Model Selection\n",
    "Aqui também só estava a querer espetar modelos para começar a ver o que dá que ainda não sei que features vão ser usadas:\n",
    "tf-idf, word embeddings, ???, features mais feitas à mão?\n",
    "\n",
    "\n",
    "Isto pelos vistos é uma cena, que não implementei (ainda..)\n",
    "\n",
    "\"The validation set uses a subset of the training data to provide an unbiased evaluation of a model. The validation data set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it. It is in this phase that hyperparameter tuning occurs.\"\n",
    "\n",
    "Wikipedia:\n",
    "The basic process of using a validation data set for model selection (as part of training data set, validation data set, and test data set) is:\n",
    "\n",
    "Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.\n",
    "\n",
    "An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:57:44.308633Z",
     "start_time": "2024-03-30T23:57:43.289263Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86       550\n",
      "           1       0.92      0.81      0.86       704\n",
      "           2       0.67      0.78      0.72       178\n",
      "           3       0.77      0.78      0.77       275\n",
      "           4       0.70      0.79      0.74       212\n",
      "           5       0.50      0.74      0.60        81\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.74      0.79      0.76      2000\n",
      "weighted avg       0.83      0.81      0.81      2000\n",
      "\n",
      "[[466   9   9  28  25  13]\n",
      " [ 22 573  49  18  19  23]\n",
      " [  7  19 139   6   6   1]\n",
      " [ 17  12   6 215  17   8]\n",
      " [ 11   4   4  11 167  15]\n",
      " [  7   5   2   3   4  60]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "logreg_classifier = LogisticRegression(max_iter=1000, )\n",
    "logreg_classifier.fit(x_train, y_train)\n",
    "y_pred = logreg_classifier.predict(x_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:57:47.417352Z",
     "start_time": "2024-03-30T23:57:47.325689Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mnb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmnb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mnb_classifier\u001b[38;5;241m.\u001b[39mpredict(x_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:776\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    774\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 776\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:898\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 898\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1418\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1415\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "\n",
    "# mnb_classifier = MultinomialNB()\n",
    "# mnb_classifier.fit(x_train, y_train)\n",
    "# y_pred = mnb_classifier.predict(x_val)\n",
    "# print(accuracy_score(y_val, y_pred))\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(confusion_matrix(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8025\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       550\n",
      "           1       0.91      0.82      0.86       704\n",
      "           2       0.65      0.78      0.71       178\n",
      "           3       0.76      0.77      0.76       275\n",
      "           4       0.70      0.75      0.72       212\n",
      "           5       0.52      0.73      0.61        81\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.73      0.78      0.75      2000\n",
      "weighted avg       0.81      0.80      0.81      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[460  14   8  32  31   5]\n",
      " [ 27 577  50  19  16  15]\n",
      " [ 10  17 139   6   3   3]\n",
      " [ 19  15   7 211  15   8]\n",
      " [ 10   5   7   8 159  23]\n",
      " [  7   7   2   2   4  59]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "y_pred = svm_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:58:47.991134Z",
     "start_time": "2024-03-30T23:58:45.992029Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6795\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75       550\n",
      "           1       0.86      0.70      0.77       704\n",
      "           2       0.48      0.63      0.54       178\n",
      "           3       0.59      0.62      0.60       275\n",
      "           4       0.54      0.58      0.56       212\n",
      "           5       0.39      0.70      0.50        81\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.60      0.66      0.62      2000\n",
      "weighted avg       0.71      0.68      0.69      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[401  21  15  54  38  21]\n",
      " [ 42 494  86  33  29  20]\n",
      " [ 11  23 113  10  11  10]\n",
      " [ 42  19  11 170  21  12]\n",
      " [ 22  13   9  18 124  26]\n",
      " [  8   3   3   4   6  57]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "y_pred = knn_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Boosting Algorithms\n",
    "\n",
    "Testing with some boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Evaluation:\n",
      "Accuracy: 0.7895\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       550\n",
      "           1       0.83      0.88      0.85       704\n",
      "           2       0.76      0.58      0.66       178\n",
      "           3       0.78      0.69      0.73       275\n",
      "           4       0.71      0.72      0.72       212\n",
      "           5       0.57      0.48      0.52        81\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.74      0.70      0.72      2000\n",
      "weighted avg       0.79      0.79      0.79      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[474  22   5  22  20   7]\n",
      " [ 28 620  22  12  15   7]\n",
      " [ 17  45 104   6   5   1]\n",
      " [ 33  33   4 189  13   3]\n",
      " [ 24  13   1   9 153  12]\n",
      " [ 14  15   0   5   8  39]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# XGBoost Classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for XGBoost\n",
    "accuracy_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "report_xgb = classification_report(y_val, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_val, y_pred_xgb)\n",
    "\n",
    "# Print the evaluation metrics for XGBoost\n",
    "print(\"XGBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_xgb)\n",
    "print(\"Classification Report:\\n\", report_xgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22440\n",
      "[LightGBM] [Info] Number of data points in the train set: 32172, number of used features: 88\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "\n",
      "LightGBM Classifier Evaluation:\n",
      "Accuracy: 0.7845\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       550\n",
      "           1       0.82      0.89      0.85       704\n",
      "           2       0.75      0.54      0.63       178\n",
      "           3       0.77      0.68      0.72       275\n",
      "           4       0.72      0.73      0.73       212\n",
      "           5       0.62      0.44      0.52        81\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.75      0.69      0.71      2000\n",
      "weighted avg       0.78      0.78      0.78      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[470  29   5  25  16   5]\n",
      " [ 30 625  21  11  13   4]\n",
      " [ 19  47  97   6   8   1]\n",
      " [ 36  32   4 186  14   3]\n",
      " [ 25  13   1   9 155   9]\n",
      " [ 16  14   2   4   9  36]]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier()\n",
    "lgb_classifier.fit(x_train, y_train)\n",
    "y_pred_lgb = lgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for LightGBM\n",
    "accuracy_lgb = accuracy_score(y_val, y_pred_lgb)\n",
    "report_lgb = classification_report(y_val, y_pred_lgb)\n",
    "conf_matrix_lgb = confusion_matrix(y_val, y_pred_lgb)\n",
    "\n",
    "# Print the evaluation metrics for LightGBM\n",
    "print(\"\\nLightGBM Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_lgb)\n",
    "print(\"Classification Report:\\n\", report_lgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Evaluation:\n",
      "Accuracy: 0.6675\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.69      0.71       550\n",
      "           1       0.79      0.77      0.78       704\n",
      "           2       0.57      0.52      0.55       178\n",
      "           3       0.57      0.53      0.55       275\n",
      "           4       0.50      0.66      0.57       212\n",
      "           5       0.38      0.44      0.41        81\n",
      "\n",
      "    accuracy                           0.67      2000\n",
      "   macro avg       0.59      0.60      0.59      2000\n",
      "weighted avg       0.68      0.67      0.67      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[381  43   8  56  54   8]\n",
      " [ 41 539  44  33  23  24]\n",
      " [ 17  45  93   5  12   6]\n",
      " [ 41  35  13 147  33   6]\n",
      " [ 31  13   1  13 139  15]\n",
      " [ 13   8   4   4  16  36]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost Classifier\n",
    "adaboost_classifier = AdaBoostClassifier()\n",
    "adaboost_classifier.fit(x_train, y_train)\n",
    "y_pred_adaboost = adaboost_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_val, y_pred_adaboost)\n",
    "report_adaboost = classification_report(y_val, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(y_val, y_pred_adaboost)\n",
    "\n",
    "# Print the evaluation metrics for AdaBoost\n",
    "print(\"AdaBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_adaboost)\n",
    "print(\"Classification Report:\\n\", report_adaboost)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_adaboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Bagging Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Evaluation:\n",
      "Accuracy: 0.817\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       550\n",
      "           1       0.91      0.84      0.87       704\n",
      "           2       0.69      0.76      0.72       178\n",
      "           3       0.78      0.78      0.78       275\n",
      "           4       0.70      0.77      0.73       212\n",
      "           5       0.57      0.70      0.63        81\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.75      0.79      0.77      2000\n",
      "weighted avg       0.82      0.82      0.82      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[472  10   8  27  27   6]\n",
      " [ 26 592  42  15  17  12]\n",
      " [  9  21 135   6   5   2]\n",
      " [ 17  17   5 215  15   6]\n",
      " [ 14   5   5   8 163  17]\n",
      " [  8   7   1   3   5  57]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "\n",
    "model1 = xgb_classifier\n",
    "model2 = svm_classifier\n",
    "model3 = logreg_classifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('model1', model1), \n",
    "    ('model2', model2),\n",
    "    ('model3', model3)\n",
    "], voting='soft')\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict the validation set\n",
    "y_pred_voting = voting_clf.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for the ensemble model\n",
    "accuracy_voting = accuracy_score(y_val, y_pred_voting)\n",
    "report_voting = classification_report(y_val, y_pred_voting)\n",
    "conf_matrix_voting = confusion_matrix(y_val, y_pred_voting)\n",
    "\n",
    "# Print the evaluation metrics for the ensemble model\n",
    "print(\"Voting Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_voting)\n",
    "print(\"Classification Report:\\n\", report_voting)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:04:17.155945Z",
     "start_time": "2024-03-31T00:04:17.139641Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88       581\n",
      "           1       0.91      0.78      0.84       695\n",
      "           2       0.58      0.77      0.66       159\n",
      "           3       0.78      0.79      0.79       275\n",
      "           4       0.73      0.76      0.75       224\n",
      "           5       0.39      0.83      0.53        66\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.72      0.80      0.74      2000\n",
      "weighted avg       0.83      0.80      0.81      2000\n",
      "\n",
      "[[492  17   8  30  21  13]\n",
      " [ 11 545  74  11  23  31]\n",
      " [  6  17 122   5   4   5]\n",
      " [ 17   9   4 218  13  14]\n",
      " [ 12   5   1  12 171  23]\n",
      " [  1   4   0   3   3  55]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate with test set\n",
    "y_pred = logreg_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_logreg = logreg_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:59:23.243407Z",
     "start_time": "2024-03-30T23:59:23.209807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred = mnb_classifier.predict(x_test)\n",
    "# print(accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:59:27.095314Z",
     "start_time": "2024-03-30T23:59:27.071996Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86       581\n",
      "           1       0.90      0.79      0.84       695\n",
      "           2       0.55      0.72      0.62       159\n",
      "           3       0.74      0.76      0.75       275\n",
      "           4       0.71      0.75      0.73       224\n",
      "           5       0.39      0.68      0.50        66\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.70      0.76      0.72      2000\n",
      "weighted avg       0.81      0.79      0.79      2000\n",
      "\n",
      "[[490  17  12  34  19   9]\n",
      " [ 15 548  75  14  19  24]\n",
      " [  8  20 114   9   5   3]\n",
      " [ 21  10   8 208  20   8]\n",
      " [ 15   4   0  13 167  25]\n",
      " [  4   9   0   3   5  45]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       581\n",
      "           1       0.80      0.86      0.83       695\n",
      "           2       0.68      0.58      0.63       159\n",
      "           3       0.78      0.64      0.70       275\n",
      "           4       0.73      0.68      0.71       224\n",
      "           5       0.58      0.45      0.51        66\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.73      0.68      0.70      2000\n",
      "weighted avg       0.78      0.78      0.78      2000\n",
      "\n",
      "[[509  35   3  18  13   3]\n",
      " [ 35 600  34   9  11   6]\n",
      " [ 10  44  93   6   4   2]\n",
      " [ 38  36   4 177  17   3]\n",
      " [ 28  19   3  13 153   8]\n",
      " [  5  15   0   5  11  30]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       581\n",
      "           1       0.80      0.86      0.83       695\n",
      "           2       0.68      0.56      0.62       159\n",
      "           3       0.75      0.63      0.69       275\n",
      "           4       0.75      0.69      0.72       224\n",
      "           5       0.59      0.44      0.50        66\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.73      0.68      0.70      2000\n",
      "weighted avg       0.77      0.78      0.77      2000\n",
      "\n",
      "[[509  33   5  18  13   3]\n",
      " [ 31 598  31  14  14   7]\n",
      " [ 13  47  89   8   1   1]\n",
      " [ 43  41   2 174  13   2]\n",
      " [ 27  18   3  14 155   7]\n",
      " [ 10  14   0   3  10  29]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73       581\n",
      "           1       0.77      0.72      0.74       695\n",
      "           2       0.49      0.55      0.51       159\n",
      "           3       0.55      0.55      0.55       275\n",
      "           4       0.55      0.63      0.59       224\n",
      "           5       0.32      0.47      0.38        66\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.57      0.60      0.59      2000\n",
      "weighted avg       0.67      0.66      0.67      2000\n",
      "\n",
      "[[409  48  15  52  50   7]\n",
      " [ 46 501  59  42  17  30]\n",
      " [ 12  36  87   9  11   4]\n",
      " [ 39  35  12 152  27  10]\n",
      " [ 23  21   5  18 142  15]\n",
      " [ 10   9   1   5  10  31]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = adaboost_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T23:59:55.572789Z",
     "start_time": "2024-03-30T23:59:53.922972Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77       581\n",
      "           1       0.85      0.66      0.75       695\n",
      "           2       0.44      0.67      0.53       159\n",
      "           3       0.59      0.63      0.61       275\n",
      "           4       0.55      0.64      0.60       224\n",
      "           5       0.35      0.65      0.46        66\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.60      0.67      0.62      2000\n",
      "weighted avg       0.72      0.68      0.69      2000\n",
      "\n",
      "[[431  27  19  49  38  17]\n",
      " [ 38 462  88  42  37  28]\n",
      " [  8  21 107   7  10   6]\n",
      " [ 33  17  16 174  22  13]\n",
      " [ 24  11   9  20 144  16]\n",
      " [  3   6   3   2   9  43]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. Cause of errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:09:10.004261Z",
     "start_time": "2024-03-31T00:09:09.997932Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotions_to_int = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'love': 2,\n",
    "    'anger': 3,\n",
    "    'fear': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "int_to_emotions = {v: k for k, v in emotions_to_int.items()}\n",
    "\n",
    "\n",
    "wrong_predictions = y_test[y_test != y_pred_logreg].index\n",
    "for i, index in enumerate(wrong_predictions):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(test_data['text'][index])\n",
    "    print('Real: ', int_to_emotions[test_data['label'][index]])\n",
    "    print('Pred:', int_to_emotions[y_pred_logreg[index]])\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:31:02.008118Z",
     "start_time": "2024-03-31T00:31:01.993186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:28:51.102411Z",
     "start_time": "2024-03-31T00:28:50.943776Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check which emotions are being confused\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "conf_matrix_copy = conf_matrix.copy()\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    conf_matrix[i, i] = 0 # make the diagonal null, to not eclipse the other values\n",
    "    \n",
    "plt.imshow(conf_matrix, cmap='viridis', interpolation='nearest')\n",
    "# annotate the axes with the emotion names\n",
    "plt.xticks(range(6), int_to_emotions.values(), rotation=45)\n",
    "plt.yticks(range(6), int_to_emotions.values())\n",
    "# add colorbar more to the right\n",
    "# plt.colorbar()\n",
    "# legend the axes with predicted and true values\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# add counts in the plot\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        if i == j:\n",
    "            plt.text(j, i, conf_matrix_copy[i, j], ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j, i, conf_matrix[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "for i in range(conf_matrix.shape[0]): # this does not count correct predictions\n",
    "    plt.text(6, i, conf_matrix[i, :].sum(), ha='center', va='center', color='black')\n",
    "for i in range(conf_matrix.shape[1]):\n",
    "    plt.text(i, 7, conf_matrix[:, i].sum(), ha='center', va='center', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The test set appears to be unbalance.\n",
    "- 'joy' is a lot more mixed with 'love' than the contrary. \n",
    "- 'surprise' has a low Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T00:45:22.096653Z",
     "start_time": "2024-03-31T00:45:21.865171Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "conf_matrix = conf_matrix / conf_matrix.sum(axis=1)[:, None] # normalize the confusion matrix\n",
    "conf_matrix_copy = conf_matrix.copy()\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    conf_matrix[i, i] = 0 # make the diagonal null, to not eclipse the other values\n",
    "plt.imshow(conf_matrix, cmap='viridis', interpolation='nearest')\n",
    "\n",
    "plt.xticks(range(6), int_to_emotions.values(), rotation=45)\n",
    "plt.yticks(range(6), int_to_emotions.values())\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# add counts in the plot\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        if i == j:\n",
    "            plt.text(j, i, f'{conf_matrix_copy[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j, i, f'{conf_matrix[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "        \n",
    "plt.title('Percentage of predictions (row sum=1)')\n",
    "# Rows add to 1\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- sadness mixed with joy and anger\n",
    "- joy mixed with love\n",
    "- love mixed with joy\n",
    "- anger mixed with sadness and joy??\n",
    "- fear mixed with sadness\n",
    "- surprise mixed with almost everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Experiment with user-inputed setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(phrase.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    my_stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody', 'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without','hereby'])\n",
    "    my_stop_words -= words_to_keep\n",
    "    tokens = [word for word in tokens if word not in my_stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def doc2vec_text(tokens):\n",
    "    model = Doc2Vec.load(\"d2v_best_stopwords.model\")\n",
    "    return model.infer_vector(tokens)\n",
    "\n",
    "def classify_emotion(number):\n",
    "    if number==0:\n",
    "        return \"0. sadness\"\n",
    "    if number==1:\n",
    "        return \"1. joy\"\n",
    "    if number==2:\n",
    "        return \"2. love\"\n",
    "    if number==3:\n",
    "        return \"3. anger\"\n",
    "    if number==4:\n",
    "        return \"4. fear\"\n",
    "    if number==5:\n",
    "        return \"5. surprise\"\n",
    "\n",
    "    return \"Not possible to identify\"\n",
    "\n",
    "\n",
    "def analyze_sentiment(phrase):\n",
    "    tokens = preprocess_text(phrase)\n",
    "    tokens_embeddings = doc2vec_text(tokens)\n",
    "\n",
    "    # Reshape to have proper structure\n",
    "    tokens_embeddings = np.array(tokens_embeddings).reshape(1, -1)\n",
    "\n",
    "    # Predict the class using the SVM classifier\n",
    "    predicted_class = svm_classifier.predict(tokens_embeddings)\n",
    "    print(classify_emotion(predicted_class[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. anger\n"
     ]
    }
   ],
   "source": [
    "phrase = \"she touched me\"\n",
    "analyze_sentiment(phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
