{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already lowercased and lacks punctuation. We will tokenize the text and remove stopwords, as well as apply lemmatization to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "#words to keep: no couldnt cry not cant cannot nor except nobody off but serious enough nothing alone down only without\n",
    "\n",
    "\n",
    "data = pd.read_json('./data/data.jsonl', lines=True)\n",
    "\n",
    "\n",
    "def pre_process_data(dataset):\n",
    "    #tokenize\n",
    "    dataset['text'] = dataset['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    #remove stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "    #print(my_stop_words)\n",
    "    words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody', 'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without','hereby'])\n",
    "    my_stop_words = my_stop_words - words_to_keep\n",
    "    \n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in my_stop_words])\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data = pre_process_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_json('./data/test.jsonl', lines=True)\n",
    "train_data = pd.read_json('./data/train.jsonl', lines=True)\n",
    "validation_data = pd.read_json('./data/validation.jsonl', lines=True)\n",
    "\n",
    "test_data = pre_process_data(test_data)\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)\n",
    "print(test_data['text'][13])\n",
    "\n",
    "print(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Vectorization\n",
    "(secção possívelmente temporária, mas queria experimentar as cenas de tf_idf depois do pré-processamento) -- matos\n",
    "não acho que convenha ser temporaria, dado que efetivamente melhora os resultados ihihihi, e é uma prática comum e recomendada pelo que estivemos a ver -- ines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertunning for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tfidfVectorizer(data, train_data, validation_data, test_data):\n",
    "    vectorizer = TfidfVectorizer(stop_words=list(my_stop_words), ngram_range=(1,2), max_features=40000)\n",
    "    # good idea to use two-grams??\n",
    "    # print(X.shape)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "    \n",
    "    # print(vectorizer.get_feature_names_out())\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW model\n",
    "Count vectorizer which is an implementation of the BOW model.\n",
    "\n",
    "The disadvantage of the BOW model is it does not consider the sequence of words, and as language does involve sequence and context, sometimes the BOW model might not be a good fit for the best-case scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def countVectorizer(data, train_data, validation_data, test_data):\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words=my_stop_words, ngram_range=(1,2), max_features=40000)\n",
    "\n",
    "    x_train= vectorizer.fit_transform(train_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    x_val = vectorizer.transform(validation_data['text'].apply(lambda x: ' '.join(x)))\n",
    "    x_test = vectorizer.transform(test_data['text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from gensim.models import Doc2Vec\n",
    "# from gensim.models.doc2vec import TaggedDocument\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# y_train = train_data['label']\n",
    "# y_val = validation_data['label']\n",
    "\n",
    "# def objective(trial):\n",
    "#     vector_size = trial.suggest_int(\"vector_size\", 50, 300)\n",
    "#     window = trial.suggest_int(\"window\", 3, 15)\n",
    "#     min_count = trial.suggest_int(\"min_count\", 1, 10)\n",
    "#     epochs = trial.suggest_int(\"epochs\", 10, 50)\n",
    "\n",
    "#     # Train Doc2Vec model\n",
    "#     tagged_data = [TaggedDocument(words=doc, tags=[str(label)]) for doc, label in zip(train_data['text'], train_data['label'])]\n",
    "#     doc2vec_model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
    "#     doc2vec_model.build_vocab(tagged_data)\n",
    "#     doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "#     # Prepare document vectors for training and test sets\n",
    "#     X_train_vecs  = [doc2vec_model.infer_vector(doc) for doc in train_data['text']]\n",
    "#     X_val_vecs = [doc2vec_model.infer_vector(doc) for doc in validation_data['text']]\n",
    "\n",
    "#     # Train Logistic Regression classifier\n",
    "#     classifier = LogisticRegression(max_iter=1000)\n",
    "#     classifier.fit(X_train_vecs, y_train)\n",
    "\n",
    "#     # Evaluate on test set\n",
    "#     y_pred = classifier.predict(X_val_vecs)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "# # Define the study object and optimize the objective function\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Print the best hyperparameters found\n",
    "# best_params = study.best_params\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# # Train the final model with the best hyperparameters\n",
    "# best_vector_size = best_params[\"vector_size\"]\n",
    "# best_window = best_params[\"window\"]\n",
    "# best_min_count = best_params[\"min_count\"]\n",
    "# best_epochs = best_params[\"epochs\"]\n",
    "\n",
    "# # Train Doc2Vec model with best hyperparameters\n",
    "# tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(train_data[\"text\"])]\n",
    "# best_doc2vec_model = Doc2Vec(vector_size=best_vector_size, window=best_window, min_count=best_min_count, epochs=best_epochs)\n",
    "# best_doc2vec_model.build_vocab(tagged_data)\n",
    "# best_doc2vec_model.train(tagged_data, total_examples=best_doc2vec_model.corpus_count, epochs=best_doc2vec_model.epochs)\n",
    "\n",
    "# # Prepare document vectors for training and test sets with the best Doc2Vec model\n",
    "# X_train_vecs  = [best_doc2vec_model.infer_vector(doc) for doc in train_data['text']]\n",
    "# X_val_vecs = [best_doc2vec_model.infer_vector(doc) for doc in validation_data['text']]\n",
    "\n",
    "# # Train Logistic Regression classifier on the final Doc2Vec vectors\n",
    "# final_classifier = LogisticRegression(max_iter=1000)\n",
    "# final_classifier.fit(X_train_vecs, y_train)\n",
    "\n",
    "# # Evaluate on test set\n",
    "# final_accuracy = final_classifier.score(X_val_vecs, y_val)\n",
    "# print(\"Final accuracy on test set with the best model:\", final_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "\n",
    "def train_doc2vec(train_data):\n",
    "    max_epochs = 16\n",
    "    vec_size = 88\n",
    "    alpha = 0.025\n",
    "    window = 4\n",
    "    min_count = 7\n",
    "\n",
    "    tagged_data = [TaggedDocument(words=doc, tags=[str(label)]) for doc, label in zip(train_data['text'], train_data['label'])]\n",
    "\n",
    "    # antes tinha workers definidos\n",
    "    model = Doc2Vec(vector_size=vec_size, window=window, min_count=min_count, epochs=max_epochs)\n",
    "    \n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=50)\n",
    "\n",
    "\n",
    "    model.save(\"d2v_best_stopwords.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    return model\n",
    "\n",
    "train_doc2vec(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def doc2vec(data, train_data, validation_data, test_data):\n",
    "\n",
    "    # training the doc2vec model\n",
    "    #model = train_doc2vec(train_data)\n",
    "    model = Doc2Vec.load(\"d2v_best_stopwords.model\")\n",
    "    \n",
    "    x_train = [model.infer_vector(doc) for doc in train_data['text']]\n",
    "    x_val = [model.infer_vector(doc) for doc in validation_data['text']]\n",
    "    x_test = [model.infer_vector(doc) for doc in test_data['text']]\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def lala(model, data):\n",
    "    vectors = []\n",
    "    for doc in data['text']:\n",
    "        document = []\n",
    "        for word in doc:\n",
    "            if word in model.wv:\n",
    "                document.append(model.wv[word])\n",
    "        vectors.append(document)\n",
    "    return vectors\n",
    "\n",
    "# test [d,c,c]  | label\n",
    "########################\n",
    "## test \n",
    "## [ [] [] [] ] | label\n",
    "\n",
    "def wordEmbeddingsVectorizer(data):\n",
    "\n",
    "    data_model = Word2Vec(data[\"text\"], vector_size=3, window=10, min_count=2, workers=10, sg=1)\n",
    "    \n",
    "    x_train = lala(data_model, train_data)\n",
    "    x_val =lala(data_model, validation_data)\n",
    "    x_test = lala(data_model, test_data)\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "lexicon_emotions = ['fear','anger', 'anticip', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy', 'anticipation']\n",
    "\n",
    "def get_features(data):\n",
    "    features = []\n",
    "    for text in data['text']:\n",
    "        text = ' '.join(text)\n",
    "        text = NRCLex(text)\n",
    "        features.append([text.affect_frequencies.get(emotion, 0) for emotion in lexicon_emotions])\n",
    "    return features\n",
    "\n",
    "# text_object = NRCLex(text='I am nervous')\n",
    "# print(text_object.affect_frequencies)\n",
    "# print(text_object.affect_dict)\n",
    "# print(text_object.top_emotions)\n",
    "# print(text_object.affect_list)\n",
    "# print(text_object.raw_emotion_scores)\n",
    "# print(text_object.lexicon)\n",
    "\n",
    "\n",
    "def lexiconVectorizer(train_data, validation_data, test_data):\n",
    "    x_train = get_features(train_data)\n",
    "    x_val = get_features(validation_data)\n",
    "    x_test = get_features(test_data)\n",
    "\n",
    "    y_train = train_data['label']\n",
    "    y_val = validation_data['label']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Choose vectorizer (featurizer)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = tfidfVectorizer(data, train_data, validation_data, test_data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = countVectorizer(data, train_data, validation_data, test_data)\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = wordEmbeddingsVectorizer(data)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = doc2vec(data, train_data, validation_data, test_data)\n",
    "x_train_lexicon, x_val_lexicon, x_test_lexicon, y_train, y_val, y_test = lexiconVectorizer(train_data, validation_data, test_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# join the features\n",
    "# import numpy as np\n",
    "\n",
    "x_train = np.concatenate((x_train, x_train_lexicon), axis=1)\n",
    "x_val = np.concatenate((x_val, x_val_lexicon), axis=1)\n",
    "x_test = np.concatenate((x_test, x_test_lexicon), axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply SMOTE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Apply SMOTE\n",
    "smote = SMOTE(random_state=4, sampling_strategy='auto', k_neighbors=18)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hypertunning number of features (using PCA) for each model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define parameters to search\n",
    "#     n_components = trial.suggest_int('n_components', 65, 88)\n",
    "\n",
    "#     # Define the pipeline with PCA and a classifier\n",
    "#     pipeline = Pipeline([\n",
    "#         ('pca', PCA(n_components=n_components)),\n",
    "#         ('clf', SVC())\n",
    "#         #('clf', LogisticRegression())\n",
    "#     ])\n",
    "\n",
    "#     # Fit the pipeline\n",
    "#     pipeline.fit(x_train, y_train)\n",
    "\n",
    "#     # Evaluate on the validation set\n",
    "#     y_pred = pipeline.predict(x_val)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "# # Optimize hyperparameters\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get the best parameters found\n",
    "# best_params = study.best_params\n",
    "\n",
    "# # Get the best model\n",
    "# best_n_components = best_params['n_components']\n",
    "# best_model = Pipeline([\n",
    "#     ('pca', PCA(n_components=best_n_components)),\n",
    "#     ('clf', )\n",
    "# ])\n",
    "# best_model.fit(x_train, y_train)\n",
    "\n",
    "# # Use the best model for prediction\n",
    "# y_pred = best_model.predict(x_test)\n",
    "\n",
    "# # Evaluate the best model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Test Accuracy:\", accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1. Model Selection\n",
    "Aqui também só estava a querer espetar modelos para começar a ver o que dá que ainda não sei que features vão ser usadas:\n",
    "tf-idf, word embeddings, ???, features mais feitas à mão?\n",
    "\n",
    "\n",
    "Isto pelos vistos é uma cena, que não implementei (ainda..)\n",
    "\n",
    "\"The validation set uses a subset of the training data to provide an unbiased evaluation of a model. The validation data set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it. It is in this phase that hyperparameter tuning occurs.\"\n",
    "\n",
    "Wikipedia:\n",
    "The basic process of using a validation data set for model selection (as part of training data set, validation data set, and test data set) is:\n",
    "\n",
    "Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.\n",
    "\n",
    "An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model hypertunning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define objective function for Optuna\n",
    "# # Define objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     # Define solver\n",
    "#     solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "\n",
    "#     if solver == 'lbfgs':\n",
    "#         penalty = 'l2'  # LBFGS only supports L2 penalty\n",
    "#     else:\n",
    "#         penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "\n",
    "#     C = trial.suggest_loguniform('C', 0.01, 10)\n",
    "\n",
    "#     # Instantiate Logistic Regression classifier with hyperparameters\n",
    "#     logreg_classifier = LogisticRegression(max_iter=1000, solver=solver, penalty=penalty, C=C)\n",
    "\n",
    "#     # Train classifier\n",
    "#     logreg_classifier.fit(x_train, y_train)\n",
    "\n",
    "#     # Predict on validation set\n",
    "#     y_pred = logreg_classifier.predict(x_val)\n",
    "\n",
    "#     # Calculate accuracy score\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "# # Create Optuna study\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get best hyperparameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# print(best_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# pca = PCA(n_components=77)\n",
    "# x_train = pca.fit_transform(x_train)\n",
    "# x_val = pca.transform(x_val)\n",
    "# x_test = pca.transform(x_test)\n",
    "\n",
    "logreg_classifier = LogisticRegression(solver='liblinear', penalty='l2', C=0.03784347512520397)\n",
    "logreg_classifier.fit(x_train, y_train)\n",
    "y_pred = logreg_classifier.predict(x_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 3.1.2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# mnb_classifier = MultinomialNB()\n",
    "# mnb_classifier.fit(x_train, y_train)\n",
    "# y_pred = mnb_classifier.predict(x_val)\n",
    "# print(accuracy_score(y_val, y_pred))\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(confusion_matrix(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Hypertunning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     # Define kernel\n",
    "#     kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "\n",
    "#     # For 'poly', 'rbf', and 'sigmoid' kernels, suggest degree parameter\n",
    "#     if kernel in ['poly', 'rbf', 'sigmoid']:\n",
    "#         degree = trial.suggest_int('degree', 2, 5)\n",
    "#     else:\n",
    "#         degree = 3  # Default degree value for kernels that don't use it\n",
    "\n",
    "#     # Instantiate SVM classifier with hyperparameters\n",
    "#     svm_classifier = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "#     # Train classifier\n",
    "#     svm_classifier.fit(x_train, y_train)\n",
    "\n",
    "#     # Predict on validation set\n",
    "#     y_pred = svm_classifier.predict(x_val)\n",
    "\n",
    "#     # Calculate accuracy score\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "# # Create Optuna study\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get best hyperparameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# print(best_params)\n",
    "\n",
    "# #[I 2024-04-01 12:39:28,014] Trial 36 finished with value: 0.8245 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 36 with value: 0.8245.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "svm_classifier = SVC(kernel='poly', degree=2, random_state=42)\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "y_pred = svm_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Hypertunning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to optimize\n",
    "#     n_neighbors = trial.suggest_int('n_neighbors', 1, 10)\n",
    "#     weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "#     algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "\n",
    "#     # Instantiate KNN classifier with hyperparameters\n",
    "#     knn_classifier = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)\n",
    "\n",
    "#     # Train classifier\n",
    "#     knn_classifier.fit(x_train, y_train)\n",
    "\n",
    "#     # Predict on validation set\n",
    "#     y_pred = knn_classifier.predict(x_val)\n",
    "\n",
    "#     # Calculate accuracy score\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "# # Create Optuna study\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# # Get best hyperparameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# print(best_params)\n",
    "\n",
    "# # [I 2024-04-01 13:53:46,403] Trial 26 finished with value: 0.701 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 26 with value: 0.701."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "hyperparameters = {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto'}\n",
    "knn_classifier = KNeighborsClassifier(**hyperparameters)\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "y_pred = knn_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Boosting Algorithms\n",
    "\n",
    "Testing with some boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# XGBoost Classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for XGBoost\n",
    "accuracy_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "report_xgb = classification_report(y_val, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_val, y_pred_xgb)\n",
    "\n",
    "# Print the evaluation metrics for XGBoost\n",
    "print(\"XGBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_xgb)\n",
    "print(\"Classification Report:\\n\", report_xgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier()\n",
    "lgb_classifier.fit(x_train, y_train)\n",
    "y_pred_lgb = lgb_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for LightGBM\n",
    "accuracy_lgb = accuracy_score(y_val, y_pred_lgb)\n",
    "report_lgb = classification_report(y_val, y_pred_lgb)\n",
    "conf_matrix_lgb = confusion_matrix(y_val, y_pred_lgb)\n",
    "\n",
    "# Print the evaluation metrics for LightGBM\n",
    "print(\"\\nLightGBM Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_lgb)\n",
    "print(\"Classification Report:\\n\", report_lgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost Classifier\n",
    "adaboost_classifier = AdaBoostClassifier()\n",
    "adaboost_classifier.fit(x_train, y_train)\n",
    "y_pred_adaboost = adaboost_classifier.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_val, y_pred_adaboost)\n",
    "report_adaboost = classification_report(y_val, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(y_val, y_pred_adaboost)\n",
    "\n",
    "# Print the evaluation metrics for AdaBoost\n",
    "print(\"AdaBoost Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_adaboost)\n",
    "print(\"Classification Report:\\n\", report_adaboost)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_adaboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Bagging Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "\n",
    "model1 = xgb_classifier\n",
    "model2 = svm_classifier\n",
    "model3 = logreg_classifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('model1', model1), \n",
    "    ('model2', model2),\n",
    "    ('model3', model3)\n",
    "], voting='soft')\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict the validation set\n",
    "y_pred_voting = voting_clf.predict(x_val)\n",
    "\n",
    "# Evaluate the performance for the ensemble model\n",
    "accuracy_voting = accuracy_score(y_val, y_pred_voting)\n",
    "report_voting = classification_report(y_val, y_pred_voting)\n",
    "conf_matrix_voting = confusion_matrix(y_val, y_pred_voting)\n",
    "\n",
    "# Print the evaluation metrics for the ensemble model\n",
    "print(\"Voting Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_voting)\n",
    "print(\"Classification Report:\\n\", report_voting)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate with test set\n",
    "y_pred = logreg_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_logreg = logreg_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred = mnb_classifier.predict(x_test)\n",
    "# print(accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = svm_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = xgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = lgb_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = adaboost_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = knn_classifier.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. Cause of errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotions_to_int = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'love': 2,\n",
    "    'anger': 3,\n",
    "    'fear': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "int_to_emotions = {v: k for k, v in emotions_to_int.items()}\n",
    "\n",
    "\n",
    "wrong_predictions = y_test[y_test != y_pred_logreg].index\n",
    "for i, index in enumerate(wrong_predictions):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(test_data['text'][index])\n",
    "    print('Real: ', int_to_emotions[test_data['label'][index]])\n",
    "    print('Pred:', int_to_emotions[y_pred_logreg[index]])\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check which emotions are being confused\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "conf_matrix_copy = conf_matrix.copy()\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    conf_matrix[i, i] = 0 # make the diagonal null, to not eclipse the other values\n",
    "    \n",
    "plt.imshow(conf_matrix, cmap='viridis', interpolation='nearest')\n",
    "# annotate the axes with the emotion names\n",
    "plt.xticks(range(6), int_to_emotions.values(), rotation=45)\n",
    "plt.yticks(range(6), int_to_emotions.values())\n",
    "# add colorbar more to the right\n",
    "# plt.colorbar()\n",
    "# legend the axes with predicted and true values\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# add counts in the plot\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        if i == j:\n",
    "            plt.text(j, i, conf_matrix_copy[i, j], ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j, i, conf_matrix[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "for i in range(conf_matrix.shape[0]): # this does not count correct predictions\n",
    "    plt.text(6, i, conf_matrix[i, :].sum(), ha='center', va='center', color='black')\n",
    "for i in range(conf_matrix.shape[1]):\n",
    "    plt.text(i, 7, conf_matrix[:, i].sum(), ha='center', va='center', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test set appears to be unbalance.\n",
    "- 'joy' is a lot more mixed with 'love' than the contrary. \n",
    "- 'surprise' has a low Precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "conf_matrix = conf_matrix / conf_matrix.sum(axis=1)[:, None] # normalize the confusion matrix\n",
    "conf_matrix_copy = conf_matrix.copy()\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    conf_matrix[i, i] = 0 # make the diagonal null, to not eclipse the other values\n",
    "plt.imshow(conf_matrix, cmap='viridis', interpolation='nearest')\n",
    "\n",
    "plt.xticks(range(6), int_to_emotions.values(), rotation=45)\n",
    "plt.yticks(range(6), int_to_emotions.values())\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# add counts in the plot\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        if i == j:\n",
    "            plt.text(j, i, f'{conf_matrix_copy[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j, i, f'{conf_matrix[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "        \n",
    "plt.title('Percentage of predictions (row sum=1)')\n",
    "# Rows add to 1\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- sadness mixed with joy and anger\n",
    "- joy mixed with love\n",
    "- love mixed with joy\n",
    "- anger mixed with sadness and joy??\n",
    "- fear mixed with sadness\n",
    "- surprise mixed with almost everything"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Experiment with user-inputed setences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "emotions = {\n",
    "    0: \"0. sadness\",\n",
    "    1: \"1. joy\",\n",
    "    2: \"2. love\",\n",
    "    3: \"3. anger\",\n",
    "    4: \"4. fear\",\n",
    "    5: \"5. surprise\"\n",
    "}\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(phrase.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    my_stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody', 'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without','hereby'])\n",
    "    my_stop_words -= words_to_keep\n",
    "    tokens = [word for word in tokens if word not in my_stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def doc2vec_text(tokens):\n",
    "    model = Doc2Vec.load(\"d2v_best_stopwords.model\")\n",
    "    return model.infer_vector(tokens)\n",
    "\n",
    "def classify_emotion(number):\n",
    "    if number==0:\n",
    "        return \"0. sadness\"\n",
    "    if number==1:\n",
    "        return \"1. joy\"\n",
    "    if number==2:\n",
    "        return \"2. love\"\n",
    "    if number==3:\n",
    "        return \"3. anger\"\n",
    "    if number==4:\n",
    "        return \"4. fear\"\n",
    "    if number==5:\n",
    "        return \"5. surprise\"\n",
    "\n",
    "    return \"Not possible to identify\"\n",
    "\n",
    "\n",
    "def analyze_sentiment(phrase):\n",
    "    tokens = preprocess_text(phrase)\n",
    "    tokens_embeddings = doc2vec_text(tokens)\n",
    "\n",
    "    # Reshape to have proper structure\n",
    "    tokens_embeddings = np.array(tokens_embeddings).reshape(1, -1)\n",
    "\n",
    "    # Predict the class using the SVM classifier\n",
    "    predicted_class = svm_classifier.predict(tokens_embeddings)\n",
    "    print(classify_emotion(predicted_class[0]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "phrase = \"i love love\"\n",
    "analyze_sentiment(phrase)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
