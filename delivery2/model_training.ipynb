{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_json('../data/data.jsonl', lines=True)\n",
    "test_data = pd.read_json('../data/test_final.jsonl', lines=True)\n",
    "train_data = pd.read_json('../data/train_final.jsonl', lines=True)\n",
    "validation_data = pd.read_json('../data/validation_final.jsonl', lines=True)\n",
    "\n",
    "# Remove duplicates\n",
    "test_data = test_data.drop_duplicates(subset=['text'])\n",
    "train_data = train_data.drop_duplicates(subset=['text'])\n",
    "validation_data = validation_data.drop_duplicates(subset=['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody',\n",
    "                           'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without'])\n",
    "my_stop_words = my_stop_words - words_to_keep\n",
    "\n",
    "def pre_process_data(dataset):\n",
    "    # Tokenize\n",
    "    #dataset['text'] = dataset['text'].apply(word_tokenize)\n",
    "    # Remove stop words\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word.lower() not in my_stop_words])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)\n",
    "test_data = pre_process_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Using BERT Tokenizer:\n",
    "BERT tokenizer requires the data to be in a specific format\n",
    "\n",
    "???????? MAS ESTÁ CERTO ANTES TAMBEM TOKENIZARMOS ????????\n",
    "\n",
    "\n",
    "* podemos usar o bert-base-uncased e o roberta-based e comparar dps\n",
    "* maybe usar embeddings tb depois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# SUPOSTAMENTE ESTE TB É BOM\n",
    "# model_name = \"roberta-base\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_data(train_data['text'].apply(lambda x: ' '.join(x)).tolist())\n",
    "val_encodings = tokenize_data(validation_data['text'].apply(lambda x: ' '.join(x)).tolist())\n",
    "test_encodings = tokenize_data(test_data['text'].apply(lambda x: ' '.join(x)).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_data['label'].tolist())\n",
    "val_dataset = TextDataset(val_encodings, validation_data['label'].tolist())\n",
    "test_dataset = TextDataset(test_encodings, test_data['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training hyppertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-13 01:00:14,613] A new study created in memory with name: no-name-45fc02cc-2e55-44fe-a813-1277121754bd\n",
      "c:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 38%|███▊      | 500/1332 [10:19:12<5:43:29, 24.77s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7813, 'grad_norm': 2.827212333679199, 'learning_rate': 2.448815774777256e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1000/1332 [17:21:11<1:42:51, 18.59s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7559, 'grad_norm': 2.259033203125, 'learning_rate': 9.77171679358232e-06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "100%|██████████| 1332/1332 [20:19:12<00:00, 23.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7080078125, 'eval_accuracy': 0.2482400889218229, 'eval_f1': 0.21928267292887021, 'eval_precision': 0.2784160033234338, 'eval_recall': 0.2482400889218229, 'eval_runtime': 1645.0227, 'eval_samples_per_second': 3.281, 'eval_steps_per_second': 0.41, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1332/1332 [20:19:16<00:00, 54.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 73156.1427, 'train_samples_per_second': 0.582, 'train_steps_per_second': 0.018, 'train_loss': 1.7605178878830001, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [33:26<00:00,  2.97s/it]    \n",
      "[I 2024-05-13 21:53:01,933] Trial 0 finished with value: 0.7517599110781771 and parameters: {'learning_rate': 3.92045987019628e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.7517599110781771.\n",
      "c:\\Users\\inesc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 19%|█▉        | 500/2663 [11:33:09<8:52:05, 14.76s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.801, 'grad_norm': 2.984633684158325, 'learning_rate': 3.0767038573243956e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1000/2663 [13:41:11<6:39:52, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7963, 'grad_norm': 3.174362897872925, 'learning_rate': 2.3654916850348915e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 1209/2663 [18:20:33<306:16:13, 758.30s/it]  "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
    "    \n",
    "    # Define TrainingArguments with hyperparameters from the trial\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # Optuna aims to minimize the objective, so if accuracy is the metric, return 1 - accuracy\n",
    "    return 1 - eval_result[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial_ = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial_.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial_.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels range:  0 to 5\n",
      "Validation labels range:  0 to 5\n",
      "Test labels range:  0 to 5\n",
      "Training data NaN values: False\n",
      "Validation data NaN values: False\n",
      "Test data NaN values: False\n",
      "1\n",
      "<transformers.trainer.Trainer object at 0x00000264BB079790>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    " # Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(\"Training labels range: \", min(train_data['label']), \"to\", max(train_data['label']))\n",
    "print(\"Validation labels range: \", min(validation_data['label']), \"to\", max(validation_data['label']))\n",
    "print(\"Test labels range: \", min(test_data['label']), \"to\", max(test_data['label']))\n",
    "\n",
    "print(\"Training data NaN values:\", train_data.isnull().values.any())\n",
    "print(\"Validation data NaN values:\", validation_data.isnull().values.any())\n",
    "print(\"Test data NaN values:\", test_data.isnull().values.any())\n",
    "\n",
    " # Initialize Trainer\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "print(os.getenv('CUDA_LAUNCH_BLOCKING'))\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7989 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  6%|▋         | 500/7989 [03:25<1:13:50,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7972, 'learning_rate': 1.8748278883464764e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1000/7989 [08:20<1:11:11,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7642, 'learning_rate': 1.749655776692953e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/7989 [13:28<1:00:43,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6429, 'learning_rate': 1.6244836650394292e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2000/7989 [18:29<1:06:08,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4076, 'learning_rate': 1.4993115533859057e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 2500/7989 [23:40<55:58,  1.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1916, 'learning_rate': 1.3741394417323821e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|███▎      | 2663/7989 [26:33<53:56,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1756267547607422, 'eval_accuracy': 0.5679881437569471, 'eval_f1': 0.5669536376467866, 'eval_precision': 0.5850901822610035, 'eval_recall': 0.5679881437569471, 'eval_runtime': 73.395, 'eval_samples_per_second': 73.547, 'eval_steps_per_second': 4.605, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3000/7989 [29:55<49:32,  1.68it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0038, 'learning_rate': 1.2489673300788585e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 3500/7989 [35:23<49:49,  1.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8031, 'learning_rate': 1.123795218425335e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4000/7989 [40:58<43:21,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7058, 'learning_rate': 9.986231067718113e-06, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 4500/7989 [46:34<39:21,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6119, 'learning_rate': 8.734509951182877e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 5000/7989 [52:16<33:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5418, 'learning_rate': 7.482788834647641e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 5326/7989 [57:11<27:35,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5322026610374451, 'eval_accuracy': 0.8206743238236384, 'eval_f1': 0.8202737266200173, 'eval_precision': 0.8313848720717878, 'eval_recall': 0.8206743238236384, 'eval_runtime': 76.4421, 'eval_samples_per_second': 70.616, 'eval_steps_per_second': 4.422, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 5500/7989 [59:02<25:44,  1.61it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4673, 'learning_rate': 6.231067718112405e-06, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6000/7989 [1:04:48<28:37,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4155, 'learning_rate': 4.979346601577169e-06, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 6500/7989 [1:12:50<26:42,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3866, 'learning_rate': 3.727625485041933e-06, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7000/7989 [1:23:21<22:31,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3762, 'learning_rate': 2.475904368506697e-06, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 7500/7989 [1:34:41<11:11,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3519, 'learning_rate': 1.2241832519714608e-06, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 7989/7989 [1:48:27<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4559192657470703, 'eval_accuracy': 0.856428306780289, 'eval_f1': 0.857196278947809, 'eval_precision': 0.8695140701219711, 'eval_recall': 0.856428306780289, 'eval_runtime': 158.0496, 'eval_samples_per_second': 34.154, 'eval_steps_per_second': 2.139, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7989/7989 [1:48:29<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6509.7128, 'train_samples_per_second': 19.635, 'train_steps_per_second': 1.227, 'train_loss': 0.8646236930118513, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338/338 [02:35<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4559192657470703, 'eval_accuracy': 0.856428306780289, 'eval_f1': 0.857196278947809, 'eval_precision': 0.8695140701219711, 'eval_recall': 0.856428306780289, 'eval_runtime': 155.5011, 'eval_samples_per_second': 34.714, 'eval_steps_per_second': 2.174, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
