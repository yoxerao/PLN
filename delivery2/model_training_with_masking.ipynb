{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:32:32.050365Z",
     "start_time": "2024-05-21T14:32:10.662221Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jcarv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcarv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jcarv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from datasets import DatasetDict\n",
    "from sklearn.feature_extraction import text\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:34:31.316481Z",
     "start_time": "2024-05-21T14:34:31.309461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! \n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! \")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "    \n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:34:35.000476Z",
     "start_time": "2024-05-21T14:34:33.588531Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_json('../data/data.jsonl', lines=True)\n",
    "test_data = pd.read_json('../data/test_final.jsonl', lines=True)\n",
    "train_data = pd.read_json('../data/train_final.jsonl', lines=True)\n",
    "validation_data = pd.read_json('../data/validation_final.jsonl', lines=True)\n",
    "\n",
    "# Remove duplicates\n",
    "test_data = test_data.drop_duplicates(subset=['text'])\n",
    "train_data = train_data.drop_duplicates(subset=['text'])\n",
    "validation_data = validation_data.drop_duplicates(subset=['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:35:13.547690Z",
     "start_time": "2024-05-21T15:35:09.755155Z"
    }
   },
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "words_to_keep = frozenset(['no', 'couldnt', 'cry', 'not', 'cant', 'cannot', 'nor', 'except', 'nobody',\n",
    "                           'off', 'but', 'serious', 'enough', 'nothing', 'alone', 'down', 'only', 'without'])\n",
    "my_stop_words = my_stop_words - words_to_keep\n",
    "\n",
    "def pre_process_data(dataset):\n",
    "    # Remove stop words\n",
    "    dataset['text'] = dataset['text'].apply(\n",
    "        lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in my_stop_words])\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_data = pre_process_data(train_data)\n",
    "validation_data = pre_process_data(validation_data)\n",
    "test_data = pre_process_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  label\n0     feel awful s job position succeed just didn t ...      0\n1                                   im alone feel awful      0\n2     ive probably mentioned but really feel proud a...      1\n3                               feeling little low days      0\n4     beleive sensitive peoples feelings tend compas...      2\n...                                                 ...    ...\n5395                   feel grumpy haven t yoga ed days      3\n5396  read blog suburb direction mentioned casually ...      3\n5397  not feel things realize violent physical suffe...      3\n5398  feel petty silly giving shit but little things...      3\n5399  remember feeling bitter couldnt pop balloons j...      3\n\n[5396 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feel awful s job position succeed just didn t ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>im alone feel awful</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ive probably mentioned but really feel proud a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feeling little low days</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beleive sensitive peoples feelings tend compas...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5395</th>\n      <td>feel grumpy haven t yoga ed days</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5396</th>\n      <td>read blog suburb direction mentioned casually ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5397</th>\n      <td>not feel things realize violent physical suffe...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5398</th>\n      <td>feel petty silly giving shit but little things...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5399</th>\n      <td>remember feeling bitter couldnt pop balloons j...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5396 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:35:15.646732Z",
     "start_time": "2024-05-21T15:35:15.638706Z"
    }
   },
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Using pre-trained model Tokenizer:\n",
    "The model tokenizer requires the data to be in a specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:50:56.356539Z",
     "start_time": "2024-05-21T15:50:50.569058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model tokenizer and model\n",
    "\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "model_name = \"roberta-base\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "# model_name = f\"./roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6).to(device)\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts, padding='max_length', return_tensors='pt', truncation=True, max_length=128).to(device)\n",
    "\n",
    "train_encodings = tokenize_data(train_data['text'].tolist())\n",
    "val_encodings = tokenize_data(validation_data['text'].tolist())\n",
    "test_encodings = tokenize_data(test_data['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['feel awful s job position succeed just didn t happen',\n 'im alone feel awful',\n 'ive probably mentioned but really feel proud actually keeping new years resolution monthly weekly goals',\n 'feeling little low days',\n 'beleive sensitive peoples feelings tend compassionate',\n 'frustrated christians feel constantly talk loving praying seen not case',\n 'people feels like going gym only worthwhile hour',\n 'feel especially pleased long time coming',\n 'struggling awful feelings saying sweet things not deserving sisters friendship agreed car just starting drive away reached hand',\n 'feel enraged but helpless time',\n 'said feeling bit rebellious',\n 'feel disillusioned claimed value truth fraud',\n 'mean stupid trip making great album things going feel ecstatic',\n 'woke feeling particularly vile tried ignore but got worse worse worse',\n 'feel vile moth burrowing way brain seeking brain means control enslave just nasty bug things did chekov star trek wrath khan',\n 'know just doing job doesnt actually thoughts feelings but little jingle does end cycle just makes excited load cant help but sing',\n 'wish knew word write write think s useless m just heartless feeling s w',\n 'feel assured no thing ultimate forgetting traces impressed memory indestructible',\n 'feel blessed everyday little man love watch grow',\n 'feel exhausted home but glad learning new things help learn',\n 'stil kaibas face tv screen amateur duelists prowling good city feel urge throttle idiotic mortals dare look eye',\n 'don t like feel uncomfortable alone quiet',\n 'left kinda feeling stupid insecure',\n 'alternate feeling sympathetic humanity misanthrope',\n 'pretty bad feeling books rushed terms story',\n 'sincerely believe client celebrity not deserves attention leaves feeling glamorous arrived',\n 'don t feel comfortable',\n 'happy don t feel burdened consumed',\n 'not dead but things not said without feeling humiliated',\n 'continually surprised need eat feel truly satisfied im eating good food sitting down lovely little experience meal satisfying soul stomach',\n 'pretty waddled hospital feeling weird lightheaded but ok',\n 'apologize day lack faith ask feel uncertain future kiev',\n 'searching direction but feel slightly assured hopeful',\n 'angry feel like life not carefree want',\n 'feel passionate today',\n 'only having day week feeling depressed seriously anxious',\n 'article published',\n 'things clearly depth peace makes feel unsure',\n 'feel relax little wouldn t friendly going tear off strip but mind working overtime trying work does want',\n 'feel doubtful place world technology applicable value work helpful think doug engelbart listening interviews goals genesis goals dogged perseverance meeting plan',\n 'im feeling rotten dont post new material doesnt mean shouldnt reading youre feeling rotten',\n 'feeling theyre probably not going thrilled',\n 'ive feeling pretty shitty pretty shitty mood past days',\n 'feel glad law only journey start weekend sojourn city pines',\n 'im looking right desperately looking creative project feel like hand need sit write fucking mother fucker fuckity fuckers fucked play feel satisfied doing',\n 'really hoping target soon glad im feeling positive week',\n 'dont know feel appreciative talent depressed suck',\n 'feel ok',\n 'feel sickened disgusted sins man despite divinity feel sickened disgusted sins man',\n 'feel nothing smart interesting write days',\n 'feel blessed grateful today',\n 'not feeling compassionate need help just want fix problems dont figure parts need long job emergency',\n 'week appointment yesterday left feeling little defeated',\n 'know isnt true but feel like unimportant insignificant person face earth',\n 'feel numb right hard express exactly feel but went little harrison walk nonetheless clear heads',\n 'feel makes brave',\n 'feeling incredibly thankful year children',\n 'write m sipping martini probably m feeling generous',\n 'recommend watching feel amazed inspired',\n 't really eloquent words m trying say just feel listless useless meaningless ve failed life',\n 'feel really funny like people',\n 'feel awful happened actually needed hear say just said world',\n 'feel divine strong',\n 'say feel amazed',\n 'actually think fairly authentic person generally pretty honest opinions desires lie white lies but comes deep dark close home negative feelings gut oh repressed fear',\n 'slept horribly m feeling doubtful like t manage figure save money',\n 'feel hopeful times not hopeful',\n 'just supposed read bible think god wants feel peaceful thats',\n 'hated growing no good memories life innocent child only feeling hated worthlessness consume memories',\n 'having really badly not wear without causing spasms diarrhea eat mouthfuls feeling miserable',\n 'kid brother broke reading spectacles',\n 'feel mouth water devoted energy evoking yelps lips',\n 'im feeling worthwhile learn terminology improve ability communicate',\n 'don t feel afraid m nurse m doing job',\n 'feeling confident work kids teach',\n 'im feeling scared',\n 'feeling really cute happy church sunday thought itd good time belly picture',\n 'feel burdened yuchun junsu doing',\n 'really say m feeling remorseful regretful ve',\n 'im associated despite feel protest whilst reaping benefits relatively peaceful society',\n 'feeling walk pressured induce but just remain strong face pressure',\n 'feel like m frolicking away precious time future',\n 'feel like useful tools',\n 'feel like im kind defective shell prevents utilizing know using tools given',\n 'feel dont know suppose',\n 'genuinely feel characters keen uncover story',\n 'im feeling adventurous toss scoop protein powder extra punch nutrition',\n 'feel awkward rose',\n 'want generous but feel greedy',\n 'feel unsure book ending',\n 'really liked feeling beloved liked except hes away missed',\n 'feel like talking snobbish uppity accent responding let just say lot flipping hair trying pull cheese wiz',\n 'don t feel needy desperate prove things',\n 'feel bad mum carries',\n 'feeling people really outgoing people feel like naturally',\n 'feeling pretty discouraged friend shared verse god hope joy peace trust overflow hope power holy spirit',\n 'try not dismissive elections feel privileged right dismissive elections but s really hard excited election half races ballot folks running completely unopposed',\n 'im feeling low creative play',\n 'feeling quite distressed approaching meal concern',\n 'feel foolish ashamed',\n 'feeling ok ish decided carry group set off later but catching quickly',\n 'feel embarrassed mean wise',\n 'feel labyrinths approached solemn purposes',\n 'hate feeling needy knowing no help feel better only thing make happy only thing need doug',\n 'know feel agitated like volcanos sudden movement thinking cant stop',\n 'realise date feel alarmed time start living',\n 'feel everybody eager competition s feeling strongly',\n 'feeling paranoid not wanting leave house',\n 'want people laugh whilst watching feel little suspicious happen',\n 'definitely dont want live forever but stay looking feeling hit maybe years earth bite thatd cool',\n 'feel rich table people sit eat',\n 'took blurry kind captures feeling relieved bliss',\n 'told designer fitting im feeling like dont belong job friends creative aspirations',\n 'feeling miserable plus eyes itch hot',\n 'reaching lot light nourishing leaves skin feeling amazing morning',\n 'feel honoured excited introduce incredibly nice travelled oh stylish href http silkyway',\n 'don t feel gentle href http www',\n 'cards leaves past feel heartless throw away',\n 'think force disappointment feel rejected job wanting shield replace academia job job actually want not',\n 'feel dissatisfied ive doing nothing',\n 'feel hardening not gorgeous breasts sucking',\n 'feel ugly certainly days dont want look act best',\n 'just feel like whiney bitch',\n 'say definitely feel like ive intelligent semester',\n 'remember feeling little jaded trip abroad like america betrayed',\n 'feel romantic just hand hand couple without kids alone free just',\n 'feel amazing putting nd class said newly frocked interior communication electrician nd class rachel rice',\n 'know im way knowledgeable feel like amazing stage life',\n 'attempt hair coloring session later m feeling brave crazy saturday nights',\n 'im going valentines day banquet tonight just got makeup hair feel gorgeous',\n 'feel honoured takes time day let know ve read posts',\n 'feel absolutely sufficed equally determined',\n 'feeling prom suck signalled gloomy clouds rainfall',\n 'feel like puppets obstacle distracting story drama supporting adding',\n 'think just feels bit threatened like guys watch off kind intruding',\n 'woke half way home vaguely remember feeling lost',\n 'pulled roadside forest sat tree relax bit reminding s journey not destination feeling sure saying invented completely lost',\n 'feel brow beaten bbc comes attack ve pumped mr lyons words wordle',\n 'feeling particularly gloomy weather worst effect didn',\n 'feel bad people try figure dont know',\n 'im feeling low going difficult time close friends choose moody weird',\n 'feeling really needy attention maybe ryan working lot home day',\n 'look like im albino toned point turning grey feeling little skeptical time hairdresser',\n 'feeling agitated departed home couldn t attest particular reason experiencing erratic type emotions time',\n 'try convince says t feel vain',\n 'wish joy feel receive letter precious children',\n 'remember year feeling thankful but having nudge remember thankful',\n 'reading writing fall quarter say decision overwork dawley text fucked but feel ought shaken small thing',\n 'love reading comments feel free leave',\n 'not know not experience buying shoes online true feel joy store not happy',\n 'feel bothered fact given chances but don t bit appreciation utter gratefulness downright souls',\n 'proud change makes feel good know broke free addiction',\n 'supposed ought feel thankful adding sarcastic edge age',\n 'constantly feel anxious twinge start great',\n 'not think lack esteem confident capabilities feel generally respected professional personal life',\n 'love feel passionate',\n 'finally signed petition feel kind hopeful',\n 'feel kind listless frustrated but better',\n 'know m going quit feel frantic smoke',\n 'feeling pleasant surprise',\n 'im feeling inside but just need calm down relax',\n 'held water large wave thought drowning',\n 'im not bad mood but feel messy unmotivated',\n 'captivating feel energetic motions euphoric feelings advisable work',\n 'feel decide m delighted fans paintings able body work m proud',\n 'feel honored but says come queenie time retire royal bedchamber',\n 'feel disgusted seeing photography',\n 'feel blessed surrounded great coaches players administrators worked relentlessly dedicated institution',\n 'feel doomed repeat horrors great depression wiemar republic depression',\n 'feeling jealous said no im not jealous know stand',\n 'feel like petty girl whos wasting time lost cause',\n 'feel arisnote useful program',\n 'help but feel deeply doubtful decision come india',\n 'started feel horny responded kisses',\n 'feel artistic maelstrom swirling',\n 'trepidation forcing issue but prayed not only honest feelings convinced right',\n 'feel bit discontent passed',\n 'feeling ecstatic feeling laid didnt care anymore',\n 'reminded smooth travel depends luck time not feel like rich person handed fortunes just scraped',\n 'took car camera scrapyard despite feeling intimidated surrounds asked pics got ok did just',\n 'write words feel solemn',\n 'feel fearful want save things people places joyfully drag center',\n 'feeling mellow',\n 'feel glad blog helpful knowledgeable explorable',\n 'im feeling unloved unwanted only person world loves child',\n 'fear exam',\n 'sure feel rich right',\n 'really enjoy physics makes think analyse gets frustrating feeling solving problem just wonderful',\n 'im quick feel guilt innocent',\n 'feeling confident vanessa mamen succeeded reminding came',\n 'eating way feel feel groggy without caffeine wake ok long dont heartburn headaches',\n 'feeling scared cause im wizard wahahaha',\n 'didn t feel lively february th goto comments leave comment',\n 'honor higher self feel alone',\n 'feel rich no money speak',\n 'wanted world feel calm',\n 'knew way finish line but didn t feel like fact felt like run suffering end',\n 'feel pressured dumb feeling',\n 'optimistic things simple things right feel jaded worn',\n 'im feeling loving homeschooling gig space comes',\n 'feel lame just writing life things',\n 'feel shitty im work not doing not contributing line im not motivated feel like leech but paid dont turn down',\n 'feel strong confident powerful',\n 'feel lethargic drained',\n 'im guessing nutrient poor carb dinner night feeling crappy huge hunger',\n 'feel generous think art dont necessarily but good art high art',\n 'feel angry',\n 'feels kareena make fantastic cop',\n 'heard myth long time ago went teacher joked going eating feeling bit homesick',\n 'feel like going blamed',\n 'feel bit paranoid like people judging',\n 'feel pretty crap prizes fake boooo',\n 'feel s just really boring end playing good vs good match happen wouldn t want play legolas look legolasses board s boring consider painting hobby',\n 'feel like need apologize finally convinced love',\n 'feel quit job hated class thumbnail width',\n 'just didn t feel m afraid',\n 'finally feel excited continue try lose weight',\n 'feel quite bad yihan feel bad',\n 'feel face cleaner looks radiant',\n 'feel hiccups cute',\n 'feel like im ungrateful cause born happy loving family hardly trouble whatsoever',\n 'feel like not respected',\n 'feel types people dangerous situation',\n 'feel lonely walk school lot shopping mall',\n 'just feel like im nothing special',\n 'feel awkward accept reward want gave',\n 'imagine sitting party really care trying tell feel waiting longing moment feeling mixed feelings',\n 'im feeling optimistic regards ease shorts',\n 'feeling helpless',\n 'feel tender tentative unfurling innocent desire mate',\n 'im not typing right feeling lethargic buzzed food drink like old joe',\n 'feeling kind regretful didnt try drill team but think make jv drill team itll better schedule ib stuff ooh today got letter arita friends thailand cute',\n 'im doctors tomorrow results blood test pick prescription im feeling bad tomorrow right im going mention hopefully appointment',\n 'ive provided understand exactly feel im discouraged ready',\n 'scent pretty generic actually feel like bath amp bodyworks didnt invest time collection like created sweet paris decided throw predictable scents',\n 'feel generous giveaway copies copyrighted works dont like address issue like but work hard produce books',\n 'march fellow students borrowed wristwatch broke glass watch got angry telling but anger didnt long fade',\n 'feel enraged people happy',\n 'kept quiet feeling little foolish quick jump conclusion',\n 'im sitting shitting bricks feeling completely terrified nervous im going make mistake cost raid achievement week',\n 'feel energetic disconnect engaging technology',\n 'sort sense but time signs point away m left just feeling confused misdirected',\n 'feel responsibility suffering',\n 'feel unimportant',\n 'want amicable easy relations good feeling supportive neighbours',\n 'say im feeling low right im trying trying but hate slow progress',\n 'feel little sentimental probably but end hour took heres href http',\n 'fighting mind trying tell not feel lonely helping oh',\n 'said glimpses time moments mercy reminded just errand feel lucky work',\n 'im feeling sentimental nostalgic',\n 'stared window minutes feeling hopeless defeated tired',\n 'feeling disliked',\n 'gotten comfortable laying boat gently righting using low brace without jerking feeling alarmed lack push',\n 'feeling mentally physically drained no longer person year ago changed',\n 'feel contented',\n 'feel mega lame hes think god knows reason probably cross mind week',\n 'kind pressure start feel irritated quickly angry snappish',\n 'feel talented teams st',\n 'feel morose nonetheless think',\n 'feel excited like know child look like teaching russian british accents hahahaha',\n 'feel ive sort faithful supporting huddle but feel like ive sooo little playing game',\n 'not angry feeling jealous envious actually took place just but just lost n upset girl wondering things shouldnt thinking not moment n',\n 'arrived cardiac floor feeling sleep deprived little edge frankly stressed',\n 'feeling quite cold',\n 'finding ways feel entertained protected attractive comfortable',\n 'ive feeling mellon collie aka melancholy past days',\n 'want capture stillness place bottle time m feeling little gloomy piece tranquility just breathe',\n 'know feeling frustrated venting energy likely explanable look surface',\n 'feeling skeptical thought im crazy try recipe like oat porridge',\n 'feel desperation longing seeping',\n 'im grandparents feel like time precious',\n 'im feeling hostile irritated',\n 'attended tried pay attention material covering participated excercises despite feeling little skeptical thought nothing gain',\n 'used feel but feel miserable',\n 'feel need entertained feel like youre bored',\n 'feel remorseful shouting times hes especially annoying',\n 'feeling hated new litter got costco brand dislikes julius using box',\n 'tend mouth shut im not enough informed but comes public education speak feel thats topic im passionate best',\n 't shake feeling taking s valuable chance comfort guidance',\n 'forgive accepting allowing having created energy high addiction dependency self righteousness result feeding thoughts feelings emotions starting point accepted allowed justifications validations',\n 'feel deeply burdened emotionally distraught whisper know god',\n 'feel valued appreciated respected feel like going merely treading water going motions starting feel previous department',\n 'feel strongly gentle transitions sleeping company comforting think sleep alongside spouses not separately',\n 'feel needy probably far detached',\n 'surge affection just feel curious feel sad way thats difficult verbalize',\n 'feel ugly time',\n 'hate feeling afraid',\n 'just attached guy im dating usually not bad but iv emotional year just feel greedy selfish wanting moment',\n 'wore flowy maternity gown feel but cute',\n 'truly feeling comfortable technology daily basis',\n 'feel friendly discussion problem ease impact substantially',\n 'feel damaged complex story truncated summer blockbuster format',\n 'im fairly certain reason choose awful frames feel rushed',\n 'stood slade frontman noddy holder group performed song cmon feel noize ecstatic stadium filled brim grown men holding plastic pint pots aloft warm lager arm arm singing word',\n 'loved not feeling rushed',\n 'want feel worthwhile enjoyable',\n 'personally feel not guarantee no innocent citizen convicted error only impose sentences convicted reversed error later come light',\n 'called sister kak lng als brthers abang ngah abang chik feels culd thers t share lve im glad accept lk im nse t share mthers lve huhu',\n 'football big deal high school',\n 'feel passionate',\n 'believed love lives feelings kind generous accepting exist independently decide not',\n 'feel somewhat ashamed',\n 'things feel abolutely fabulous inside',\n 'im usually feeling blank know posted today but bachelorette talk guess say',\n 'tried empathetic pretended empathetic acted making sure needs family example but keenly aware feeling resentful time',\n 'feeling depressed having giant apple pie face not particularly good idea',\n 'feel vicious cycle life thrown stop',\n 'like no love feel happy sight but don t know need',\n 'don t feel m smart enough accomplish goals socialize people don t believe m pretty constantly eat lost motivation lose weight happy',\n 'probably hour wouldve stayed longer but feeling hot',\n 'im feeling needy music just sit promise not compare men life',\n 'upset time birthday assignment parents forgetting birthday year friends really brighten day feel loved time',\n 'feel like end nothing worthwhile going happen',\n 'feel broke inside but wont admit',\n 'cant help but feel devastated no idea shes going',\n 'guess grateful feel day thankful unwavering love support friends family remind m worth fight',\n 'im feeling lot confident walking outside',\n 'ive postponed writing post feel like vulnerable write',\n 'feel heat disapproval failure breathing hot neck heart',\n 'hate feel needy',\n 'wrote walked connected shared people working living grshino loved feeling honoured welcomed beautiful people community',\n 'sad lonely trying organize play student years instead materializing',\n 'not feeling terrific did manage bit creating',\n 'feel wonderful young man drives way taking off adventure best friends elementary school felt happy',\n 'havent written wishlist post august list things want getting longer second thought id better write down gets unmanageable p christmas month away feeling generous',\n 'cant stay like longer feel like freak dont want sex cause like shagging bouncy castle hair cut shorter feel like frump complete fat frump',\n 'feel irritated angry sad upset',\n 'feel rich use',\n 'arrive stand tiny box office window feeling interviewed border police officer convinced criminal trying flee country',\n 'started feeling weird saturday morning hardly steps without breath wanting throw',\n 'washington dc years advocate cnmi foreign workers feel productive',\n 'feeling little stressed but seriously no nothing blame but',\n 'im feeling invigorated not moment soon',\n 'didnt know but went home later experiment feel accepted but experimented learned new feeling feeling greed',\n 'cant feel lust just need reassured shes amazing',\n 'cant shake feeling anymore like know terrible going happen',\n 'not tell good makes feel im funny',\n 'don t cookie cutters make cookies but year maybe twice m feeling benevolent',\n 'finally admit im feeling sorry img src http community',\n 'feel people try promote positive body image expense type body',\n 'fallen victim sheltering scheme homeschool world feeling hopeless lost right goes href http www',\n 'just feel disillusioned moment',\n 'felt won argument felt feeling somewhat hostile separated long distance',\n 'feeling adventurous yesterday decided whip',\n 'love m feeling romantic season coming posted wednesday th january',\n 'event got angry got aversion',\n 'wear necklaces knowing despite feeling heartbroken loved man steals heart',\n 'feeling weepy blue',\n 'feel melancholy time',\n 'used zpg tools particular way slept feeling misalignment resolved',\n 'dog tired feeling hopeless savior wants hope',\n 'feel like need say downsides day series unfortunate events today led sit bathtub hour listening blondie',\n 'mumble murmur means marvel exist feel uncomfortable ashamed admit distrust albino',\n 'feeling quite generous today work excerpt favorite stories written',\n 'cant crying feeling unimportant',\n 'talk feeling unloved m taken granted',\n 'feel like cardinals fucked times team picks season',\n 'feeling energetic happy',\n 'feel lucky',\n 'feel poisoned hateful angry buildup leaving feel lighter',\n 'im feeling petty mean right im going say',\n 'feel like folks unsure annoint hilltoppers best team sun belt',\n 'left session feeling little offended little uncomfortable lot convinced need radical change life',\n 'feel person supporting journey',\n 'feel happy',\n 'feel heartless beast subject book guest jp max online site xanax fish stay',\n 'remember feeling helpless years ago holding child arms weeping waiting grown brother sister home elementary school waiting hear friends family new york city ok',\n 'feel whiney didnt want tell people crap',\n 'feel highly disturbed not sad enough missing husband',\n 'feel relieved thought',\n 'feel greedy ashamed',\n 'enjoyed build air suspense confusion but cant help but feel dissatisfied ending',\n 'im feeling cranky today lack sleep horrible allergies im not sure',\n 'don t feel insulted s like feeling proud actually',\n 'feel comfortable handing editor',\n 'just feel like im worthless piece shit times week',\n 'love mozart pink floyd sondheim irving berlin j lo beethoven complex but feels supportive beneath main melody',\n 'don t mean middle night moment feeling low but day consciously weighing pros cons',\n 'feel intelligent walking class entered',\n 'ive feeling hopeless lately',\n 'feel gentle firmness fingers jaw jaw revealing stubborn realize hands ones worked wood beauty touched leper make ugly skin beautiful pierced nails make life beautiful',\n 'feel countries killed tortured benefiting tourism money making',\n 'know m not alone feeling deeply horribly wronged outraged hurt scared utterly defenseless',\n 'feel discouraged morning not good place handling peyton',\n 'sinking feeling m getting just bit depressed know just want morning',\n 'super excited only day but feeling creative productive',\n 'think feel violent',\n 'feel grouchy class post count link href http adventuresofjkl',\n 'wonder looking materials but im feeling reassured energized',\n 'don t feel funny',\n 'im feeling bit grouchy',\n 'feel id passionate invest time not mention enjoy writing',\n 'realised actually allowed people admit unworthiness not competent intelligent enough just avoid feeling awkward know technicalities subject',\n 'think helped make new room feel special',\n 'im scheduled run today but feel shaky lethargic',\n 'feeling opted shoot photojournalistically completely prop free personal fave',\n 'little extra right m feeling like husband vital parenting stage year old but parenting books m itching read',\n 'feel like didnt need grasp comfortable capable trying new',\n 'cant help feeling sorry',\n 'feel really relaxed gift giving',\n 'ive said really feel feel remorseful long happened im forgiven not',\n 'feel thrilled hair extra bounce step don t worry outfit make',\n 'feel like people given lot wasn t appreciative',\n 'feel offended people like',\n 'feel particularly ecstatic',\n 'aggravated running late school wants change pants feel funny begs weeks chuck e',\n 'feel looked charming',\n 'feeling miserable days',\n 'took demo pinch salt only quick job feeling suspicious',\n 'im feeling movement loving',\n 'dont know say feel super realistic',\n 'flip pieces double height necessary quickly feeling adventurous try hop day',\n 'didnt sleep id hoped im feeling kinda grumpy annoyed',\n 'feel uncertain people assuring',\n 'feeling cold electric heater throws welcome heat face cross arms embrace shivering stops feel',\n 'feel tremendously honoured people drop read thoughts views',\n 'just told things feel went suffering hour errr days weeks',\n 'feel intimidated thing right just learning ropes field but awesome potencial earnings eduation fitness health field thanks real honesty intelligence sharing',\n 'admit only time feel bit shaky',\n 'achieved little but change not feel bothered indismissable fact',\n 'feel fully acclimatized curious feel difference running altitude m ft',\n 'love grapefruit macaron portrait feel gives feeing sweet but terrible taste like grapefruit just like macaron just looks like pretty cookie but complex',\n 'ive gone phases feeling disappointed dont followers glad ive recorded thoughts feelings regularly children remember',\n 'feel especially lonely',\n 'feel like terrible person feel like not generally tonight',\n 'forget use dentures feel resigned mouth day',\n 'feel al gore beliefs but wasting time',\n 'say catholics immediately scouting feeling wont long harder accepted scouting openly catholic openly gay',\n 'want kalani brother sister grow feel mad helpless thats feel totally helpless just',\n 'feel romantic male species',\n 'feel feeling but not convinced way',\n 'loved feeling festive day',\n 'feel like theyre going try say broke charge new',\n 'im feeling selfish know people worse but cant control',\n 'couple days without putting book time start feeling cranky restless withdrawal symptoms',\n 'feel inadequate',\n 'cant articulate im feeling words cant fear feel child safety world uncertain',\n 'feeling pretty glad',\n 'feeling shy people',\n 'just feel like pathetic exhibitionist',\n 'feel like dont communicate anymore delighted share details trip not completely traumatized flight home',\n 'feel need topic write hate neglecting beloved little blog',\n 'felt slightly self conscious having intense feelings longing barely know',\n 'depends day fckin exam thats only way keeps feeling lonely',\n 'use recipe feel going fail naturally saves time precious ingredients',\n 'post questions comments schedule away fans not feeling ignored',\n 'hope content feeling creative',\n 'wasnt entirely sure ready having just come wonderful time family feeling pretty unsure ability job but know',\n 'ride seven hours not feel like beaten',\n 'tend gravitate away male narrated novels female feel like cant relate but didnt deter away gentle wounds',\n 'thankful people inner circle friends family mean world feel blessed beautiful souls life',\n 'love able publish online but cant help but feel ache longing handwritten things pleasures simpler time',\n 'feel irritable think impending fights arguments local mall darling opinionated children buy wont buy',\n 'june measurements say im bit surprised ive feeling discouraged',\n 'honored words gratitude thrilled day life helped women shift feelings positive',\n 'feel convinced portion scripture intended foretell resurrection saints distinct ungodly explicit information given concerning concerning',\n 'feel grouchy point changes panic having feeling like',\n 'im feeling bit depressed',\n 'feel like needs disclaimer no way romanticising shitty aspects pairing okay fucked belief summary castiel holds world palm hand',\n 'mean good thing feeling deprived hungry day except night',\n 'discovered feeling disturbed confused movie but news meant depressed shouldve joined girls having shots but just went tame option glass wine',\n 'no longer live alone but tonight feel alone amp quite insecure',\n 'reactions days feel absolutely unplugged heart races lethargic want sleep but not try money live',\n 'feel stupid putting internet but blog documenting journey life thought',\n 'listened mind clear feel little agitated',\n 'feeling uncharacteristically disturbed',\n 'feel inadequate way possible',\n 'kind feel like topic shouldnt blog peta type people violent wouldnt want targeted',\n 'im afraid getting hurt feel hurt wont able pick',\n 'feeling determined week doing im not going let self hating self going docters depression panic attacks hopefully help',\n 'feel like mad douche lived wrong way school fear losing individuality',\n 'feel safety change important users educated process',\n 'wasnt feeling just stood aside amp let people fall',\n 'im feeling abysmally dull',\n 'feel sailed clever race im proud tactical choices boat handling',\n 'view not better live actions results yielded without feeling missed without feeling regret did not say experience',\n 'guess overall feel cautiously hopeful',\n 'feeling creative not near computer little room uninspired',\n 'feel shamed mama angry but love hearing youre drink',\n 'want respected but want feel loyal know',\n 'im feeling smug ive acquired enough knowledge true gardener said learn new things make realize little really know',\n 'im not feeling bitter today',\n 'feel little weird calling ceo company bob but relented corrected repeatedly',\n 'feel pillars supporting debarkle interesting highly relevant issue wish poke later',\n 'sorry feel unimportant unappreciated',\n 'feel like dont believe love anymore im not talking romantic love dont think want thing',\n 'want create space enter feel welcomed warmth love christ',\n 'got ta say im feeling nervous excited',\n 'feel little shy sharing pictures hospital little special personal but want forget day',\n 'nauseous feeling shaky',\n 'woke feeling cold seizure',\n 'feel feel like hope life nothing doomed laughed backs people shake heads disbelief disappointment',\n 'mean gosh live minutes away but feel like distance relationship lame',\n 'cant feel blessed happy awesome sweet friends world whore',\n 'sat car crying holding pieces mail feeling overwhelmed',\n 'remember feeling pleased proud ive achieved ive',\n 'love sincerely feel thankful',\n 'only feel stress n feel unhappy test really stress write express feelings',\n 'feel really positive excited r choices without doubt accept offer',\n 'anemic sooo tired lightheaded spaced feeling but anxiety attacks terrified passing',\n 'feel virtuous href http twitter',\n 'thought peers lacking months year vote feeling hopeless watch news click refresh button night',\n 'just feel reluctant aberdeen',\n 'feel im heart know im worry caring unless im following p really like baby text thought u',\n 'wait long appointment wondering point feel fine',\n 'don t feel depressed m missing american traditions commodities',\n 'feel assured god provide guide family day',\n 'im feeling mellow probably listen pink floyds animals wall favorite albums',\n 'say feel amazed humbled awesome',\n 'feel n ignored',\n 'took minute appreciate trees calming energy gave time feeling little bit irritable',\n 'don t feel mega rushed',\n 'begun feel lucky farm',\n 'god s presence feel scared lonely sad but feel presence im happy like m rewarded god feel really thankful',\n 'feel tend greedy person capable overcoming tendency easier',\n 'stuck traffic jam instead feeling irritated slow drive dont just relax enjoy drive home',\n 'feel but im really eager movie',\n 'im certified personal trainer feel free advantage ask fitness related questions like',\n 'feel lucky didn t jump straight hypothermia able carry race',\n 'feel safe community feel protected',\n 'feeling im going annoyed far im entertained',\n 'feel jealous seeing balik kampung',\n 'want grow happy cheerful home people wont make feel burdened peoples mood swings',\n 'feel music just finds way peoples hearts said ali',\n 'feel heartbroken disgusted',\n 'ive wondered feel like hit exiting cars day feel frantic upset family problems',\n 'im feeling wimpy aka crying comforts trustworthy',\n 'place mentally feeling really quite irritable anxious tormented',\n 'hate rap hate fact advertisers feel need make commericals people feel arent intelligent enough speak understand english',\n 'ive feeling lonely months feel like used feel not good',\n 'feel petty wont probably',\n 'helped phase guys taking rental car laura feeling bit delicate night',\n 'ive couple weeks feel things gotten pretty dull sadly ive completely neglected pretty break wasnt lettuce stacking chips gettin bread making cheese bringing home bacon basically ive obese break',\n 'feel pathetic feeling way dont deserve life',\n 'honestly thought impossible point feel pretty',\n 'feel invigorated home going journey healthier lifestyle',\n 'suppose just feel completely alone better surrounded friends arent',\n 'rusty but feel like flowed ok',\n 'feel inspired make christmas presents im giving away',\n 'feel like ve neglectful',\n 'feel way t just girl s happy got life lives moment not future',\n 'feel horny successive orgasms',\n 'feel bringing hateful subject cruel student start hating messenger',\n 'feeling really distracted recently',\n 'anxiety panic attack feelings say just let anxious feeling pass problem don t know',\n 'able feel like today buoied sense peace calm only come hope',\n 'picture throwing arms exultation looking feeling thrilled awesome tucking success proverbial belt',\n 'feel like not faithful friend son brother uncle worker church member',\n 'feel aggravated thought comes mind little regards',\n 'heard rumour st year exam results fear failures',\n 'feel liked',\n 'going send ship but im feeling reluctant becuase dont want customs handling cant',\n 'apologize feels insulted inner eye s vision beauty',\n 'flew athens copenhagen holiday plane caught storm began shake thouhght going crash',\n 'not feel like writing grouchy blog',\n 'little down feels hopeless times lot anxiety not sure but try remember meant fit expect',\n 'don t understand sitcoms feel need weepy episode',\n 'feel cool breeze come like did nightly basis hospital died',\n 'cant help but feel smug honestly im not fault',\n 'feel privileged spent time',\n 'feel like single parent roommate fond baby',\n 'im tired feeling grouchy things happening',\n 'feel little regretful waiting',\n 'cant sleep life feels totally totally fucked makes no sense level sober but living self centered self willed thought action iam world pain right',\n 'mean men think feel need dont look wimpy',\n 'only online grappling feelings loss need life submissive thats not fed',\n 'im finding ive trying manifest practices general feel fearful',\n 'highly recommend late night walk good friend needs spot cheerin youre feeling listless whatnot peace',\n 'expectations thats probably feel dissastisfied popular novels',\n 'feel surprised answer',\n 'sit patio windows feel like not bothered glare harsh sunlight',\n 'westerner local happily chatting mandarin feel rude m unable long',\n 'entry dont access but feel trusting today no apparent reason',\n 'im home beach im feeling need productive',\n 'did time reaching feeling smug satisfied left only come weeks later',\n 'feel divisions fine long facilitate life',\n 'not really let spoil mood but did cause think little feel irritated',\n 'live years but feel lived vain stronger not people succeeded only nothing carefully successful maybe did not fail like but think just jealous',\n 'feel homesick easy life',\n 'im feeling impressed point time',\n 'feel lil dazed actually',\n 'work weekend m heading feeling little bit defeated',\n 'know uncomfortable but feel like worthwhile goal accomplish upcoming year',\n 'basically wrote future self m feeling generous think like just guy break list queries',\n 'good feeling today broke curfew like minutes night good feeling today',\n 'walking home work feeling extremely joyful kick accomplishments work week deliverables finished time good quality',\n 'feel restless quite bored',\n 'feel canadians coming complacent workings country weve fared recession',\n 'end feeling insecure body fucking minute spend trying look ok running treadmill',\n 'started feel awkward sexy french guy came stood doorway age relative baby way behaving',\n 'feel fine live germany remastered',\n 'feeling good',\n 'told feeling told hold literally weeks able mountains',\n 'like way say pissed off but annoy fuck feel like going violent rampage',\n 'feel dance ladies dance feel joyful minute hug strains disarli d arienzo tantori',\n 'feel strangely invigorated prospect',\n 'possibly screwing landlord thing big time isolating cutting off ties thing m doing feeling overwhelmed stuff written pm real',\n 'busting ass break run feel energetic',\n 'think english department fantastic idea feel little intelligent time classes challenges mind different way music classes lovely',\n 'think list resolutions came beginning year but im feeling worthless try',\n 'feel unpleasant tightness chest wasnt minutes ago',\n 'im feeling bit tortured today',\n 'try make people happy satisfied but turn feel dissatisfied feel like people dont try',\n 'feel make year precious weeks',\n 'only seen chungking express feel like director make faithful film adaptation terms not only story but concepts themes',\n 'feel safe',\n 'im feeling bitter citrusy hop taste tongue not',\n 'bet feels like assaulted beautiful woman',\n 'dream terrifying used fear rest day dream come true',\n 'progress but strong feeling just years motto',\n 'having normal day didnt feel completely normal day mtx',\n 'wanted feel love freedom deprived off home',\n 'feeling little brave try make bread time',\n 'know think buck think biker urine pants fishing buddy doesn t say but s secretly smarter doesn t want feel like moron saying intelligent',\n 'met years ago national writing convention figure assemble nametags feel befriending valuable thing garnered conference',\n 'ill happy select handful classically beautiful items make feel fabulous',\n 'irritated did not feel sympathetic',\n 'im feeling proud attempting write song',\n 'say pocket add british accent feel lame',\n 'feel hopeful',\n 'feel quite disturbed',\n 'feel disturbed terrified like normal citizen',\n 'im beautiful makes feel like ornament but tell thinking feel valued',\n 'im feeling scared alone',\n 'not feel pity sorry without believing lesser really wtra',\n 'feel privileged lucky work set but love working comfort home naturally couple downsides particularly no annual leave',\n 'apologized way hubs said feels really needy',\n 'feel like neglectful past not given blog attention needed',\n 'feel amused o',\n 'feeling barack obama surprised able accomplish tearing america down years',\n 'good person feel acceptable normal',\n 'feeling extremely sympathetic mr laws',\n 'feel like telling fever away bt stubborn listen',\n 'feel like bit hesitant challenging',\n 'just tell m not feeling accept situation night m not going particularly lively',\n 'feeling peaceful wasnt bothered fact eat shower',\n 'know taking feel cold coming helpful warding off cold',\n 'couldnt help but feel happy dressed',\n 'really feel huh horny eh',\n 'im just feeling sentimental but im grateful gift family ive given family late life gift',\n 'feel inside desires compassionate but knows fate assigned only use sword kill',\n 'stop feeling heartbroken unfollows',\n 'feeling little discouraged point',\n 'look feel despised',\n 'feeling soon but im trusting way timing',\n 'dont listen music but like franz ferdinand keane ancafe but fans bunch idiots sorry x feel like theyre taking word nyappy vain sad',\n 'end school function celebrate leaving friends id known years',\n 'remember feeling slightly intimidated speaking professor week ago previously only seen balcony bader theatre',\n 'feeling pretty groggy went off sleep easily good night waking pain',\n 'feel film cautiously optimistic but not lighting world',\n 'went bed beginnings sore throat woke feeling tender decided look',\n 'feel people just rude arrogant inconsiderate',\n 'hate feeling grumpy pregnant',\n 'brain tumor left hypothalamus not sure related but feeling extremely irritable sad like m going outburst anger problems speaking today tried talk but words not come right',\n 'feel homesick ga boys week',\n 'turned didnt feel like bumn hated everyones birthday somethin but bday plan im dealin crap written class fn url href http journals',\n 'feel like remain strong quality food serving',\n 'id try healthy pub grub appetizer super bowl weekend youre feeling brave',\n 'just try photograph feel ideas taken shape far bit unsuccessful far ideas concerned',\n 'feel like shouldnt happy life surely trouble just corner',\n 'waiting feel enraged helpless',\n 'finally game feels wonderful',\n 'admit im feeling lot hopeful food ducation did started reading',\n 'seen not feeling apprehensive time phone rings',\n 'im really feeling homesick today sit fresno look gray sky wet patio fence',\n 'ive places far life but feel super blessed living costa rica interning amazing organization thought young life',\n 'feel uncomfortable thought swapping straight handlebars add helper ends hill climbing',\n 'pretty upset lack food fact commented facebook page solution words volunteer year police feeling frustrated considering running jester year',\n 'did not feel scared sad',\n 'feeling drained',\n 'feeling anxious watching play asked hit balls did alright',\n 'feel humiliated angry days',\n 'feel like putting silk pijamas face face looks radiant happy morning',\n 'feel like accepted vessel',\n 'came feel absolutely helpless change course life life living unutterably painful not going',\n 'feel way days just wish pain suffering',\n 'feeling going nights like did lonely early s rituals jamie got new job goes bed early',\n 'set consultation therapist week went today spoke general feelings things end reassured did not atlephobia but instead social anxiety apparently really common',\n 'feel like know enough let know smart girl',\n 'admit feel comfortable meandering stitch but practice im sure ill learn love doing loops',\n 'allowing feel frustrated roll present plans',\n 'feeling adventurous but not stupid',\n 'im feeling nostalgic pictures carter past year',\n 'feel shaken just seeing',\n 'feel honored contributors petrified feel best',\n 'using time wisely actually finish without feeling rushed overwhelmed big problem',\n 'okay actually thats lie feel fantastic felt horrible weeks stop sit ground breath breath regularly',\n 'feel dignified especially mass professional meeting',\n 'completed story feeling angered lack justice simply not understanding frances trouble purchasing grave marker brodens unmarked grave',\n 'felt feel delighted mother happy got fun activities previously unheard regular tuesday afternoon',\n 'cant remember little mermaid feeling carefree beauty life gets lost massive piles homework budgeting off finances eventually daunting choices life adult',\n 'know wont feel thing im heartbroken',\n 'id energetic gone bike went straight past town feeling virtuous getting exercise',\n 'feel m thinking reassured means',\n 'im not feel unimportant unloved upset',\n 'reading chapter feeling little morose but second page felt really heavy sudden',\n 'really did feel valued individual not just client processed',\n 'like feeling unsure im feeling bit pain but good soul occasional doses',\n 'ive forgotten feel devoted sex dried body',\n 'wandered store feeling irritable staring long spells time dizzying options food',\n 'cant believe thought having feelings opinions bad',\n 'stood closet calling fat feeling ugly relate need kinder',\n 'shared story far feel bashful sharing',\n 'know ve wrong but feel absolutely humiliated having wear tag common criminal',\n 'received feel respected',\n 'active thyroid means hungry time no energy low metabolism week feel like worthless',\n 'feel moments precious share',\n 'read takda lah feel rushed sangat',\n 'able wants make individual speaks feel said worthwhile worth understanding consequently worthwhile having said',\n 'ive going school stuck tiny space facing table pilled books feeling isolated',\n 'ive feeling irritable restarted advair',\n 'really don t like feeling target blank href http twitter',\n 'feel distraught season finale',\n 'feel positively martin jols time club optimistic future',\n 'quite pleased state feel eager school traveled summer vacation',\n 'felt fighting battle going win just didnt feel strong enough attempt fight anymore',\n 'really shouldn t feel smart',\n 'liked build fighters but don t know feel ugly ass wing gundam zero type wings',\n 'feel touch caress gentle soft like angels wishes memory strokes spine electing chills beneath reality',\n 'slight ok huge temptation weary feel burdened',\n 'feeling crappy going down hill entire week',\n 'zone feel agitated need leave',\n 'feel wronged ways',\n 'sitting night feeling cranky sorts wondering hell wrong realized course',\n 'feel aching pain inside spreads veins makes feel like im withering away',\n 'got feeling quite irritable but expectations profound anxiety id felt past days gone',\n 'feel like summer days precious reason',\n 'boards id feel really idiotic self censor',\n 'honest admit feelings time time but jonah knows god gracious lucky jonah god gracious gracious people like',\n 'im watching guys prepare missions review intelligence reports talk jobs feel sense longing mission',\n 'feeling helpless not explain',\n 'think secretly gets sort satisfaction knowing havent gotten feel tortured',\n 'picture cant help but feel shy',\n 'feel not title need ideas sweet',\n 'legitimately came feels selfish egocentric',\n 'feel like presence hated',\n 'love way feels love permanence love nostalgic feeling keys fingertips',\n 'start feel despairing snow melt',\n 'hate feel victimized threats',\n 'know d facing lot sneering derision moment publish but hell care blog m feeling shitty depressed right just bare',\n 'feel like hated but m scared listen tape',\n 'im feeling nostalgic seeing familiar backdrop',\n 'feel im not smart enough',\n 'feel exhausted grateful support',\n 'tend bare lipped work nude light pink lip balm gloss light red m feeling particularly brave actually time minute rush leave flat morning',\n 'feel im falling vicious circle vision interesting',\n 'ill just end saying greatest feeling world exh opening adventurous way',\n 'feel loved',\n 'im feeling apprehensive resistant new different usually settle down resign level faith allows accept appears fate',\n 'feel tad resentful don t access no idea use software',\n 'feel trap honestly look foolish world warn unsaved friends family certain things happen predicted prophet dont happen way said',\n 'love comments feel free leave',\n 'feeling adventurous kale chips inspired href http www',\n 'feel owe adoring fans lj entry',\n 'feel relationship patron ignored',\n 'just feel kind completely defeated today',\n 'feel like title share facebook facebook target blank rel nofollow class twitter href http twitter',\n 'think god gently showing way doesnt feel gentle deeper levels loved',\n 'im feeling socially pressured tell lou need anti social time',\n 'years old time appalled vapidness self reference musicians feel theyre oh fucking clever ive distrustful tried',\n 'feel ecstatic awards function perform year',\n 'feel want reach pull parts theyre useful tools onward journey',\n 'think im scared feeling shy ask x hai yai yai xd nvm shall nap weather looks pretty good',\n 'feel reassured musical convictions wish congratulate radiohead accomplishments',\n 'feeling mad bb fomo right href http twitter',\n 'cue title post but feeling adventurous no following heck',\n 'feel privileged god given gifts furthest reaches imagination physical form children',\n 'feel gloomy n crap hope ill manage pull face kids im meeting today havnt seen years',\n 'feel say accepted mothers passing',\n 'know feeling feel hopeless useless fat ugly lose weight age bother right',\n 'enter blog doesnt feel ignored',\n 'feel things going great rapport',\n 'knew misunderstanding years',\n 'left feeling unimportant unheard unvalidated',\n 'feel like mission casual keeping commandments but im understanding important wonderful commandments',\n 'like real feel truthful write im feeling theres no shame human clarity maybe help whos going',\n 'feel strange going homeopathic long time',\n 'im feeling brave went introducing sir actually able come time',\n 'not look face catch eye cant shake feeling hes annoyed',\n 'love emotion comes music make feel thankful gift music',\n 'badass but right im feeling dumb awesome',\n 'ive read blog posts articles response saying women shouldnt feel pressured world look certain way',\n 'really feel need establish acceptable life consider mentally prepared enough kind schooling',\n 'feel nervous wondering ll getting damaging storm course days just look sky think god heaven loved ones passed away loved ones pass away future',\n 'cope presence without feeling distressed force quiet resigned friendship',\n 'feel overwhelmed feel overflow begin flow fingertips burst beneath skin',\n 'know ll feel successful life program create created genuinely wanted create frankly couldn t stop thinking creating',\n 'read books march tbr stack kind book fall headfirst feel dazed hours later come air',\n 'literally feel amused grin spreading abdomen amp not stop face smiling response',\n 'miss miss chats but old chats used lead making feel special not fight chat but hold fear fighting things going astray',\n 'going stop letting little things overwhelm like meal planning keeping laundry pile control instead going plan fun outings camping trip things look forward instead feeling regretful not keeping daily grind',\n 'know id feel awkward ashamed did subjected',\n 'winced felt miyavi s rage managed hold shiro feeling bit dazed',\n 'continue feel like address comment sure arise',\n 'know sweet client welcomed girls but didnt want make feel nervous distracted',\n 'feel like runner share facebook href http delicious',\n 'feel hopeful discount medicinal value hopefulness',\n 'hate weak feel awful feels like im wrong feeling way',\n 'feel bad chris allergic but alex cat people',\n 'feel rude not knowng',\n 'feel like m impolite send someplace',\n 'just feel hopeless lost overwhelmed wondering like',\n 'feel badly didnt reply suggestions meeting face face nor gracious offer hospitality im cincinnati area arrange',\n 'feel somewhat dazed',\n 'allowed feel bit grumpy years younger',\n 'feel going popular week lots fab entries use buttons projects way',\n 'admit feeling assured knowing internal parts higher quality',\n 'feel uptight ass sores fuck gay police',\n 'feeling inspired',\n 'passed ba exams class results',\n 'picked left confines safety house hours sunday but ive feeling really bad upset tum worst headache imagine',\n 'just feel like not sure',\n 'went summer feeling like disliked',\n 'let happen cant feelifng trusting forgiving look best ppl',\n 'blushing feeling terrific',\n 'look just feels looks ugly',\n 'went bed feeling hopeful but uncertain',\n 'petrified starts university strong feeling hes going meet gorgeous smart caring',\n 'happen surround forbidden male company enjoying learned feel satisfied happy without having pleased',\n 'feel like aliens prototype sequel wildly successful revered movie',\n 'know pulling lecture material week dont end feeling frantic sunday but just dont want',\n 'feel disturbed people break pieces right love',\n 'dig feel like supporting',\n 'feel truly blessed path',\n 'feel ungrateful not honor',\n 'guess lot parents feel way kids naturally ungrateful',\n 'sort like way meditation session runs simple but makes feel bit uncomfortable',\n 'feeling confident thought actively deciding indulge slightly okay',\n 'only asked question couple times but responded miss certain things home but don t feel really homesick',\n 'used feel awkward single social functions',\n 'feeling accepted challenging',\n 'feel relieved outcome encouraged future deep trust elders',\n 'feel love coming like breeze wind passion flows openness only love let lay field daisies flowers disregard time romantic hours dom colucci',\n 'assert better feel rich actually rich',\n 'just invented feelings book embarrassment empathy youre just embarrassed character page screen hide face bear',\n 'little tired remaking certain things but feel thrilled continue making',\n 'feeling hesitant entire thing',\n 'feeling little overwhelmed lately',\n 'wanted checked but im feeling like just terrible terrible cold not influenza ish getting better day',\n 'feel important little girls follow strong female characters but feel important boys',\n 'feel paranoid dont like not good fact dancing efforts showed thought christmas meant fun',\n 'know fact knew feeling things unloved rejected boring seriously not only come huge surprise but shown absolute lie',\n 'feeling completely irritated right no idea maybe usually just getting home school hour god awful videos imposed time',\n 'sensing feeling strong attraction',\n 'feeling little homesick xd',\n 'feel prepares brain creative endeavors',\n 'feel doubtful person sincerity rebuilding relationship',\n 'going learn cryptic crosswords feel smart accomplished',\n 'heard did not feel pleasant but didnt feel bad luckily',\n 'feel burdened equally blessed',\n 'wish dance swing definitely feel no cold',\n 'shield heart everytime feel like just heartbroken shield thicker thicker eventually no ones gon na able heart nor able love certain',\n 'feel blank pain deeply completely accept',\n 'feel bit fearful feeling stressed drinking',\n 'feeling embarrassed saw',\n 'feel need brush artistic skills dont progress stick men soon nikki catch',\n 'sadist provoked sister presence tried force sister accompanying place',\n 'didnt feel cheated deprived',\n 'feel bit unprotected',\n 'sure people feel safer fbi investigating suspicious incidents',\n 'feel like ive gained pounds week but surprised scale morning',\n 'feeling homesick honestly little confused god brought',\n 'feeling shame wife looking thinking s not smart handy enough anymore loser',\n 'feel vicious beings closer actually attacking im happy peaceful theyre distance',\n 'friend hear news makes ecstatically happy sitting straight chair feel grow little weepy',\n 'feel havent posted couple weeks follow worthwhile',\n 'left feeling lighter relieved emmotionally spirutally',\n 'guess not having kirk begging time makes feel unimportant',\n 'feeling disappointed people conformity daily life pulled promise uniqueness expression',\n 'feel totally stunned but somewhat better ive chance look answers notes',\n 'thoughts but chose express way feel abused',\n 'feeling enraged thing off couple days',\n 'feel brave enough say day',\n 'feel happy instantly walk door',\n 'im feeling bit sociable dont think ill able express want say',\n 'just feel numb dont know whats happening',\n 'dunno m feeling bit troubled right just felt like writing make better',\n 'feel lot celebrities footballers musicians actually gave eco friendly causes help lot able make technologies cut down emissions make eco friendly tools long run stop illnesses occur environment',\n 'end feeling drained process putting heart mind soul words black screen text editor',\n 'feel society tries gracious place inhumane behaviour tolerated',\n 'feel immensely happy greatness god',\n 'observed vary body language tone voice automatically response conversation partner feel report creates emotional level nobody actually taught',\n 'feel embarrassed stop state house enough wish pretend werent home does stop',\n 'feel achieve wonderful goals',\n 'feel like vicious cirlce try lose end giving',\n 'woke feeling shitty allowed mood dictate morning hiding computer game chess',\n 'philosophy perfect match family independence feel just overwhelmingly joyful times ask things like mow lawn dj',\n 'feel punished finished feel bad left ex husband busted child s life',\n 'feel defeated deflated',\n 'feel enriched experience felt worthwhile',\n 'feel like denial summers tragic turn events possession gramma camp',\n 'feeling wont need shes smart little cookie',\n 'im going pace eat feel lousy rest night',\n 'feeling lonely no friends left pray help comes holly soon',\n 'feel relatively gracious sort uh semi dumped continues peculiarly distant impassive spending time',\n 'feel truthful say swim moment happen percent mile open drinking water happen fairly nicely dedicated',\n 'im not feeling clever moment',\n 'wasn t tune sort feelings numb',\n 't help but feel proud padres fan time',\n 'feel fairly rich overall',\n 'not really know feel but thankful god way hope future person standing way no longer',\n 'feel bad starting new topic time feel like saying but guess thats point message board',\n 'got feeling weekend runners not casual',\n 'did feel little ugly gym but decided didn t care',\n 'try perfect strength power normally ends leaving frustrated feeling worthless',\n 'cleaned porch feel pretty dmaned virtuous',\n 'feel oh glamorous looking super fabulous im insecure cant ignore flashing cameras try best handle modling lil sis say mashalla',\n 'feeling fucked pretty warm night',\n 'approach end course finally feeling little nostalgic',\n 'feel like isnt going work pull away people extent hostile but not feeling talkative talkative people finding things annoying moreso normally',\n 'feel truly accepted nurtured cared loved',\n 'feel fond haven t really chance know',\n 'feel like burton hugely talented writer but book just wasnt right',\n 'feel like people gordon friendly home said peter',\n 'feel gracious sitting typing hear sounds family sleeping',\n 'feel annoyed ask help knowing actually faster',\n 'learning daily responsibility hurts wounds healing feels unpleasant gets job moving trauma wholeness',\n 'mean hes just sweet bea feel like totally understands but hand dangerous hes vampire',\n 'feel like brain cells given violent shake',\n 'feels like flew dazed confused haze',\n 'cant assistance feeling thats loyal end open played blinder election',\n 'feel dissatisfied life',\n 'realized oh feelings think really precious experience',\n 'feel victimized police arent law enforcers revenue generators city but understand positions respect danger ultimately safe',\n 'feel bit unsure learn stuff learning stuff right way just getting way',\n 'passionate helping people train mind feel relaxed',\n 'feel committee play vital role',\n 'write feel inspired',\n 'love hearing illustrative examples gods goodness but deep recesses soul feel tinge envious heartache',\n 'shocked feel angry',\n 'able secure autograph year old urias feel weird just typing older thinking photo new blue sharpie',\n 'im feeling happy excited',\n 'feeling kinda insecure lot things lately hefty history readings class presentations welcome respite semi boring life',\n 'feel blessed sisters',\n 'left feeling regretful uncertain sad',\n 'feeling disturbed',\n 'start feel little curious',\n 'feel vital work aspects learning everyday',\n 'thought guitar instrument choice comes entertained feeling entertained comes hear mass',\n 'run feels fabulous',\n 't help feeling weirdness like brink goodness spilling messy mismanaged place like etsy',\n 'think things better organised nowadays think results good quality games reflect but theres problem feel engineers clever approach opposed optimised sensible way adds',\n 'dont but feel super bad taking days off trip sunnus sports camp alr',\n 'wondered ppl second chance but feel relationship tyler doomed wont',\n 'feeling ll soon horny guy post bw image like ll comes',\n 'feel anna shouldnt blamed writers given',\n 'mean common feel half hopeless but faith',\n 'feel unpleasant t stop pee miles jim crow no car wants risk fight jail death forgot tinkle place saw brown people walking',\n 'play interact day long feel lot frantic maximize minute',\n 'want feel inside fingers ache slip tender folds flesh hear calling pretend dont feel but matter moments know youll scream',\n 'believe feelings truthful aren t reliable indicator truth',\n 'feeling grumpy reasons',\n 'feel search engine marketing plays important function internet website',\n 'envision kinds awful things happening control feels terrible',\n 'defensive feeling attacked wanting run away beloved queers persuasions demonstrations',\n 'feel troubled feel like given',\n 'feel like people want cycle continues feel guilty kicks ol feelings',\n 'just want sympathize feeling ok holy cow valid feeling way weird twisted way asking sympathy feel stealing sympathy worse off lt holy amp t messed female sound',\n 'enema feel amazing',\n 'feel bit confused',\n 'think site truly hella smile feel reassured morally good kind individuals world',\n 'ill feel fab yah but im not right im just ball freaking im reading babyness lynn let use book ladys egg released stay alive ready fertilized hours',\n 'feel strongly casual ive',\n 'used think protecting people hiding feeling gloomy but believe exposing true selves possible real only way escape enter world relationships',\n 'doing not gods not receive blessings not abilities god wants feel situation extraordinarily unpleasant not easy solved',\n 'not feeling creative today',\n 'feeling increasingly confident not only did bella not require hospital admission but capable caring children',\n 'feel like mind blank',\n 'feeling broke emotionally physically mentally',\n 'didnt feel homesick oregon but probably only felt close',\n 'hate feeling helpless amy beck day ago',\n 'feel surprisingly sympathetic',\n 'feeling body reach fingers aching finger',\n 'feel respected significant client happens',\n 'recommend presence sugars processing possibly trans fat people feel pleasant chocolate happy feeling',\n 'feeling pretty crappy couple days',\n 'feeling today surely longing hold arms pain admitting probably end',\n 'feel like crap week gary supportive voice keeps suggestiving swine flu',\n 'im super picky minor things bugging but think overall look feel perfect',\n 'remember reading feeling terrified thought simply enthralled brilliant concept',\n 'appreciate feelings carefree',\n 'feel lame mentioning',\n 'believe feeling duality suffering soul growth upright position card action telling ending decline change direction associated emotions',\n 'feel like im treading delicate ground dont want say idd regret',\n 'feel insulted racist idiots russian ministers not shaking hand',\n 'sick feeling rotten eating enjoy food makes feel',\n 'im left feeling nostalgic lonely',\n 'dunnno just feel sorta discontent but im tired stuff just wan na bed',\n 'feel m special experience different things people experienced',\n 'feel like held supportive people right feel truly blessed',\n 'want remember feeling sweet life womb',\n 'll forgive lot but t help but feel supporting material cast isn t really par',\n 'did feel just insecure today simply cause didn t know better',\n 'hadn t waited morning arrive feeling displaced alone',\n 'especially tears read messages sent abroad hellip especially think cassiopeia members feeling ishihara pd nov ishihara pd considerate international friends',\n 'feel unpleasant boring whining ridiculous',\n 'remember feeling sort numb like couldn t really happening but thrilled belief time',\n 'criticize strength courage feel instead stubborn cowardice',\n 'doomed werent asians girls werent really feeling but doomed down food food different',\n 'feel blank incomprehension face',\n ...]"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['text'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:50:59.323322Z",
     "start_time": "2024-05-21T15:50:59.298320Z"
    }
   },
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    0, 35702, 11522,  ...,     1,     1,     1],\n        [    0,   757,  1937,  ...,     1,     1,     1],\n        [    0,  2088,  1153,  ...,     1,     1,     1],\n        ...,\n        [    0,  3654,   619,  ...,     1,     1,     1],\n        [    0, 35702, 25070,  ...,     1,     1,     1],\n        [    0, 40451,  2157,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encodings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:51:03.354758Z",
     "start_time": "2024-05-21T15:51:03.332781Z"
    }
   },
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_not_finetuned_name = \"roberta-base\"\n",
    "model_not_finetuned = AutoModelForSequenceClassification.from_pretrained(model_not_finetuned_name, num_labels=6).to(device).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:40:44.406260Z",
     "start_time": "2024-05-21T14:40:42.441092Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:51:10.643100Z",
     "start_time": "2024-05-21T15:51:10.630095Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset: TextDataset = TextDataset(train_encodings, train_data['label'].tolist())\n",
    "val_dataset: TextDataset = TextDataset(val_encodings, validation_data['label'].tolist())\n",
    "test_dataset: TextDataset = TextDataset(test_encodings, test_data['label'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:51:12.960353Z",
     "start_time": "2024-05-21T15:51:12.945347Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that saves the training stats of the model to a file for further comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:51:14.952771Z",
     "start_time": "2024-05-21T15:51:14.940771Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainingStatsCallback(TrainerCallback):\n",
    "    \"\"\"A callback that logs and stores the progress of training.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics_df = pd.DataFrame()\n",
    "        self.output_dir = './training_stats'\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Assuming `metrics` is passed as part of kwargs and is a dictionary\n",
    "        metrics = kwargs.get('metrics')\n",
    "        new_record = pd.DataFrame([metrics])\n",
    "        self.metrics_df = pd.concat([self.metrics_df, new_record], ignore_index=True)\n",
    "        self.metrics_df.to_csv(os.path.join(self.output_dir, 'metrics.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training hyppertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# def model_init():\n",
    "#     return AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Hyperparameters to tune\n",
    "#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "#     num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
    "#     per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
    "    \n",
    "#     # Define TrainingArguments with hyperparameters from the trial\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"./results\",\n",
    "#         learning_rate=learning_rate,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         weight_decay=0.01,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"accuracy\",\n",
    "#     )\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model_init=model_init,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         compute_metrics=compute_metrics\n",
    "#     )\n",
    "    \n",
    "#     trainer.train()\n",
    "#     eval_result = trainer.evaluate()\n",
    "    \n",
    "#     # Optuna aims to minimize the objective, so if accuracy is the metric, return 1 - accuracy\n",
    "#     return 1 - eval_result[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# trial_ = study.best_trial\n",
    "\n",
    "# print(f\"  Value: {trial_.value}\")\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial_.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:51:20.434382Z",
     "start_time": "2024-05-21T15:51:20.426395Z"
    }
   },
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:51:37.562542Z",
     "start_time": "2024-05-21T15:51:34.261864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels range:  0 to 5\n",
      "Validation labels range:  0 to 5\n",
      "Test labels range:  0 to 5\n",
      "Training data NaN values: False\n",
      "Validation data NaN values: False\n",
      "Test data NaN values: False\n",
      "<transformers.trainer.Trainer object at 0x0000020122DEF070>\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(\"Training labels range: \", min(train_data['label']), \"to\", max(train_data['label']))\n",
    "print(\"Validation labels range: \", min(validation_data['label']), \"to\", max(validation_data['label']))\n",
    "print(\"Test labels range: \", min(test_data['label']), \"to\", max(test_data['label']))\n",
    "\n",
    "print(\"Training data NaN values:\", train_data.isnull().values.any())\n",
    "print(\"Validation data NaN values:\", validation_data.isnull().values.any())\n",
    "print(\"Test data NaN values:\", test_data.isnull().values.any())\n",
    "\n",
    " # Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[TrainingStatsCallback]\n",
    ")\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcarv\\AppData\\Local\\Temp\\ipykernel_25020\\1380442169.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='14' max='5328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  14/5328 06:56 < 51:11:11, 0.03 it/s, Epoch 0.02/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-21T15:51:50.501677Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcarv\\AppData\\Local\\Temp\\ipykernel_25020\\1380442169.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [85/85 01:50]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.20716449618339539,\n 'eval_accuracy': 0.9303445720637273,\n 'eval_f1': 0.9315931941911905,\n 'eval_precision': 0.9380685377892941,\n 'eval_recall': 0.9303445720637273,\n 'eval_runtime': 111.9461,\n 'eval_samples_per_second': 48.22,\n 'eval_steps_per_second': 0.759}"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:01:55.731581Z",
     "start_time": "2024-05-21T15:00:03.770442Z"
    }
   },
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "  2%|â–         | 500/21304 [11:07<7:40:03,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7966, 'grad_norm': 1.9971829652786255, 'learning_rate': 1.9530604581299287e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  5%|â–         | 1000/21304 [22:07<7:32:30,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7285, 'grad_norm': 5.741614818572998, 'learning_rate': 1.9061209162598577e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  7%|â–‹         | 1500/21304 [33:16<7:17:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4968, 'grad_norm': 5.530463695526123, 'learning_rate': 1.8591813743897863e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 1909/21304 [42:33<7:43:33,  1.43s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path = f'./my_trained_models/{model_name}'\n",
    "trainer.save_model(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:03:59.057362Z",
     "start_time": "2024-05-17T09:03:58.620375Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Domain adaptation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): GELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "model_m = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_m.to(device)\n",
    "\n",
    "initial_model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)  # for comparisons\n",
    "initial_model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:21:18.204012Z",
     "start_time": "2024-05-19T17:21:13.835981Z"
    }
   },
   "execution_count": 316
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:21:28.028405Z",
     "start_time": "2024-05-19T17:21:27.738097Z"
    }
   },
   "execution_count": 318
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> I am feeling weird right now, because of the weather.'\n",
      "'>>> I am feeling strange right now, because of the weather.'\n",
      "'>>> I am feeling better right now, because of the weather.'\n",
      "'>>> I am feeling bad right now, because of the weather.'\n",
      "'>>> I am feeling uncomfortable right now, because of the weather.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"I am feeling [MASK] right now, because of the weather.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "token_logits = model_m(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T15:49:15.565602Z",
     "start_time": "2024-05-19T15:49:15.328768Z"
    }
   },
   "execution_count": 244
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_data),\n",
    "    \"validation\": Dataset.from_pandas(validation_data),\n",
    "    \"test\": Dataset.from_pandas(test_data)\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:22:01.712259Z",
     "start_time": "2024-05-19T17:22:01.577269Z"
    }
   },
   "execution_count": 319
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/42607 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48869d137bb94406b6658d4c36b541a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5398 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbfdeed207d848d29afcc5c4a14757c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5396 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8360e539649d4fd785620d2a3c779ef4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n        num_rows: 42607\n    })\n    validation: Dataset({\n        features: ['__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n        num_rows: 5398\n    })\n    test: Dataset({\n        features: ['__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n        num_rows: 5396\n    })\n})"
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    # result = tokenizer(examples[\"text\"], padding=True, truncation=True, return_tensors='pt').to(device)  \n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:22:07.726906Z",
     "start_time": "2024-05-19T17:22:03.330555Z"
    }
   },
   "execution_count": 320
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "512"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T13:48:10.043306Z",
     "start_time": "2024-05-19T13:48:10.032310Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chunk_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:22:44.380274Z",
     "start_time": "2024-05-19T17:22:44.363220Z"
    }
   },
   "execution_count": 321
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()\n",
    "                             if k != '__index_level_0__'}\n",
    "    \n",
    "    concatenated_examples['__index_level_0__'] = examples['__index_level_0__']\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:22:58.098113Z",
     "start_time": "2024-05-19T17:22:58.079867Z"
    }
   },
   "execution_count": 322
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/42607 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cb8ee32b72545d3871c56ccd48eded8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5398 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c319ce3db35b44feadbec0e9ff8b4c07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5396 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ffe41ae94214ecabddce040db82e08e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 1320\n    })\n    validation: Dataset({\n        features: ['__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 167\n    })\n    test: Dataset({\n        features: ['__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 167\n    })\n})"
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:23:08.130506Z",
     "start_time": "2024-05-19T17:23:01.227651Z"
    }
   },
   "execution_count": 323
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets.remove_columns([\"__index_level_0__\", \"token_type_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:23:12.804014Z",
     "start_time": "2024-05-19T17:23:12.782252Z"
    }
   },
   "execution_count": 324
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:23:21.083131Z",
     "start_time": "2024-05-19T17:23:21.066032Z"
    }
   },
   "execution_count": 325
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "        \n",
    "    res = default_data_collator(features)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:23:28.793820Z",
     "start_time": "2024-05-19T17:23:28.774744Z"
    }
   },
   "execution_count": 326
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This was the method used, not the for loops that will appear below\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "# logging_steps = 1\n",
    "model_name = f\"{model_checkpoint}-wwm\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_checkpoint}-wmm\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=8,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:23:40.560471Z",
     "start_time": "2024-05-19T17:23:40.445309Z"
    }
   },
   "execution_count": 329
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load local model\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_m,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    test_dataset=lm_datasets[\"test\"],\n",
    "    # data_collator=data_collator,\n",
    "    data_collator=whole_word_masking_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:08.545289Z",
     "start_time": "2024-05-19T15:55:08.529737Z"
    }
   },
   "execution_count": 265
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:08.991587Z",
     "start_time": "2024-05-19T15:55:08.973446Z"
    }
   },
   "execution_count": 266
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # masked_inputs = whole_word_masking_data_collator(features)\n",
    "    \n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:23:51.597593Z",
     "start_time": "2024-05-19T17:23:51.577489Z"
    }
   },
   "execution_count": 330
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/167 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c81eb8bb3e7241689dc4b328011512b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\"])\n",
    "eval_dataset = lm_datasets[\"validation\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=lm_datasets[\"validation\"].column_names,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:24:07.219085Z",
     "start_time": "2024-05-19T17:24:06.994711Z"
    }
   },
   "execution_count": 332
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:24:13.980029Z",
     "start_time": "2024-05-19T17:24:13.964466Z"
    }
   },
   "execution_count": 333
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 167\n})"
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:24:14.703626Z",
     "start_time": "2024-05-19T17:24:14.688708Z"
    }
   },
   "execution_count": 334
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import torch_default_data_collator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "collator = default_data_collator\n",
    "# collator = whole_word_masking_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:25:11.928935Z",
     "start_time": "2024-05-19T18:25:11.907847Z"
    }
   },
   "execution_count": 353
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1320\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 167\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# lm_datasets[\"validation\"].features\n",
    "print(train_dataloader.dataset)\n",
    "print(eval_dataloader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:25:14.540771Z",
     "start_time": "2024-05-19T18:25:14.525630Z"
    }
   },
   "execution_count": 354
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model_m.parameters(), lr=4e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:05:07.420243Z",
     "start_time": "2024-05-19T18:05:07.400156Z"
    }
   },
   "execution_count": 349
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 8\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:25:20.723302Z",
     "start_time": "2024-05-19T18:25:20.711752Z"
    }
   },
   "execution_count": 355
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/168 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5dcd6f0771d4bafaf0ecd3c5f7d5206"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 1: Training Perplexity: 1.0016015851285005\n",
      ">>> Epoch 1: Perplexity: 28291.66003866716\n",
      ">>> Epoch 2: Training Perplexity: 1.0017305385592177\n",
      ">>> Epoch 2: Perplexity: 18920.612252451443\n",
      ">>> Epoch 3: Training Perplexity: 1.0008708001809226\n",
      ">>> Epoch 3: Perplexity: 30854.916717727923\n",
      ">>> Epoch 4: Training Perplexity: 1.0004474036828919\n",
      ">>> Epoch 4: Perplexity: 33737.87399734711\n",
      ">>> Epoch 5: Training Perplexity: 1.0003026585428925\n",
      ">>> Epoch 5: Perplexity: 38197.62142543441\n",
      ">>> Epoch 6: Training Perplexity: 1.0003180343121076\n",
      ">>> Epoch 6: Perplexity: 48887.474472099966\n",
      ">>> Epoch 7: Training Perplexity: 1.0002184581205678\n",
      ">>> Epoch 7: Perplexity: 50281.94303011594\n",
      ">>> Epoch 8: Training Perplexity: 1.0002276090000621\n",
      ">>> Epoch 8: Perplexity: 49376.646908227136\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "output_dir = f\"{model_name}-wwm\"\n",
    "\n",
    "# this was not the method used for training but the results were essentially the same?? But I had some doubts about the loss/perplexity here\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model_m.train()\n",
    "    training_losses = []\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model_m(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        training_losses.append(loss.repeat(batch_size)) \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    training_losses = torch.cat(training_losses)  # this seems useless and confusing, but wtv\n",
    "    training_losses = training_losses[: len(train_dataloader.dataset)]\n",
    "    try:\n",
    "        training_perplexity = math.exp(torch.mean(training_losses))\n",
    "    except OverflowError:\n",
    "        training_perplexity = float(\"inf\")\n",
    "    print(f\">>> Epoch {epoch + 1}: Training Perplexity: {training_perplexity}\")\n",
    "        \n",
    "\n",
    "    # Evaluation\n",
    "    model_m.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model_m(**batch)\n",
    "\n",
    "        loss:torch.Tensor = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch + 1}: Perplexity: {perplexity}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T19:52:00.513857Z",
     "start_time": "2024-05-19T18:25:27.626759Z"
    }
   },
   "execution_count": 357
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/167 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25cb444de314491faa9e9359ebe0f3d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the model\n",
    "# use the test dataset and the whole_word_masking_data_collator\n",
    "test_dataset = lm_datasets[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=lm_datasets[\"test\"].column_names,\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    })\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "# test_dataloader = accelerator.prepare(test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T19:53:46.341289Z",
     "start_time": "2024-05-19T19:53:46.176289Z"
    }
   },
   "execution_count": 358
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 167\n})"
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T16:49:29.399805Z",
     "start_time": "2024-05-19T16:49:29.378802Z"
    }
   },
   "execution_count": 296
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "losses = []\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_m(**batch)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    losses.append(loss.repeat(batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T19:54:23.192567Z",
     "start_time": "2024-05-19T19:54:13.818771Z"
    }
   },
   "execution_count": 359
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "losses = torch.cat(losses)\n",
    "losses = losses[: len(test_dataset)]\n",
    "try:\n",
    "    perplexity = math.exp(torch.mean(losses))\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T19:54:28.146883Z",
     "start_time": "2024-05-19T19:54:28.133880Z"
    }
   },
   "execution_count": 360
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 54452.582913266815\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Perplexity: {perplexity}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T19:54:28.887778Z",
     "start_time": "2024-05-19T19:54:28.875784Z"
    }
   },
   "execution_count": 361
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> I am feeling better right now, because of the weather.'\n",
      "'>>> I am feeling bad right now, because of the weather.'\n",
      "'>>> I am feeling good right now, because of the weather.'\n",
      "'>>> I am feeling sick right now, because of the weather.'\n",
      "'>>> I am feeling strange right now, because of the weather.'\n"
     ]
    }
   ],
   "source": [
    "# test on some input\n",
    "text = \"I am feeling [MASK] right now, because of the weather.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "token_logits = model_m(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T19:54:32.914412Z",
     "start_time": "2024-05-19T19:54:32.670408Z"
    }
   },
   "execution_count": 362
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save model_m\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T19:56:40.014559Z",
     "start_time": "2024-05-19T19:56:38.094264Z"
    }
   },
   "execution_count": 363
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  label\n0     feel awful s job position succeed just didn t ...      0\n1                                   im alone feel awful      0\n2     ive probably mentioned but really feel proud a...      1\n3                               feeling little low days      0\n4     beleive sensitive peoples feelings tend compas...      2\n...                                                 ...    ...\n5395                   feel grumpy haven t yoga ed days      3\n5396  read blog suburb direction mentioned casually ...      3\n5397  not feel things realize violent physical suffe...      3\n5398  feel petty silly giving shit but little things...      3\n5399  remember feeling bitter couldnt pop balloons j...      3\n\n[5396 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feel awful s job position succeed just didn t ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>im alone feel awful</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ive probably mentioned but really feel proud a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feeling little low days</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beleive sensitive peoples feelings tend compas...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5395</th>\n      <td>feel grumpy haven t yoga ed days</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5396</th>\n      <td>read blog suburb direction mentioned casually ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5397</th>\n      <td>not feel things realize violent physical suffe...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5398</th>\n      <td>feel petty silly giving shit but little things...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5399</th>\n      <td>remember feeling bitter couldnt pop balloons j...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5396 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:37:25.993669Z",
     "start_time": "2024-05-21T14:37:25.980122Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f e e l   a w f u l   s   j o b   p o s i t i o n   s u c c e e d   j u s t   d i d n   t   h a p p e n\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 6.8039, -1.5189, -1.9315,  0.1075, -3.1939, -1.8422]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor(0, device='cuda:0')\n",
      "i m   a l o n e   f e e l   a w f u l\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 6.9390, -1.8971, -1.9743, -0.2243, -2.3850, -1.9237]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor(0, device='cuda:0')\n",
      "i v e   p r o b a b l y   m e n t i o n e d   b u t   r e a l l y   f e e l   p r o u d   a c t u a l l y   k e e p i n g   n e w   y e a r s   r e s o l u t i o n   m o n t h l y   w e e k l y   g o a l s\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0124,  7.0101,  0.3619, -2.2290, -2.1821, -1.2951]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor(1, device='cuda:0')\n",
      "f e e l i n g   l i t t l e   l o w   d a y s\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 6.5821, -2.4901, -0.4102, -0.7309, -2.3033, -1.5668]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor(0, device='cuda:0')\n",
      "b e l e i v e   s e n s i t i v e   p e o p l e s   f e e l i n g s   t e n d   c o m p a s s i o n a t e\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8729,  1.6744,  6.3005, -2.0582, -2.4697, -1.6340]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor(2, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y2= []\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# ' '.join(x)\n",
    "for p in test_data['text'][:5]:\n",
    "    print(' '.join(p))\n",
    "    ti = tokenizer(' '.join(p),padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():  # Deactivate gradients for the following code\n",
    "        out = model(**ti)\n",
    "        \n",
    "    pred = torch.argmax(out.logits)\n",
    "    print(out)\n",
    "    print(pred)\n",
    "    y2.append(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:22:06.828895Z",
     "start_time": "2024-05-21T15:22:06.457669Z"
    }
   },
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:06:22.145602Z",
     "start_time": "2024-05-21T15:04:30.264603Z"
    }
   },
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 6.8038945 , -1.518913  , -1.931491  ,  0.10749761, -3.1938698 ,\n        -1.8422035 ],\n       [ 6.938984  , -1.8970717 , -1.9743444 , -0.22429684, -2.3849494 ,\n        -1.9237057 ],\n       [-2.0124316 ,  7.0101357 ,  0.36193562, -2.2289808 , -2.1821327 ,\n        -1.2951416 ],\n       ...,\n       [-1.3564557 , -0.8732065 , -1.2695175 ,  7.1644497 , -1.0327799 ,\n        -2.229077  ],\n       [-1.2067674 , -1.1778868 , -0.94585097,  6.913796  , -1.2110066 ,\n        -1.8400692 ],\n       [-1.2924892 , -1.9809064 , -1.2988603 ,  7.1253195 , -0.12671858,\n        -2.079677  ]], dtype=float32)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:07:33.385608Z",
     "start_time": "2024-05-21T15:07:33.367608Z"
    }
   },
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred= []\n",
    "for p in test_data['text']:\n",
    "    ti = tokenizer(' '.join(p), return_tensors=\"pt\").to(device)\n",
    "    out = model(**ti)\n",
    "    pred = torch.argmax(out.logits)\n",
    "    y_pred.append(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:24:34.174666Z",
     "start_time": "2024-05-21T15:22:50.014533Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(4, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(0, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(3, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(1, device='cuda:0'),\n tensor(3, device='cuda:0'),\n ...]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:42:58.250222Z",
     "start_time": "2024-05-21T14:42:57.798754Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0       0\n1       0\n2       1\n3       0\n4       2\n       ..\n5395    3\n5396    3\n5397    3\n5398    3\n5399    3\nName: label, Length: 5396, dtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:43:16.320379Z",
     "start_time": "2024-05-21T14:43:16.313386Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1466   20    4   42   32    2]\n",
      " [  47 1621  119   30    8   10]\n",
      " [   4    0  424    1    0    2]\n",
      " [   6    3    1  722   21    2]\n",
      " [   1    2    1   12  564   13]\n",
      " [   0    0    0    1    0  215]]\n",
      "Accuracy:  0.9288361749444033\n",
      "Precision:  0.8993631860303525\n",
      "Recall:  0.9510063948994584\n",
      "F1:  0.9217117252272052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# to tensor\n",
    "y_test = test_data['label'].tolist()\n",
    "# plain list of numbers\n",
    "# y_pred_np = [int(i) for i in y_pred]\n",
    "y_pred_np = [int(i) for i in preds.predictions.argmax(axis=1)] \n",
    "# y_test.append(2)\n",
    "# y_pred_np.append(2)\n",
    "print(confusion_matrix(y_test, y_pred_np))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_np))\n",
    "print('Precision: ', precision_score(y_test, y_pred_np, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred_np, average='macro'))\n",
    "print('F1: ', f1_score(y_test, y_pred_np, average='macro'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:25:27.666474Z",
     "start_time": "2024-05-21T15:25:27.626911Z"
    }
   },
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_pred_np)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:55:39.779652Z",
     "start_time": "2024-05-21T14:55:39.765655Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 0,\n 1,\n 0,\n 2,\n 2,\n 1,\n 1,\n 1,\n 3,\n 3,\n 0,\n 1,\n 3,\n 3,\n 1,\n 0,\n 1,\n 2,\n 0,\n 0,\n 4,\n 0,\n 2,\n 3,\n 1,\n 1,\n 0,\n 0,\n 1,\n 5,\n 4,\n 1,\n 1,\n 2,\n 0,\n 1,\n 4,\n 1,\n 4,\n 0,\n 1,\n 0,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 2,\n 2,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 5,\n 0,\n 5,\n 0,\n 1,\n 5,\n 0,\n 4,\n 1,\n 1,\n 3,\n 0,\n 3,\n 2,\n 1,\n 4,\n 1,\n 4,\n 1,\n 0,\n 0,\n 1,\n 4,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 3,\n 4,\n 1,\n 3,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 4,\n 0,\n 1,\n 0,\n 1,\n 0,\n 3,\n 4,\n 1,\n 4,\n 4,\n 1,\n 1,\n 1,\n 1,\n 0,\n 5,\n 1,\n 2,\n 3,\n 0,\n 3,\n 1,\n 0,\n 0,\n 1,\n 0,\n 2,\n 5,\n 5,\n 1,\n 1,\n 1,\n 1,\n 0,\n 2,\n 4,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 4,\n 3,\n 0,\n 1,\n 1,\n 4,\n 1,\n 1,\n 3,\n 1,\n 1,\n 4,\n 1,\n 2,\n 1,\n 0,\n 4,\n 1,\n 1,\n 4,\n 0,\n 1,\n 1,\n 1,\n 3,\n 1,\n 0,\n 3,\n 3,\n 1,\n 4,\n 2,\n 1,\n 1,\n 0,\n 1,\n 1,\n 4,\n 1,\n 4,\n 1,\n 1,\n 0,\n 4,\n 1,\n 1,\n 1,\n 1,\n 0,\n 4,\n 1,\n 0,\n 1,\n 1,\n 0,\n 4,\n 0,\n 2,\n 0,\n 0,\n 1,\n 0,\n 0,\n 2,\n 3,\n 1,\n 0,\n 0,\n 4,\n 0,\n 0,\n 1,\n 0,\n 4,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 3,\n 0,\n 1,\n 0,\n 2,\n 1,\n 4,\n 2,\n 0,\n 0,\n 0,\n 0,\n 2,\n 2,\n 3,\n 3,\n 0,\n 4,\n 1,\n 4,\n 0,\n 0,\n 2,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 4,\n 0,\n 1,\n 0,\n 3,\n 1,\n 0,\n 1,\n 2,\n 3,\n 0,\n 3,\n 1,\n 0,\n 0,\n 3,\n 4,\n 2,\n 1,\n 3,\n 4,\n 0,\n 1,\n 0,\n 3,\n 1,\n 1,\n 2,\n 0,\n 1,\n 2,\n 0,\n 5,\n 0,\n 4,\n 3,\n 1,\n 1,\n 1,\n 0,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 0,\n 1,\n 0,\n 3,\n 0,\n 3,\n 1,\n 1,\n 2,\n 0,\n 2,\n 1,\n 0,\n 0,\n 1,\n 1,\n 4,\n 2,\n 0,\n 1,\n 0,\n 1,\n 1,\n 2,\n 1,\n 3,\n 1,\n 1,\n 5,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 5,\n 1,\n 0,\n 1,\n 0,\n 0,\n 3,\n 1,\n 2,\n 3,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 4,\n 2,\n 0,\n 0,\n 3,\n 1,\n 1,\n 3,\n 3,\n 4,\n 3,\n 2,\n 1,\n 3,\n 0,\n 0,\n 0,\n 1,\n 3,\n 3,\n 3,\n 3,\n 1,\n 0,\n 2,\n 0,\n 1,\n 0,\n 2,\n 4,\n 3,\n 0,\n 0,\n 1,\n 3,\n 3,\n 1,\n 5,\n 3,\n 2,\n 0,\n 1,\n 4,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 3,\n 1,\n 5,\n 1,\n 0,\n 4,\n 2,\n 1,\n 1,\n 3,\n 4,\n 3,\n 1,\n 0,\n 4,\n 4,\n 3,\n 5,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 3,\n 2,\n 1,\n 1,\n 0,\n 3,\n 3,\n 0,\n 4,\n 1,\n 4,\n 0,\n 1,\n 2,\n 2,\n 0,\n 1,\n 0,\n 1,\n 4,\n 0,\n 2,\n 1,\n 2,\n 3,\n 0,\n 1,\n 1,\n 3,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 3,\n 0,\n 0,\n 3,\n 0,\n 1,\n 3,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 2,\n 1,\n 3,\n 5,\n 2,\n 0,\n 2,\n 1,\n 4,\n 4,\n 4,\n 3,\n 0,\n 0,\n 1,\n 4,\n 1,\n 1,\n 0,\n 1,\n 4,\n 1,\n 0,\n 4,\n 2,\n 1,\n 0,\n 1,\n 1,\n 5,\n 0,\n 3,\n 3,\n 1,\n 4,\n 3,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 0,\n 1,\n 0,\n 4,\n 4,\n 3,\n 1,\n 0,\n 3,\n 2,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 2,\n 3,\n 3,\n 4,\n 1,\n 1,\n 2,\n 3,\n 4,\n 2,\n 4,\n 3,\n 4,\n 3,\n 0,\n 0,\n 1,\n 1,\n 1,\n 2,\n 3,\n 0,\n 3,\n 4,\n 0,\n 4,\n 0,\n 1,\n 5,\n 3,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 0,\n 0,\n 5,\n 5,\n 0,\n 1,\n 2,\n 0,\n 1,\n 4,\n 1,\n 4,\n 0,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 5,\n 1,\n 1,\n 0,\n 0,\n 4,\n 3,\n 1,\n 1,\n 1,\n 3,\n 0,\n 4,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 4,\n 0,\n 1,\n 0,\n 0,\n 1,\n 5,\n 1,\n 2,\n 3,\n 4,\n 1,\n 1,\n 3,\n 1,\n 2,\n 0,\n 2,\n 0,\n 0,\n 3,\n 1,\n 0,\n 0,\n 4,\n 0,\n 1,\n 2,\n 3,\n 3,\n 3,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 3,\n 1,\n 1,\n 4,\n 0,\n 1,\n 4,\n 3,\n 4,\n 0,\n 4,\n 0,\n 1,\n 2,\n 4,\n 0,\n 0,\n 1,\n 1,\n 1,\n 3,\n 1,\n 2,\n 4,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 4,\n 2,\n 3,\n 0,\n 0,\n 4,\n 0,\n 1,\n 0,\n 1,\n 3,\n 1,\n 0,\n 3,\n 0,\n 4,\n 1,\n 1,\n 1,\n 1,\n 0,\n 2,\n 0,\n 0,\n 3,\n 3,\n 3,\n 0,\n 3,\n 1,\n 0,\n 2,\n 2,\n 4,\n 3,\n 4,\n 1,\n 3,\n 0,\n 2,\n 0,\n 0,\n 0,\n 3,\n 2,\n 1,\n 0,\n 1,\n 3,\n 1,\n 2,\n 4,\n 3,\n 0,\n 1,\n 1,\n 2,\n 0,\n 0,\n 0,\n 2,\n 4,\n 1,\n 1,\n 1,\n 4,\n 1,\n 3,\n 1,\n 1,\n 0,\n 2,\n 0,\n 0,\n 1,\n 3,\n 0,\n 1,\n 1,\n 5,\n 1,\n 3,\n 1,\n 0,\n 4,\n 1,\n 4,\n 4,\n 4,\n 1,\n 5,\n 1,\n 1,\n 0,\n 0,\n 5,\n 1,\n 4,\n 1,\n 1,\n 0,\n 0,\n 3,\n 3,\n 0,\n 2,\n 5,\n 3,\n 1,\n 1,\n 4,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 4,\n 0,\n 2,\n 1,\n 0,\n 0,\n 4,\n 1,\n 0,\n 0,\n 2,\n 1,\n 2,\n 1,\n 0,\n 1,\n 4,\n 5,\n 0,\n 1,\n 4,\n 0,\n 3,\n 1,\n 0,\n 1,\n 4,\n 1,\n 1,\n 0,\n 3,\n 0,\n 0,\n 4,\n 0,\n 1,\n 3,\n 0,\n 0,\n 4,\n 5,\n 0,\n 1,\n 3,\n 0,\n 1,\n 1,\n 0,\n 0,\n 5,\n 0,\n 3,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 2,\n 1,\n 0,\n 0,\n 1,\n 3,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 2,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 3,\n 2,\n 3,\n 2,\n 2,\n 1,\n 1,\n 1,\n 3,\n 0,\n 3,\n 3,\n 5,\n 2,\n 3,\n 1,\n 0,\n 4,\n 1,\n 1,\n 1,\n 3,\n 3,\n 4,\n 1,\n 4,\n 2,\n 0,\n 0,\n 5,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 2,\n 0,\n 0,\n 0,\n 4,\n 2,\n 1,\n 3,\n 1,\n 0,\n 2,\n 0,\n 0,\n 1,\n 5,\n 4,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 2,\n 0,\n 1,\n 1,\n 0,\n 2,\n 2,\n 1,\n 4,\n 1,\n 0,\n 0,\n 2,\n 3,\n 0,\n 2,\n 0,\n 1,\n 2,\n 1,\n 2,\n 4,\n 0,\n 2,\n 0,\n 0,\n 3,\n 0,\n 0,\n ...]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:54:05.969734Z",
     "start_time": "2024-05-21T14:54:05.948726Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 4,\n 1,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 4,\n 3,\n 3,\n 0,\n 1,\n 3,\n 0,\n 0,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 0,\n 3,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 3,\n 0,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 0,\n 1,\n 3,\n 3,\n 0,\n 3,\n 3,\n 3,\n 0,\n 3,\n 3,\n 1,\n 3,\n 0,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 1,\n 0,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 0,\n 1,\n 3,\n 1,\n 1,\n 3,\n 3,\n 1,\n 0,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 0,\n 0,\n 0,\n 0,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 0,\n 1,\n 3,\n 0,\n 0,\n 1,\n 3,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 0,\n 4,\n 0,\n 0,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 0,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 0,\n 1,\n 1,\n 3,\n 1,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 4,\n 3,\n 1,\n 1,\n 3,\n 4,\n 1,\n 1,\n 3,\n 1,\n 3,\n 1,\n 0,\n 3,\n 1,\n 1,\n 0,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 3,\n 0,\n 3,\n 1,\n 3,\n 3,\n 1,\n 4,\n 1,\n 1,\n 4,\n 3,\n 1,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 0,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 0,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 1,\n 3,\n 4,\n 1,\n 3,\n 1,\n 1,\n 0,\n 3,\n 1,\n 3,\n 0,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 0,\n 0,\n 3,\n 1,\n 0,\n 3,\n 0,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 3,\n 0,\n 0,\n 1,\n 3,\n 0,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 0,\n 1,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 0,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 4,\n 1,\n 0,\n 1,\n 0,\n 0,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 0,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 0,\n 1,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 0,\n 3,\n 3,\n 1,\n 3,\n 3,\n 0,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 4,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 3,\n 4,\n 0,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 3,\n 3,\n 0,\n 3,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 0,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 1,\n 3,\n 1,\n 1,\n 0,\n 3,\n 3,\n 1,\n 0,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 0,\n 3,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 0,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 0,\n 1,\n 3,\n 1,\n 3,\n 1,\n 4,\n 1,\n 3,\n 0,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 0,\n 3,\n 3,\n 0,\n 1,\n 1,\n 3,\n 1,\n 1,\n 0,\n 3,\n 3,\n 1,\n 0,\n 1,\n 4,\n 0,\n 0,\n 3,\n 3,\n 1,\n 3,\n 0,\n 3,\n 3,\n 3,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 0,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 0,\n 1,\n 3,\n 4,\n 1,\n 3,\n 1,\n 4,\n 1,\n 3,\n 1,\n 1,\n 3,\n 0,\n 0,\n 3,\n 3,\n 3,\n 3,\n 0,\n 1,\n 3,\n 1,\n 0,\n 0,\n 3,\n 0,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 0,\n 0,\n 0,\n 3,\n 3,\n 3,\n 3,\n 0,\n 1,\n 1,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 4,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 4,\n 0,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 0,\n 3,\n 0,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 3,\n 0,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 3,\n 0,\n 3,\n 3,\n 1,\n 0,\n 1,\n 3,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 3,\n 3,\n 3,\n 1,\n 0,\n 3,\n 3,\n 3,\n 1,\n 4,\n 3,\n 3,\n 3,\n 1,\n 0,\n 1,\n 1,\n 0,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 1,\n 0,\n 3,\n 0,\n 1,\n 1,\n 3,\n 1,\n 0,\n 1,\n 3,\n 3,\n 0,\n 4,\n 3,\n 3,\n 0,\n 1,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 3,\n 3,\n 1,\n 1,\n 0,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 1,\n 3,\n 0,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 0,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 3,\n 1,\n 1,\n 0,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 3,\n 3,\n 0,\n 1,\n 1,\n 1,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 3,\n 3,\n 3,\n 3,\n 1,\n 0,\n 3,\n 1,\n 3,\n 3,\n 1,\n 3,\n 0,\n 1,\n 3,\n 3,\n 1,\n 3,\n 3,\n 1,\n 1,\n 3,\n ...]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:55:43.127740Z",
     "start_time": "2024-05-21T14:55:43.110755Z"
    }
   },
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{2}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_test) - set(y_pred_np)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:56:04.924161Z",
     "start_time": "2024-05-21T14:56:04.904157Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcarv\\AppData\\Local\\Temp\\ipykernel_25020\\1380442169.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# apply the data collator to the first 3 examples\n",
    "d = data_collator(test_dataset[:3])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:32:27.041726Z",
     "start_time": "2024-05-21T15:32:27.023727Z"
    }
   },
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[   0,  506,  364,  364,  784, 1437, 1437,   10,  885,  856, 1717,  784,\n         1437, 1437,  579, 1437, 1437, 1236, 1021,  741, 1437, 1437,  181, 1021,\n          579,  939,  326,  939, 1021,  295, 1437, 1437,  579, 1717,  740,  740,\n          364,  364,  385, 1437, 1437, 1236, 1717,  579,  326, 1437, 1437,  385,\n          939,  385,  295, 1437, 1437,  326, 1437, 1437, 1368,   10,  181,  181,\n          364,  295,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1],\n        [   0,  118,  475, 1437, 1437,   10,  784, 1021,  295,  364, 1437, 1437,\n          856,  364,  364,  784, 1437, 1437,   10,  885,  856, 1717,  784,    2,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1],\n        [   0,  118,  748,  364, 1437, 1437,  181,  910, 1021,  741,   10,  741,\n          784, 1423, 1437, 1437,  475,  364,  295,  326,  939, 1021,  295,  364,\n          385, 1437, 1437,  741, 1717,  326, 1437, 1437,  910,  364,   10,  784,\n          784, 1423, 1437, 1437,  856,  364,  364,  784, 1437, 1437,  181,  910,\n         1021, 1717,  385, 1437, 1437,   10,  740,  326, 1717,   10,  784,  784,\n         1423, 1437, 1437,  449,  364,  364,  181,  939,  295,  821, 1437, 1437,\n          295,  364,  885, 1437, 1437, 1423,  364,   10,  910,  579, 1437, 1437,\n          910,  364,  579, 1021,  784, 1717,  326,  939, 1021,  295, 1437, 1437,\n          475, 1021,  295,  326, 1368,  784, 1423, 1437, 1437,  885,  364,  364,\n          449,  784, 1423, 1437, 1437,  821, 1021,   10,  784,  579,    2,    1,\n            1,    1,    1,    1,    1,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0, 0, 1])}"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T15:32:34.458536Z",
     "start_time": "2024-05-21T15:32:34.438019Z"
    }
   },
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
